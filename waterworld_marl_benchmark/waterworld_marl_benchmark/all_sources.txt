================================================================================
FILE: dump_sources.py
================================================================================
import os
import sys

# 根目录与输出文件
root = sys.argv[1] if len(sys.argv) > 1 else "."
outfile = sys.argv[2] if len(sys.argv) > 2 else "all_sources.txt"

# 要包含的扩展名
exts = {".py", ".yaml", ".yml"}
# 要排除的目录
exclude_dirs = {".git", ".venv", "venv", "__pycache__", ".ipynb_checkpoints"}

# 打开输出文件
with open(outfile, "w", encoding="utf-8") as out:
    for dirpath, dirnames, filenames in os.walk(root):
        # 过滤不希望遍历的目录
        dirnames[:] = [d for d in dirnames if d not in exclude_dirs]

        rel_dir = os.path.relpath(dirpath, root)

        for fn in sorted(filenames):
            # 获取扩展名并检查是否在包含列表中
            ext = os.path.splitext(fn)[1].lower()
            if ext not in exts:
                continue

            # 排除以 fix 开头的 Python 文件
            if ext == ".py" and fn.lower().startswith("fix"):
                continue

            path = os.path.join(dirpath, fn)
            rel = os.path.normpath(os.path.join(rel_dir, fn)) if rel_dir != '.' else fn

            out.write("=" * 80 + "\n")
            out.write(f"FILE: {rel}\n")
            out.write("=" * 80 + "\n")

            try:
                with open(path, "r", encoding="utf-8") as f:
                    out.write(f.read())
            except UnicodeDecodeError:
                # 降级为容错读取
                with open(path, "r", encoding="utf-8", errors="replace") as f:
                    out.write(f.read())
            except Exception as e:
                out.write(f"\n[⚠️ ERROR reading file: {e}]\n")

            out.write("\n\n")

print(f"✅ Done -> {outfile}")


================================================================================
FILE: test_config_system.py
================================================================================
"""
测试配置系统是否正常工作
"""

from src.utils.config_loader import (
    get_mode_config,
    get_env_config,
    get_algo_config,
    get_training_config
)
from src.utils.path_manager import PathManager
from src.utils.naming import FileNaming
from src.utils.banner import print_mode_banner

def test_config_loading():
    """测试配置加载"""
    print("="*70)
    print("测试配置加载")
    print("="*70)
    
    # 测试运行模式配置
    print("\n1. 加载debug模式配置...")
    debug_config = get_mode_config("debug")
    print(f"   ✓ Debug训练步数: {debug_config['total_timesteps']}")
    
    # 测试环境配置
    print("\n2. 加载环境配置...")
    env_config = get_env_config("waterworld_standard")
    print(f"   ✓ Predator数量: {env_config['environment']['n_predators']}")
    
    # 测试算法配置
    print("\n3. 加载算法配置...")
    ppo_config = get_algo_config("PPO")
    print(f"   ✓ PPO学习率: {ppo_config['hyperparameters']['learning_rate']}")
    
    # 测试训练配置
    print("\n4. 加载训练配置...")
    stage_config = get_training_config("stage1_1_prey_warmup")
    print(f"   ✓ Stage名称: {stage_config['stage']['name']}")
    
    print("\n✅ 配置加载测试通过！\n")


def test_path_management():
    """测试路径管理"""
    print("="*70)
    print("测试路径管理")
    print("="*70)
    
    for mode in ["debug", "dryrun", "prod"]:
        print(f"\n{mode.upper()} 模式:")
        pm = PathManager(mode, "test_experiment")
        print(f"  模型目录: {pm.get_model_dir()}")
        print(f"  日志目录: {pm.get_tensorboard_dir()}")
    
    print("\n✅ 路径管理测试通过！\n")


def test_naming():
    """测试文件命名"""
    print("="*70)
    print("测试文件命名")
    print("="*70)
    
    naming = FileNaming()
    
    # 测试模型文件名生成
    filename = naming.generate_model_filename(
        train_algo="PPO",
        train_side="prey",
        version="v1",
        opponent_info="RANDOM_pred",
        run_mode="debug"
    )
    print(f"\nDebug模式文件名: {filename}")
    
    filename = naming.generate_model_filename(
        train_algo="SAC",
        train_side="predator",
        version="v2",
        opponent_info="MIX_prey_pool_v1",
        run_mode="prod"
    )
    print(f"Prod模式文件名: {filename}")
    
    print("\n✅ 文件命名测试通过！\n")


def test_banner():
    """测试横幅显示"""
    print("="*70)
    print("测试横幅显示")
    print("="*70)
    
    for mode in ["debug", "dryrun"]:  # prod需要确认，跳过
        config = get_mode_config(mode)
        print_mode_banner(mode, config)
    
    print("✅ 横幅显示测试通过！\n")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("🧪 配置系统测试")
    print("="*70 + "\n")
    
    try:
        test_config_loading()
        test_path_management()
        test_naming()
        test_banner()
        
        print("="*70)
        print("✅ 所有测试通过！配置系统工作正常")
        print("="*70 + "\n")
    
    except Exception as e:
        print(f"\n❌ 测试失败: {e}\n")
        import traceback
        traceback.print_exc()

================================================================================
FILE: test_training_system.py
================================================================================
"""
训练系统综合测试
验证所有组件是否正常工作
"""

import sys
from pathlib import Path

# 添加项目根目录到路径
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_imports():
    """测试所有模块导入"""
    print("\n" + "="*70)
    print("测试模块导入")
    print("="*70)
    
    try:
        # 核心模块
        from src.core import (
            WaterworldEnvManager,
            OpponentPool,
            MultiAgentTrainer,
            AgentManager
        )
        print("  ✓ 核心模块")
        
        # 算法模块
        from src.algorithms import (
            create_algorithm,
            PPOWrapper,
            A2CWrapper,
            SACWrapper,
            TD3Wrapper,
            RandomPolicy
        )
        print("  ✓ 算法模块")
        
        # 回调模块
        from src.callbacks import (
            MultiAgentTensorBoardCallback,
            CheckpointCallback,
            EvalCallback,
            FreezeCallback,
            ProgressBarCallback
        )
        print("  ✓ 回调模块")
        
        # 工具模块
        from src.utils.config_loader import get_mode_config, get_env_config, get_algo_config
        from src.utils.path_manager import PathManager
        from src.utils.naming import FileNaming
        from src.utils.logger import create_logger
        from src.utils.config_validator import validator
        print("  ✓ 工具模块")
        
        print("\n✅ 所有模块导入成功！\n")
        return True
    
    except Exception as e:
        print(f"\n❌ 模块导入失败: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_config_system():
    """测试配置系统"""
    print("="*70)
    print("测试配置系统")
    print("="*70)
    
    try:
        from src.utils.config_loader import (
            get_mode_config,
            get_env_config,
            get_algo_config,
            get_training_config
        )
        
        # 测试运行模式配置
        debug_config = get_mode_config("debug")
        assert 'total_timesteps' in debug_config
        print("  ✓ 运行模式配置")
        
        # 测试环境配置
        env_config = get_env_config("waterworld_standard")
        assert 'environment' in env_config
        print("  ✓ 环境配置")
        
        # 测试算法配置
        ppo_config = get_algo_config("PPO")
        assert 'hyperparameters' in ppo_config
        print("  ✓ 算法配置")
        
        # 测试训练配置
        stage_config = get_training_config("stage1_1_prey_warmup")
        assert 'stage' in stage_config
        print("  ✓ 训练阶段配置")
        
        print("\n✅ 配置系统测试通过！\n")
        return True
    
    except Exception as e:
        print(f"\n❌ 配置系统测试失败: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_environment_creation():
    """测试环境创建"""
    print("="*70)
    print("测试环境创建")
    print("="*70)
    
    try:
        from src.core import WaterworldEnvManager
        from src.utils.config_loader import get_env_config
        
        # 创建环境管理器
        env_config = get_env_config("waterworld_fast")  # 使用快速环境
        env_manager = WaterworldEnvManager(env_config)
        print("  ✓ 环境管理器创建")
        
        # 创建环境
        env = env_manager.create_env()
        print("  ✓ 环境创建")
        
        # 测试重置
        obs, infos = env.reset()
        print(f"  ✓ 环境重置 (agents: {len(env.agents)})")
        
        # 测试单步
        actions = {agent: env.action_space(agent).sample() for agent in env.agents}
        obs, rewards, terminations, truncations, infos = env.step(actions)
        print("  ✓ 环境单步")
        
        # 清理
        env_manager.close()
        print("  ✓ 环境清理")
        
        print("\n✅ 环境创建测试通过！\n")
        return True
    
    except Exception as e:
        print(f"\n❌ 环境创建测试失败: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_algorithm_creation():
    """测试算法创建"""
    print("="*70)
    print("测试算法创建")
    print("="*70)
    
    try:
        from src.algorithms import create_algorithm
        from src.core import WaterworldEnvManager
        from src.utils.config_loader import get_env_config, get_algo_config
        import gymnasium as gym
        
        # 创建环境获取空间信息
        env_config = get_env_config("waterworld_fast")
        env_manager = WaterworldEnvManager(env_config)
        env_manager.create_env()
        
        obs_space = env_manager.get_observation_space("predator")
        action_space = env_manager.get_action_space("predator")
        
        # 测试每个算法
        for algo_name in ['PPO', 'A2C', 'SAC', 'TD3', 'RANDOM']:
            algo_config = get_algo_config(algo_name)
            algorithm = create_algorithm(
                algo_name=algo_name,
                observation_space=obs_space,
                action_space=action_space,
                config=algo_config,
                device='cpu'
            )
            print(f"  ✓ {algo_name} 算法创建")
        
        env_manager.close()
        
        print("\n✅ 算法创建测试通过！\n")
        return True
    
    except Exception as e:
        print(f"\n❌ 算法创建测试失败: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_trainer_initialization():
    """测试训练器初始化"""
    print("="*70)
    print("测试训练器初始化")
    print("="*70)
    
    try:
        from src.core import MultiAgentTrainer
        
        # 创建最小配置的训练器
        trainer = MultiAgentTrainer(
            train_side='prey',
            train_algo='PPO',
            opponent_config={
                'type': 'algorithm',
                'side': 'predator',
                'algorithm': 'RANDOM',
                'freeze': True
            },
            experiment_name='test_experiment',
            stage_name='test_stage',
            generation=0,
            version='v1',
            run_mode='debug',
            total_timesteps=100  # 极少步数
        )
        print("  ✓ 训练器初始化")
        
        # 测试setup（不实际运行训练）
        trainer.setup()
        print("  ✓ 训练器设置")
        
        # 清理
        trainer.cleanup()
        print("  ✓ 训练器清理")
        
        print("\n✅ 训练器初始化测试通过！\n")
        return True
    
    except Exception as e:
        print(f"\n❌ 训练器初始化测试失败: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_mini_training():
    """测试完整的迷你训练流程"""
    print("="*70)
    print("测试迷你训练流程")
    print("="*70)
    print("这将运行一个超短的训练（200步）来验证完整流程")
    print("-"*70)
    
    try:
        from src.core import MultiAgentTrainer
        
        # 创建训练器
        trainer = MultiAgentTrainer(
            train_side='prey',
            train_algo='PPO',
            opponent_config={
                'type': 'algorithm',
                'side': 'predator',
                'algorithm': 'RANDOM',
                'freeze': True
            },
            experiment_name='mini_test',
            stage_name='test_mini_training',
            generation=0,
            version='v1',
            run_mode='debug',
            total_timesteps=200,  # 只训练200步
            n_envs=1,
            eval_freq=-1,  # 禁用评估
            checkpoint_freq=-1  # 禁用检查点
        )
        
        print("\n开始迷你训练...")
        
        # 设置
        trainer.setup()
        
        # 训练
        trainer.train()
        
        # 评估
        eval_results = trainer.evaluate(n_episodes=2)
        
        # 保存（但不加入池）
        trainer.save_model(save_to_pool=False)
        
        # 保存摘要
        trainer.save_training_summary()
        
        # 清理
        trainer.cleanup()
        
        print("\n✅ 迷你训练流程测试通过！")
        mean_reward = eval_results.get('mean_reward', None)
        if mean_reward is not None:
            print(f"   平均奖励: {mean_reward:.2f}")
        else:
            print(f"   平均奖励: N/A")
        print()
        return True
    
    except Exception as e:
        print(f"\n❌ 迷你训练流程测试失败: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def main():
    """运行所有测试"""
    print("\n" + "="*70)
    print("🧪 训练系统综合测试")
    print("="*70 + "\n")
    
    tests = [
        ("模块导入", test_imports),
        ("配置系统", test_config_system),
        ("环境创建", test_environment_creation),
        ("算法创建", test_algorithm_creation),
        ("训练器初始化", test_trainer_initialization),
        ("迷你训练流程", test_mini_training),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"❌ {test_name}测试崩溃: {e}")
            results[test_name] = False
    
    # 打印汇总
    print("="*70)
    print("测试结果汇总")
    print("="*70)
    
    for test_name, passed in results.items():
        status = "✅ 通过" if passed else "❌ 失败"
        print(f"{test_name:20s}: {status}")
    
    print("="*70)
    
    # 统计
    total = len(results)
    passed = sum(results.values())
    failed = total - passed
    
    print(f"\n总计: {total} 个测试")
    print(f"通过: {passed} 个")
    print(f"失败: {failed} 个")
    
    if failed == 0:
        print("\n🎉 所有测试通过！系统已准备就绪！")
        return 0
    else:
        print(f"\n⚠️  {failed} 个测试失败，请检查错误信息")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

================================================================================
FILE: debug_outputs/current/experiments/stage1.1_prey_warmup/TD3_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 3000
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_warmup
  generation: 0
  mode_config:
    archive_on_exit: true
    checkpoint_freq: -1
    clear_on_start: true
    deterministic_eval: true
    eval_freq: -1
    filename_prefix: DEBUG_
    log_level: DEBUG
    max_archives: 5
    n_envs: 1
    n_eval_episodes: 3
    output_base_dir: debug_outputs/current
    save_checkpoints: false
    save_final_model: false
    seed: 42
    tensorboard_enabled: false
    total_timesteps: 1000
    verbose: 2
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: debug
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: -1
    deterministic_eval: true
    eval_freq: -1
    n_envs: 1
    n_eval_episodes: 3
    save_checkpoints: false
    save_final_model: false
    show_progress: true
    tensorboard_enabled: false
    total_timesteps: 1000
    verbose: 2
  version: v1
timestamp: '2025-10-13T20:02:21.558108'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/A2C_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T21:03:13.318957'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/PPO_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:56:07.328380'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/RANDOM_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: custom
      name: RANDOM
    device: cpu
    hyperparameters: {}
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: RANDOM_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: RANDOM
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 50000
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:55:12.697257'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/SAC_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T21:06:20.197986'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/TD3_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T21:09:25.024869'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/A2C_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:04:44.754133'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/PPO_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T19:57:46.901723'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/RANDOM_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: custom
      name: RANDOM
    device: cpu
    hyperparameters: {}
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: RANDOM_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: RANDOM
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 5000
    verbose: 1
  version: v1
timestamp: '2025-10-13T19:57:46.809251'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/SAC_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:07:42.682084'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/TD3_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:10:39.595390'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_19/A2C_prey_v19_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 19
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_19
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v19
timestamp: '2025-10-14T02:06:13.857992'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_19/PPO_prey_v19_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 19
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_19
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v19
timestamp: '2025-10-14T01:59:02.695531'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_19/SAC_prey_v19_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 19
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_19
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v19
timestamp: '2025-10-14T02:09:24.658605'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_19/TD3_prey_v19_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 19
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_19
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v19
timestamp: '2025-10-14T02:12:32.517904'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_8/A2C_predator_v8_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 8
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_8
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v8
timestamp: '2025-10-13T23:05:26.912540'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_8/PPO_predator_v8_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 8
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_8
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v8
timestamp: '2025-10-13T22:58:14.320165'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_8/SAC_predator_v8_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 8
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_8
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v8
timestamp: '2025-10-13T23:08:33.026354'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_8/TD3_predator_v8_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 8
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_8
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v8
timestamp: '2025-10-13T23:11:40.087757'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_3/A2C_prey_v3_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 3
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_3
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v3
timestamp: '2025-10-13T23:39:06.933738'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_3/PPO_prey_v3_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 3
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_3
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v3
timestamp: '2025-10-13T23:32:52.999775'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_3/SAC_prey_v3_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 3
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_3
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v3
timestamp: '2025-10-13T23:41:20.059762'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_3/TD3_prey_v3_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 3
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_3
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v3
timestamp: '2025-10-13T23:43:30.516345'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_15/A2C_prey_v15_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 15
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_15
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v15
timestamp: '2025-10-14T01:00:05.910424'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_15/PPO_prey_v15_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 15
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_15
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v15
timestamp: '2025-10-14T00:53:07.560176'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_15/SAC_prey_v15_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 15
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_15
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v15
timestamp: '2025-10-14T01:03:08.157110'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_15/TD3_prey_v15_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 15
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_15
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v15
timestamp: '2025-10-14T01:06:09.105872'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_12/A2C_predator_v12_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 12
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_12
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v12
timestamp: '2025-10-14T00:11:07.229136'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_12/PPO_predator_v12_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 12
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_12
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v12
timestamp: '2025-10-14T00:03:57.217370'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_12/SAC_predator_v12_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 12
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_12
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v12
timestamp: '2025-10-14T00:14:15.353443'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_12/TD3_predator_v12_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 12
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_12
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v12
timestamp: '2025-10-14T00:17:22.168337'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_2/A2C_predator_v2_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 2
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_2
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v2
timestamp: '2025-10-13T23:26:06.638104'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_2/PPO_predator_v2_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 2
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_2
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v2
timestamp: '2025-10-13T23:19:38.971214'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_2/SAC_predator_v2_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 2
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_2
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v2
timestamp: '2025-10-13T23:28:22.144806'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_2/TD3_predator_v2_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 2
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_2
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v2
timestamp: '2025-10-13T23:30:36.954869'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_5/A2C_prey_v5_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 5
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_5
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v5
timestamp: '2025-10-13T22:16:39.151667'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_5/PPO_prey_v5_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 5
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_5
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v5
timestamp: '2025-10-13T22:09:40.267268'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_5/SAC_prey_v5_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 5
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_5
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v5
timestamp: '2025-10-13T22:19:41.237049'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_5/TD3_prey_v5_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 5
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_5
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v5
timestamp: '2025-10-13T22:22:41.391525'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_10/A2C_predator_v10_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 10
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_10
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v10
timestamp: '2025-10-13T23:38:24.480207'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_10/PPO_predator_v10_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 10
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_10
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v10
timestamp: '2025-10-13T23:31:06.817019'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_10/SAC_predator_v10_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 10
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_10
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v10
timestamp: '2025-10-13T23:41:36.746970'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_10/TD3_predator_v10_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 10
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_10
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v10
timestamp: '2025-10-13T23:44:48.174923'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_9/A2C_prey_v9_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 9
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_9
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v9
timestamp: '2025-10-13T23:21:52.143509'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_9/PPO_prey_v9_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 9
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_9
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v9
timestamp: '2025-10-13T23:14:48.671874'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_9/SAC_prey_v9_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 9
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_9
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v9
timestamp: '2025-10-13T23:24:58.235728'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_9/TD3_prey_v9_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 9
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_9
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v9
timestamp: '2025-10-13T23:28:02.358081'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_7/A2C_prey_v7_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 7
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_7
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v7
timestamp: '2025-10-13T22:49:14.065508'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_7/PPO_prey_v7_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 7
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_7
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v7
timestamp: '2025-10-13T22:42:14.893699'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_7/SAC_prey_v7_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 7
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_7
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v7
timestamp: '2025-10-13T22:52:15.172643'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_7/TD3_prey_v7_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 7
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_7
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v7
timestamp: '2025-10-13T22:55:14.824515'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_16/A2C_predator_v16_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 16
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_16
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v16
timestamp: '2025-10-14T01:16:22.049686'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_16/PPO_predator_v16_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 16
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_16
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v16
timestamp: '2025-10-14T01:09:07.933363'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_16/SAC_predator_v16_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 16
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_16
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v16
timestamp: '2025-10-14T01:19:30.213901'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_16/TD3_predator_v16_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 16
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_16
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v16
timestamp: '2025-10-14T01:22:40.194781'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_6/A2C_predator_v6_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 6
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_6
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v6
timestamp: '2025-10-13T22:32:52.335549'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_6/PPO_predator_v6_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 6
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_6
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v6
timestamp: '2025-10-13T22:25:41.106256'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_6/SAC_predator_v6_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 6
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_6
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v6
timestamp: '2025-10-13T22:36:00.766592'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_6/TD3_predator_v6_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 6
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_6
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v6
timestamp: '2025-10-13T22:39:07.606228'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_20/PPO_predator_v20_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 20
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_20
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v20
timestamp: '2025-10-14T02:15:35.795579'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_17/A2C_prey_v17_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 17
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_17
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v17
timestamp: '2025-10-14T01:32:58.612310'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_17/PPO_prey_v17_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 17
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_17
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v17
timestamp: '2025-10-14T01:25:50.507483'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_17/SAC_prey_v17_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 17
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_17
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v17
timestamp: '2025-10-14T01:36:04.585738'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_17/TD3_prey_v17_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 17
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_17
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v17
timestamp: '2025-10-14T01:39:08.566699'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_4/A2C_predator_v4_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 4
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_4
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v4
timestamp: '2025-10-13T22:00:18.008891'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_4/PPO_predator_v4_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 4
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_4
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v4
timestamp: '2025-10-13T21:53:03.283044'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_4/SAC_predator_v4_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 4
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_4
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v4
timestamp: '2025-10-13T22:03:27.291306'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_4/TD3_predator_v4_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 4
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_4
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v4
timestamp: '2025-10-13T22:06:33.007575'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_13/A2C_prey_v13_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 13
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_13
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v13
timestamp: '2025-10-14T00:27:29.381684'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_13/PPO_prey_v13_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 13
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_13
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v13
timestamp: '2025-10-14T00:20:30.603820'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_13/SAC_prey_v13_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 13
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_13
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v13
timestamp: '2025-10-14T00:30:30.236340'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_13/TD3_prey_v13_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 13
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_13
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v13
timestamp: '2025-10-14T00:33:31.105186'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_14/A2C_predator_v14_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 14
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_14
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v14
timestamp: '2025-10-14T00:43:46.730877'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_14/PPO_predator_v14_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 14
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_14
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v14
timestamp: '2025-10-14T00:36:31.295427'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_14/SAC_predator_v14_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 14
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_14
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v14
timestamp: '2025-10-14T00:46:55.304911'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_14/TD3_predator_v14_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 14
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_14
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v14
timestamp: '2025-10-14T00:50:02.121000'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_11/A2C_prey_v11_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 11
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_11
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v11
timestamp: '2025-10-13T23:54:55.981495'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_11/PPO_prey_v11_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 11
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_11
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v11
timestamp: '2025-10-13T23:47:56.922850'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_11/SAC_prey_v11_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 11
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_11
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v11
timestamp: '2025-10-13T23:57:57.837789'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_11/TD3_prey_v11_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 11
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_11
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v11
timestamp: '2025-10-14T00:00:57.340484'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_18/A2C_predator_v18_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 18
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_18
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v18
timestamp: '2025-10-14T01:49:31.567227'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_18/PPO_predator_v18_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 18
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_18
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v18
timestamp: '2025-10-14T01:42:11.082890'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_18/SAC_predator_v18_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 18
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_18
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v18
timestamp: '2025-10-14T01:52:41.989882'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_18/TD3_predator_v18_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 18
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_18
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v18
timestamp: '2025-10-14T01:55:52.542682'


================================================================================
FILE: scripts/analysis/analyze_results.py
================================================================================


================================================================================
FILE: scripts/analysis/plot_results.py
================================================================================
"""
可视化分析结果
生成热力图、泛化曲线、雷达图等
"""

import sys
import argparse
import json
import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# 添加项目根目录
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.config_loader import config_loader


def parse_args():
    parser = argparse.ArgumentParser(description='可视化评估结果')
    parser.add_argument('--input', type=str, required=True,
                        help='交叉评估结果目录')
    parser.add_argument('--output', type=str, default=None,
                        help='输出目录（默认为input目录）')
    return parser.parse_args()


def load_results(results_dir: Path):
    """加载评估结果"""
    # 加载原始结果
    raw_path = results_dir / "raw_results.pkl"
    if raw_path.exists():
        with open(raw_path, 'rb') as f:
            results_matrix = pickle.load(f)
    else:
        print(f"❌ 未找到原始结果: {raw_path}")
        return None
    
    # 加载自适应性得分
    adapt_path = results_dir / "adaptability_scores.json"
    if adapt_path.exists():
        with open(adapt_path, 'r') as f:
            adaptability_data = json.load(f)
    else:
        adaptability_data = None
    
    return results_matrix, adaptability_data


def plot_heatmap(results_matrix, output_path, metric='catch_rate'):
    """
    绘制热力图
    
    Args:
        results_matrix: 结果矩阵
        output_path: 输出路径
        metric: 要显示的指标
    """
    # 提取算法列表
    algos = list(results_matrix.keys())
    
    # 构建矩阵
    matrix = np.zeros((len(algos), len(algos)))
    
    for i, pred_algo in enumerate(algos):
        for j, prey_algo in enumerate(algos):
            metrics = results_matrix[pred_algo].get(prey_algo)
            if metrics:
                matrix[i, j] = metrics.get(metric, 0)
            else:
                matrix[i, j] = np.nan
    
    # 绘制热力图
    plt.figure(figsize=(10, 8))
    
    # 创建mask来标记对角线
    mask = np.zeros_like(matrix, dtype=bool)
    
    sns.heatmap(
        matrix,
        annot=True,
        fmt='.3f',
        cmap='RdYlGn',
        xticklabels=algos,
        yticklabels=algos,
        vmin=0, vmax=1,
        cbar_kws={'label': 'Predator Catch Rate'},
        linewidths=0.5,
        linecolor='gray',
        mask=mask
    )
    
    # 标记对角线（In-Distribution）
    for i in range(len(algos)):
        plt.gca().add_patch(plt.Rectangle((i, i), 1, 1,
                            fill=False, edgecolor='blue', lw=3))
    
    plt.xlabel('Prey Algorithm', fontsize=12, fontweight='bold')
    plt.ylabel('Predator Algorithm', fontsize=12, fontweight='bold')
    plt.title(f'Cross-Algorithm Performance Matrix\n'
              f'Metric: {metric.replace("_", " ").title()}\n'
              f'Blue boxes = In-Distribution (trained together)',
              fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  ✓ 热力图: {output_path}")


def plot_generalization_curves(results_matrix, adaptability_data, output_path):
    """绘制泛化曲线"""
    
    # 加载策略距离配置
    eval_config = config_loader.load_yaml('evaluation/cross_eval.yaml')
    policy_distances = eval_config['analysis']['policy_distance']['distances']
    
    plt.figure(figsize=(10, 6))
    
    algos = [a for a in results_matrix.keys() if a != 'RANDOM']
    
    for pred_algo in algos:
        # 收集(距离, 性能)点对
        points = []
        
        for prey_algo in algos:
            if prey_algo in policy_distances.get(pred_algo, {}):
                distance = policy_distances[pred_algo][prey_algo]
                
                metrics = results_matrix[pred_algo].get(prey_algo)
                if metrics:
                    performance = metrics['catch_rate']
                    points.append((distance, performance))
        
        # 按距离排序
        points.sort(key=lambda x: x[0])
        
        if points:
            distances = [p[0] for p in points]
            performances = [p[1] for p in points]
            
            plt.plot(distances, performances, marker='o', linewidth=2,
                    label=f'{pred_algo}', markersize=8)
    
    plt.xlabel('Policy Distance from Training Distribution', fontsize=12)
    plt.ylabel('Catch Rate (Performance)', fontsize=12)
    plt.title('Generalization Curves: Performance vs Opponent Distance',
              fontsize=14, fontweight='bold')
    plt.legend(loc='best')
    plt.grid(alpha=0.3)
    plt.axhline(0.5, color='gray', linestyle='--', alpha=0.5,
                label='Balance Line (50%)')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  ✓ 泛化曲线: {output_path}")


def plot_ranking_bars(adaptability_data, output_path):
    """绘制排名柱状图"""
    
    if not adaptability_data:
        print("  ⚠️  跳过排名图: 无自适应性数据")
        return
    
    ranking = adaptability_data['ranking']
    
    algos = [r['algorithm'] for r in ranking]
    adapt_scores = [r['adaptability_score'] for r in ranking]
    in_dist = [r['in_dist_performance'] for r in ranking]
    ood_avg = [r['ood_avg_performance'] for r in ranking]
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # 子图1: 自适应性得分
    ax1 = axes[0]
    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(algos)))
    bars1 = ax1.barh(algos, adapt_scores, color=colors)
    ax1.set_xlabel('Adaptability Score', fontsize=12)
    ax1.set_title('Algorithm Adaptability Ranking', fontsize=13, fontweight='bold')
    ax1.set_xlim(0, 1.1)
    
    # 添加数值标签
    for bar, score in zip(bars1, adapt_scores):
        width = bar.get_width()
        ax1.text(width + 0.02, bar.get_y() + bar.get_height()/2,
                f'{score:.3f}', va='center', fontsize=10)
    
    # 子图2: In-Dist vs OOD 性能
    ax2 = axes[1]
    x = np.arange(len(algos))
    width = 0.35
    
    bars1 = ax2.bar(x - width/2, in_dist, width, label='In-Distribution',
                    color='steelblue', alpha=0.8)
    bars2 = ax2.bar(x + width/2, ood_avg, width, label='Out-of-Distribution',
                    color='coral', alpha=0.8)
    
    ax2.set_ylabel('Catch Rate', fontsize=12)
    ax2.set_title('In-Distribution vs OOD Performance', fontsize=13, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(algos)
    ax2.legend()
    ax2.set_ylim(0, 0.8)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  ✓ 排名图: {output_path}")


def plot_performance_distribution(results_matrix, output_path):
    """绘制性能分布箱线图"""
    
    algos = [a for a in results_matrix.keys() if a != 'RANDOM']
    
    # 收集每个算法的OOD性能
    ood_performances = {algo: [] for algo in algos}
    
    for pred_algo in algos:
        for prey_algo in algos:
            if prey_algo != pred_algo and prey_algo != 'RANDOM':
                metrics = results_matrix[pred_algo].get(prey_algo)
                if metrics:
                    ood_performances[pred_algo].append(metrics['catch_rate'])
    
    # 绘制箱线图
    plt.figure(figsize=(10, 6))
    
    data = [ood_performances[algo] for algo in algos]
    positions = range(1, len(algos) + 1)
    
    bp = plt.boxplot(data, positions=positions, labels=algos,
                     patch_artist=True, widths=0.6)
    
    # 设置颜色
    colors = plt.cm.Set3(np.linspace(0, 1, len(algos)))
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    plt.ylabel('Catch Rate (OOD Performance)', fontsize=12)
    plt.xlabel('Predator Algorithm', fontsize=12)
    plt.title('Out-of-Distribution Performance Distribution\n'
              '(Performance against unseen opponents)',
              fontsize=14, fontweight='bold')
    plt.grid(axis='y', alpha=0.3)
    plt.axhline(0.5, color='red', linestyle='--', alpha=0.5,
                label='Balance Line')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  ✓ 性能分布图: {output_path}")


def main():
    args = parse_args()
    
    results_dir = Path(args.input)
    output_dir = Path(args.output) if args.output else results_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*70)
    print("📊 生成可视化结果")
    print("="*70)
    print(f"输入目录: {results_dir}")
    print(f"输出目录: {output_dir}")
    print("="*70 + "\n")
    
    # 加载结果
    print("📂 加载数据...")
    results_data = load_results(results_dir)
    
    if results_data is None:
        print("❌ 加载失败")
        return
    
    results_matrix, adaptability_data = results_data
    print("  ✓ 数据加载完成\n")
    
    # 生成各种图表
    print("🎨 生成图表...")
    
    # 1. 热力图
    plot_heatmap(
        results_matrix,
        output_dir / "heatmap_catch_rate.png",
        metric='catch_rate'
    )
    
    # 2. 泛化曲线
    plot_generalization_curves(
        results_matrix,
        adaptability_data,
        output_dir / "generalization_curves.png"
    )
    
    # 3. 排名柱状图
    if adaptability_data:
        plot_ranking_bars(
            adaptability_data,
            output_dir / "ranking_comparison.png"
        )
    
    # 4. 性能分布箱线图
    plot_performance_distribution(
        results_matrix,
        output_dir / "performance_distribution.png"
    )
    
    print("\n" + "="*70)
    print("✅ 可视化完成！")
    print("="*70)
    print(f"\n📁 图表保存在: {output_dir}")
    print("\n生成的图表:")
    print("  1. heatmap_catch_rate.png         - 性能热力图")
    print("  2. generalization_curves.png      - 泛化曲线")
    print("  3. ranking_comparison.png         - 算法排名对比")
    print("  4. performance_distribution.png   - 性能分布")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/training/train_single.py
================================================================================
"""
单次训练脚本（通用）
可用于训练任意配置的单个模型
"""

import argparse
import sys
from pathlib import Path

# 添加项目根目录到路径
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_env_config, get_training_config


def parse_args():
    """解析命令行参数"""
    parser = argparse.ArgumentParser(description='单次训练脚本')
    
    # 核心参数
    parser.add_argument('--train-side', type=str, required=True,
                        choices=['predator', 'prey'],
                        help='训练方')
    parser.add_argument('--train-algo', type=str, required=True,
                        choices=['PPO', 'A2C', 'SAC', 'TD3', 'RANDOM'],
                        help='训练算法')
    parser.add_argument('--opponent-type', type=str, required=True,
                        choices=['algorithm', 'fixed_model', 'mixed_pool'],
                        help='对手类型')
    
    # 对手配置
    parser.add_argument('--opponent-algo', type=str, default='RANDOM',
                        help='对手算法（当opponent-type=algorithm时使用）')
    parser.add_argument('--opponent-model', type=str, default=None,
                        help='对手模型路径（当opponent-type=fixed_model时使用）')
    parser.add_argument('--opponent-pool', type=str, default=None,
                        help='对手池路径（当opponent-type=mixed_pool时使用）')
    parser.add_argument('--fixed-ratio', type=float, default=0.7,
                        help='固定对手占比（当opponent-type=mixed_pool时使用）')
    
    # 实验元数据
    parser.add_argument('--experiment-name', type=str, required=True,
                        help='实验名称')
    parser.add_argument('--stage-name', type=str, required=True,
                        help='训练阶段名称')
    parser.add_argument('--version', type=str, default='v1',
                        help='版本号')
    parser.add_argument('--generation', type=int, default=0,
                        help='代数')
    
    # 运行模式
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun','test',  'prod'],
                        help='运行模式')
    
    # 环境配置
    parser.add_argument('--env-config', type=str, default='waterworld_standard',
                        help='环境配置名称')
    
    # 训练配置（覆盖默认值）
    parser.add_argument('--timesteps', type=int, default=None,
                        help='总训练步数')
    parser.add_argument('--n-envs', type=int, default=None,
                        help='并行环境数')
    parser.add_argument('--eval-freq', type=int, default=None,
                        help='评估频率')
    parser.add_argument('--checkpoint-freq', type=int, default=None,
                        help='检查点频率')
    
    # 其他
    parser.add_argument('--device', type=str, default='auto',
                        help='计算设备（auto/cuda/cpu）')
    parser.add_argument('--seed', type=int, default=None,
                        help='随机种子')
    parser.add_argument('--notes', type=str, default='',
                        help='实验备注')
    
    # 保存选项
    parser.add_argument('--save-to-pool', action='store_true',
                        help='是否保存到固定池')
    parser.add_argument('--pool-name', type=str, default=None,
                        help='固定池名称')
    parser.add_argument('--check-freeze', action='store_true',
                        help='是否检查冻结条件')
    
    return parser.parse_args()


def build_opponent_config(args):
    """根据参数构建对手配置"""
    
    # 对手角色（与训练方相反）
    opponent_side = 'prey' if args.train_side == 'predator' else 'predator'
    
    if args.opponent_type == 'algorithm':
        return {
            'type': 'algorithm',
            'side': opponent_side,
            'algorithm': args.opponent_algo,
            'freeze': True
        }
    
    elif args.opponent_type == 'fixed_model':
        if not args.opponent_model:
            raise ValueError("使用fixed_model时必须指定--opponent-model")
        
        return {
            'type': 'fixed_model',
            'side': opponent_side,
            'path': args.opponent_model,
            'freeze': True
        }
    
    elif args.opponent_type == 'mixed_pool':
        if not args.opponent_pool:
            raise ValueError("使用mixed_pool时必须指定--opponent-pool")
        
        return {
            'type': 'mixed_pool',
            'side': opponent_side,
            'pool_path': args.opponent_pool,
            'mix_strategy': {
                'fixed_ratio': args.fixed_ratio,
                'sampling': 'uniform'
            },
            'freeze': True
        }
    
    else:
        raise ValueError(f"未知的对手类型: {args.opponent_type}")


def main():
    """主函数"""
    args = parse_args()
    
    # 构建对手配置
    opponent_config = build_opponent_config(args)
    
    # 加载环境配置
    env_config = get_env_config(args.env_config)
    
    # 创建训练器
    trainer = MultiAgentTrainer(
        train_side=args.train_side,
        train_algo=args.train_algo,
        opponent_config=opponent_config,
        experiment_name=args.experiment_name,
        stage_name=args.stage_name,
        generation=args.generation,
        version=args.version,
        run_mode=args.mode,
        env_config=env_config,
        total_timesteps=args.timesteps,
        n_envs=args.n_envs,
        eval_freq=args.eval_freq,
        checkpoint_freq=args.checkpoint_freq,
        device=args.device,
        seed=args.seed,
        notes=args.notes
    )
    
    # 运行训练
    freeze_criteria = None
    if args.check_freeze:
        from src.utils.config_loader import config_loader
        run_modes_config = config_loader.load_yaml("run_modes.yaml")
        freeze_criteria = run_modes_config['freeze_criteria'][args.train_side]
    
    trainer.run(
        save_to_pool=args.save_to_pool,
        pool_name=args.pool_name,
        check_freeze=args.check_freeze,
        freeze_criteria=freeze_criteria
    )
    
    print("\n✅ 训练完成！")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/training/train_stage1_1.py
================================================================================
"""
Stage 1.1: Prey预热训练
训练所有算法的Prey对抗RANDOM Predator
"""

import argparse
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config, get_env_config  # ← 加上 get_env_config


def parse_args():
    """解析命令行参数"""
    parser = argparse.ArgumentParser(description='Stage 1.1: Prey预热训练')
    
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun', 'test', 'prod'],
                        help='运行模式')
    
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3'],
                        help='要训练的算法列表')
    
    parser.add_argument('--env-config', type=str, default='waterworld_standard',
                        help='环境配置')
    
    parser.add_argument('--device', type=str, default='auto',
                        help='计算设备')
    
    parser.add_argument('--timesteps', type=int, default=None,
                        help='总训练步数（覆盖配置）')
    
    return parser.parse_args()


def train_one_prey_algo(algo: str, args, stage_config: dict):
    """训练单个Prey算法"""
    
    print(f"\n{'='*70}")
    print(f"{'记录' if algo == 'RANDOM' else '训练'} {algo}_prey vs RANDOM_predator")
    print(f"{'='*70}\n")
    
    # 构建对手配置
    opponent_config = {
        'type': 'algorithm',
        'side': 'predator',
        'algorithm': 'RANDOM',
        'freeze': True
    }
    
    # 确定训练步数
    if algo == 'RANDOM':
        # RANDOM 只需要运行足够的 episodes 来记录基线
        # 例如运行 50k 步（大约 50 个 episodes）
        timesteps = 5000
    else:
        # 正常算法的训练步数
        timesteps = args.timesteps
    
    # 创建训练器
    # ✅ 根据模式选择环境配置
    if args.mode == 'test':
        env_config_name = 'waterworld_fast'
        print(f"🏃 测试模式，使用快速环境: max_cycles=500")
    else:
        env_config_name = 'waterworld_standard'
    
    # 创建训练器
    trainer = MultiAgentTrainer(
        train_side='prey',
        train_algo=algo,
        opponent_config=opponent_config,
        experiment_name=f"{algo}_prey_warmup",
        stage_name=stage_config['stage']['name'],
        generation=stage_config['stage']['generation'],
        version='v1',
        run_mode=args.mode,
        env_config=get_env_config(env_config_name),  # ✅ 使用动态环境
        total_timesteps=timesteps,
        device=args.device
    )
    
    # 获取冻结条件
    freeze_config = stage_config.get('freeze_on_success', {})
    freeze_criteria = freeze_config.get('criteria', {})
    
    # 运行训练
    # RANDOM 不需要保存到池
    save_to_pool = freeze_config.get('enabled', False) if algo != 'RANDOM' else False
    
    eval_results = trainer.run(
        save_to_pool=save_to_pool,
        pool_name=freeze_config.get('save_to_pool'),
        check_freeze=freeze_config.get('enabled', False) if algo != 'RANDOM' else False,
        freeze_criteria=freeze_criteria
    )
    
    return eval_results

def main():
    """主函数"""
    args = parse_args()
    
    # 加载Stage 1.1配置
    stage_config = get_training_config('stage1_1_prey_warmup')
    
    # 获取算法列表
    algos_to_train = args.algos or stage_config.get('algorithms_to_train', ['PPO', 'A2C', 'SAC', 'TD3'])
    
    print(f"\n{'='*70}")
    print(f"🎯 Stage 1.1: Prey预热训练")
    print(f"{'='*70}")
    print(f"运行模式: {args.mode}")
    print(f"训练算法: {', '.join(algos_to_train)}")
    print(f"{'='*70}\n")
    
    # ========== 新增：先运行 RANDOM 作为基线 ==========
    print(f"\n{'='*70}")
    print(f"步骤 0: 记录 RANDOM Baseline")
    print(f"{'='*70}\n")
    
    # 将 RANDOM 添加到训练列表的最前面
    all_algos = ['RANDOM'] + algos_to_train
    
    # ========== 训练所有算法（包括 RANDOM）==========
    results = {}
    for algo in all_algos:
        try:
            eval_results = train_one_prey_algo(algo, args, stage_config)
            results[algo] = eval_results
        except KeyboardInterrupt:
            print(f"\n⚠️  {algo} 训练被中断")
            break
        except Exception as e:
            print(f"\n❌ {algo} 训练失败: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # 打印汇总（突出显示与 RANDOM 的对比）
    print(f"\n{'='*70}")
    print(f"✅ Stage 1.1 完成")
    print(f"{'='*70}")
    
    # 显示 RANDOM 基线
    if 'RANDOM' in results and results['RANDOM']:
        baseline_reward = results['RANDOM'].get('mean_reward', 0)
        print(f"\n📊 Random Baseline: {baseline_reward:.2f}")
        
        print(f"\n训练结果（相对于 Random）:")
        for algo in algos_to_train:  # 只显示训练的算法
            if algo in results and results[algo]:
                reward = results[algo].get('mean_reward', 0)
                improvement = ((reward - baseline_reward) / abs(baseline_reward) * 100) if baseline_reward != 0 else 0
                status = "✅" if improvement > 0 else "❌"
                print(f"  {status} {algo}: {reward:.2f} ({improvement:+.1f}% vs Random)")
    else:
        print(f"\n训练结果:")
        for algo, result in results.items():
            if result:
                print(f"  {algo}: 平均奖励 = {result.get('mean_reward', 'N/A'):.2f}")
    
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/training/train_stage1_2.py
================================================================================
"""
Stage 1.2: Predator引导训练
训练所有算法的Predator对抗固定的Prey池v1
"""

import argparse
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config, get_env_config  # ← 添加 get_env_config

def parse_args():
    """解析命令行参数"""
    parser = argparse.ArgumentParser(description='Stage 1.2: Predator引导训练')
    
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun','test',  'prod'],
                        help='运行模式')
    
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3'],
                        help='要训练的算法列表')
    
    parser.add_argument('--prey-pool', type=str, default='outputs/fixed_pools/prey_pool_v1',
                        help='Prey固定池路径')
    
    parser.add_argument('--device', type=str, default='auto',
                        help='计算设备')
    
    parser.add_argument('--timesteps', type=int, default=None,
                        help='总训练步数（覆盖配置）')
    
    return parser.parse_args()


def train_one_predator_algo(algo: str, args, stage_config: dict):
    """训练单个Predator算法"""
    
    print(f"\n{'='*70}")
    print(f"{'记录' if algo == 'RANDOM' else '训练'} {algo}_predator vs prey_pool_v1")
    print(f"{'='*70}\n")
    
    # 构建对手配置
    opponent_config = {
        'type': 'mixed_pool',
        'side': 'prey',
        'pool_path': args.prey_pool,
        'mix_strategy': {
            'fixed_ratio': 0.7,
            'sampling': 'uniform'
        },
        'freeze': True
    }
    
    # ✅ 添加这段：确定训练步数
    if algo == 'RANDOM':
        # RANDOM 只需要运行足够的 episodes 来记录基线
        timesteps = 50000  # 大约 50 个 episodes
    else:
        # 正常算法的训练步数
        timesteps = args.timesteps
    # ✅ 根据模式选择环境配置
    if args.mode == 'test':
        env_config_name = 'waterworld_fast'
        print(f"🏃 测试模式，使用快速环境: max_cycles=500")
    else:
        env_config_name = 'waterworld_standard'
    # 创建训练器
    trainer = MultiAgentTrainer(
        train_side='predator',
        train_algo=algo,
        opponent_config=opponent_config,
        experiment_name=f"{algo}_predator_guided",
        env_config=get_env_config(env_config_name),  # ✅ 使用动态环境
        stage_name=stage_config['stage']['name'],
        generation=stage_config['stage']['generation'],
        version='v1',
        run_mode=args.mode,
        total_timesteps=timesteps,  # ✅ 使用 timesteps 而不是 args.timesteps
        device=args.device
    )
    
    # 获取冻结条件
    freeze_config = stage_config.get('freeze_on_success', {})
    freeze_criteria = freeze_config.get('criteria', {})
    
    # 运行训练
    # RANDOM 不需要保存到池
    save_to_pool = freeze_config.get('enabled', False) if algo != 'RANDOM' else False
    
    eval_results = trainer.run(
        save_to_pool=save_to_pool,
        pool_name=freeze_config.get('save_to_pool'),
        check_freeze=freeze_config.get('enabled', False) if algo != 'RANDOM' else False,
        freeze_criteria=freeze_criteria
    )
    
    return eval_results

def main():
    """主函数"""
    args = parse_args()
    
    # 检查prey_pool是否存在
    prey_pool_path = Path(args.prey_pool)
    if not prey_pool_path.exists():
        print(f"❌ Prey固定池不存在: {prey_pool_path}")
        print(f"请先运行 Stage 1.1 训练")
        sys.exit(1)
    
    # 加载Stage 1.2配置
    stage_config = get_training_config('stage1_2_pred_guided')
    
    # 获取算法列表
    algos_to_train = args.algos or stage_config.get('algorithms_to_train', ['PPO', 'A2C', 'SAC', 'TD3'])
    
    print(f"\n{'='*70}")
    print(f"🎯 Stage 1.2: Predator引导训练")
    print(f"{'='*70}")
    print(f"运行模式: {args.mode}")
    print(f"训练算法: {', '.join(algos_to_train)}")
    print(f"Prey池路径: {args.prey_pool}")
    print(f"{'='*70}\n")
    
    # ========== 新增：先运行 RANDOM 作为基线 ==========
    print(f"\n{'='*70}")
    print(f"步骤 0: 记录 RANDOM Baseline")
    print(f"{'='*70}\n")
    
    # 将 RANDOM 添加到训练列表的最前面
    all_algos = ['RANDOM'] + algos_to_train
    
    # ========== 训练所有算法（包括 RANDOM）==========
    results = {}
    for algo in all_algos:
        try:
            eval_results = train_one_predator_algo(algo, args, stage_config)
            results[algo] = eval_results
        except KeyboardInterrupt:
            print(f"\n⚠️  {algo} 训练被中断")
            break
        except Exception as e:
            print(f"\n❌ {algo} 训练失败: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ========== 打印汇总（突出显示与 RANDOM 的对比）==========
    print(f"\n{'='*70}")
    print(f"✅ Stage 1.2 完成")
    print(f"{'='*70}")
    
    # 显示 RANDOM 基线
    if 'RANDOM' in results and results['RANDOM']:
        baseline_reward = results['RANDOM'].get('mean_reward', 0)
        print(f"\n📊 Random Baseline: {baseline_reward:.2f}")
        
        print(f"\n训练结果（相对于 Random）:")
        for algo in algos_to_train:  # 只显示训练的算法
            if algo in results and results[algo]:
                reward = results[algo].get('mean_reward', 0)
                improvement = ((reward - baseline_reward) / abs(baseline_reward) * 100) if baseline_reward != 0 else 0
                status = "✅" if improvement > 0 else "❌"
                print(f"  {status} {algo}: {reward:.2f} ({improvement:+.1f}% vs Random)")
    else:
        print(f"\n训练结果:")
        for algo, result in results.items():
            if result:
                reward = result.get('mean_reward', 0)
                if isinstance(reward, (int, float)):
                    print(f"  {algo}: 平均奖励 = {reward:.2f}")
                else:
                    print(f"  {algo}: 平均奖励 = {reward}")
    
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/training/train_stage1_3.py
================================================================================
"""
Stage 1.3: 共进化训练
Predator和Prey交替训练，共同进化
"""

import argparse
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config, get_env_config  # ← 加上 get_env_config


def parse_args():
    """解析命令行参数"""
    parser = argparse.ArgumentParser(description='Stage 1.3: 共进化训练')
    
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun','test',  'prod'],
                        help='运行模式')
    
    parser.add_argument('--max-generations', type=int, default=20,
                        help='最大代数')
    
    parser.add_argument('--start-generation', type=int, default=2,
                        help='起始代数')
    
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3'],
                        help='要训练的算法列表')
    
    parser.add_argument('--device', type=str, default='auto',
                        help='计算设备')
    
    parser.add_argument('--timesteps-per-gen', type=int, default=None,
                        help='每代的训练步数')
    
    return parser.parse_args()


def train_one_generation(
    generation: int,
    train_side: str,
    opponent_pool_path: str,
    algos: list,
    args,
    stage_config: dict
):
    """训练一代"""
    
    print(f"\n{'='*70}")
    print(f"🔄 Generation {generation}: 训练 {train_side.upper()}")
    print(f"{'='*70}\n")
    
    results = {}
    
    for algo in algos:
        print(f"\n{'-'*70}")
        print(f"训练 {algo}_{train_side} (Gen {generation})")
        print(f"{'-'*70}\n")
        
        # 构建对手配置
        opponent_side = 'prey' if train_side == 'predator' else 'predator'
        opponent_config = {
            'type': 'mixed_pool',
            'side': opponent_side,
            'pool_path': opponent_pool_path,
            'mix_strategy': {
                'fixed_ratio': 0.7,
                'sampling': 'uniform'
            },
            'freeze': True
        }
        
        # ✅ 根据模式选择环境配置
        if args.mode == 'test':
            env_config_name = 'waterworld_fast'
            print(f"🏃 测试模式，使用快速环境: max_cycles=500")
        else:
            env_config_name = 'waterworld_standard'
        
        # 创建训练器
        trainer = MultiAgentTrainer(
            train_side=train_side,
            train_algo=algo,
            opponent_config=opponent_config,
            experiment_name=f"{algo}_{train_side}_coevo",
            stage_name=f"{stage_config['stage']['name']}/Gen_{generation}",
            generation=generation,
            version=f"v{generation}",
            run_mode=args.mode,
            env_config=get_env_config(env_config_name),  # ✅ 使用动态环境
            total_timesteps=args.timesteps_per_gen,
            device=args.device
        )
        
        # 获取冻结条件
        freeze_config = stage_config.get('freeze_on_success', {})
        freeze_criteria = freeze_config.get('criteria', {}).get(train_side, {})
        
        # 运行训练
        try:
            eval_results = trainer.run(
                save_to_pool=freeze_config.get('enabled', False),
                pool_name=f"{train_side}_pool_v{generation}",
                check_freeze=freeze_config.get('enabled', False),
                freeze_criteria=freeze_criteria
            )
            results[algo] = eval_results
        
        except Exception as e:
            print(f"\n❌ {algo} 训练失败: {e}")
            import traceback
            traceback.print_exc()
            results[algo] = None
    
    return results


def check_convergence(generation_results: list, threshold: float = 0.03) -> bool:
    """
    检查是否收敛
    
    Args:
        generation_results: 最近几代的结果列表
        threshold: 性能变化阈值
    
    Returns:
        是否收敛
    """
    if len(generation_results) < 5:
        return False
    
    # 取最近5代的平均奖励
    recent_rewards = []
    for gen_result in generation_results[-5:]:
        rewards = [r.get('mean_reward', 0) for r in gen_result.values() if r]
        if rewards:
            recent_rewards.append(sum(rewards) / len(rewards))
    
    if len(recent_rewards) < 5:
        return False
    
    # 计算变化率
    mean_reward = sum(recent_rewards) / len(recent_rewards)
    max_deviation = max(abs(r - mean_reward) for r in recent_rewards)
    
    change_rate = max_deviation / (abs(mean_reward) + 1e-6)
    
    print(f"\n📊 收敛检查:")
    print(f"  最近5代平均奖励: {recent_rewards}")
    print(f"  均值: {mean_reward:.2f}")
    print(f"  最大偏差: {max_deviation:.2f}")
    print(f"  变化率: {change_rate:.2%}")
    print(f"  阈值: {threshold:.2%}")
    
    return change_rate < threshold


def main():
    """主函数"""
    args = parse_args()
    # ✅ 根据运行模式选择配置文件
    if args.mode == 'test':
        config_name = 'stage1_3_coevolution_test'  # 使用测试配置
        print("🧪 TEST模式：使用测试配置（1代，2算法）")
    else:
        config_name = 'stage1_3_coevolution'        # 使用正式配置
    # 检查初始池是否存在
    prey_pool_path = Path("outputs/fixed_pools/prey_pool_v1")
    pred_pool_path = Path("outputs/fixed_pools/predator_pool_v1")
    
    if not prey_pool_path.exists() or not pred_pool_path.exists():
        print("❌ 初始对手池不存在")
        print(f"  Prey池: {prey_pool_path} - {'✓' if prey_pool_path.exists() else '✗'}")
        print(f"  Predator池: {pred_pool_path} - {'✓' if pred_pool_path.exists() else '✗'}")
        print("\n请先运行 Stage 1.1 和 Stage 1.2")
        sys.exit(1)
    
    # 加载Stage 1.3配置
    stage_config = get_training_config(config_name)
    
    # 获取配置
    coevo_config = stage_config.get('coevolution', {})
    max_generations = args.max_generations or coevo_config.get('max_generations', 20)
    start_generation = args.start_generation or coevo_config.get('start_generation', 2)
    
    algos_to_train = args.algos or stage_config.get('algorithms_to_train', ['PPO', 'A2C', 'SAC', 'TD3'])
    
    print(f"\n{'='*70}")
    print(f"🎯 Stage 1.3: 共进化训练")
    print(f"{'='*70}")
    print(f"运行模式: {args.mode}")
    print(f"训练算法: {', '.join(algos_to_train)}")
    print(f"代数范围: {start_generation} - {max_generations}")
    print(f"{'='*70}\n")
    
    # 记录所有代的结果
    all_results = []
    
    # 共进化循环
    for generation in range(start_generation, max_generations + 1):
        
        # 奇偶代交替训练
        if generation % 2 == 0:
            # 偶数代：训练Predator
            train_side = 'predator'
            opponent_pool = str(prey_pool_path)
        else:
            # 奇数代：训练Prey
            train_side = 'prey'
            opponent_pool = str(pred_pool_path)
        
        # 训练当前代
        try:
            gen_results = train_one_generation(
                generation=generation,
                train_side=train_side,
                opponent_pool_path=opponent_pool,
                algos=algos_to_train,
                args=args,
                stage_config=stage_config
            )
            
            all_results.append(gen_results)
        
        except KeyboardInterrupt:
            print(f"\n⚠️  Generation {generation} 被中断")
            break
        
        except Exception as e:
            print(f"\n❌ Generation {generation} 失败: {e}")
            import traceback
            traceback.print_exc()
            break
        
        # 检查收敛
        convergence_config = coevo_config.get('convergence', {})
        if convergence_config.get('enabled', True):
            if check_convergence(
                all_results,
                threshold=convergence_config.get('performance_change_threshold', 0.03)
            ):
                print(f"\n✅ 在 Generation {generation} 达到收敛")
                break
    
    # 打印最终汇总
    print(f"\n{'='*70}")
    print(f"✅ Stage 1.3 完成")
    print(f"{'='*70}")
    print(f"总共训练了 {len(all_results)} 代")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/evaluation/cross_eval.py
================================================================================
"""
交叉评估主脚本
执行 5×5 算法对战矩阵评估
"""

import sys
import argparse
import pickle
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List

# 添加项目根目录
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from eval_single_matchup import evaluate_single_matchup
from metrics_calculator import MetricsCalculator
from src.utils.config_loader import config_loader


def parse_args():
    parser = argparse.ArgumentParser(description='交叉评估')
    parser.add_argument('--mode', type=str, default='test',
                        choices=['test', 'dryrun', 'prod'],
                        help='评估模式')
    parser.add_argument('--n-episodes', type=int, default=None,
                        help='每个组合的评估episode数')
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3', 'RANDOM'],
                        help='要评估的算法列表')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='输出目录')
    return parser.parse_args()


def find_model_path(base_dir: Path, algo: str, side: str) -> Path:
    """
    查找模型文件
    
    优先级：
    1. 最新的模型（按时间戳）
    2. 固定池中的模型
    """
    # 搜索路径
    search_patterns = [
        base_dir / f"stage1.*_{side}_*" / f"{algo}_{side}_*.zip",
        base_dir / f"*{side}*" / f"{algo}_{side}_*.zip",
        base_dir / f"**/{algo}_{side}_*.zip",
    ]
    
    found_models = []
    for pattern in search_patterns:
        found_models.extend(base_dir.glob(str(pattern.relative_to(base_dir))))
    
    if not found_models:
        return None
    
    # 返回最新的
    found_models.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return found_models[0]


def main():
    args = parse_args()
    
    # 加载配置
    eval_config = config_loader.load_yaml('evaluation/cross_eval.yaml')
    mode_config = eval_config['evaluation']['model_pools'][args.mode]
    
    # 参数
    algos = args.algos
    n_episodes = args.n_episodes or eval_config['evaluation']['n_episodes']
    output_dir = Path(args.output_dir) if args.output_dir else \
                 Path(f"{args.mode}_outputs/evaluation_results")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # 保存目录
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = output_dir / f"cross_eval_{timestamp}"
    run_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*70)
    print("🧪 交叉评估 (Cross-Algorithm Evaluation)")
    print("="*70)
    print(f"模式:        {args.mode}")
    print(f"算法:        {', '.join(algos)}")
    print(f"Episodes:    {n_episodes}")
    print(f"输出目录:    {run_dir}")
    print("="*70 + "\n")
    
    # 查找所有模型
    print("📂 查找模型文件...")
    
    saved_models_base = Path(mode_config['saved_models_base'])
    
    predator_models = {}
    prey_models = {}
    
    for algo in algos:
        if algo == 'RANDOM':
            predator_models[algo] = None
            prey_models[algo] = None
            print(f"  ✓ {algo}: 使用随机策略")
            continue
        
        # 查找 predator
        pred_path = find_model_path(saved_models_base, algo, 'predator')
        if pred_path:
            predator_models[algo] = pred_path
            print(f"  ✓ {algo}_predator: {pred_path.name}")
        else:
            print(f"  ✗ {algo}_predator: 未找到")
            predator_models[algo] = None
        
        # 查找 prey
        prey_path = find_model_path(saved_models_base, algo, 'prey')
        if prey_path:
            prey_models[algo] = prey_path
            print(f"  ✓ {algo}_prey: {prey_path.name}")
        else:
            print(f"  ✗ {algo}_prey: 未找到")
            prey_models[algo] = None
    
    # 执行交叉评估
    print(f"\n{'='*70}")
    print(f"🎯 开始交叉评估 ({len(algos)}×{len(algos)} = {len(algos)**2} 组合)")
    print(f"{'='*70}\n")
    
    results_matrix = {}
    total_matchups = len(algos) * len(algos)
    current_matchup = 0
    
    for pred_algo in algos:
        results_matrix[pred_algo] = {}
        
        for prey_algo in algos:
            current_matchup += 1
            print(f"\n[{current_matchup}/{total_matchups}] "
                  f"评估: {pred_algo}_pred vs {prey_algo}_prey")
            print("-" * 70)
            
            # 获取模型路径
            pred_path = predator_models.get(pred_algo)
            prey_path = prey_models.get(prey_algo)
            
            # 检查模型是否存在
            if pred_path is None and pred_algo != 'RANDOM':
                print(f"  ⚠️  跳过: {pred_algo}_predator 模型未找到")
                results_matrix[pred_algo][prey_algo] = None
                continue
            
            if prey_path is None and prey_algo != 'RANDOM':
                print(f"  ⚠️  跳过: {prey_algo}_prey 模型未找到")
                results_matrix[pred_algo][prey_algo] = None
                continue
            
            # 执行评估
            try:
                metrics = evaluate_single_matchup(
                    predator_model_path=pred_path,
                    prey_model_path=prey_path,
                    predator_algo=pred_algo,
                    prey_algo=prey_algo,
                    env_config_name='waterworld_fast',
                    n_episodes=n_episodes,
                    deterministic=True,
                    verbose=1
                )
                
                results_matrix[pred_algo][prey_algo] = metrics
                
            except Exception as e:
                print(f"  ❌ 评估失败: {e}")
                import traceback
                traceback.print_exc()
                results_matrix[pred_algo][prey_algo] = None
    
    # 保存原始结果
    print(f"\n{'='*70}")
    print("💾 保存结果...")
    print(f"{'='*70}")
    
    # 保存为pickle（完整数据）
    raw_results_path = run_dir / "raw_results.pkl"
    with open(raw_results_path, 'wb') as f:
        pickle.dump(results_matrix, f)
    print(f"  ✓ 原始结果: {raw_results_path}")
    
    # 保存为JSON（主要指标）
    json_results = {}
    for pred_algo, prey_dict in results_matrix.items():
        json_results[pred_algo] = {}
        for prey_algo, metrics in prey_dict.items():
            if metrics is not None:
                # 只保存主要指标
                json_results[pred_algo][prey_algo] = {
                    'catch_rate': metrics['catch_rate'],
                    'survival_rate': metrics['survival_rate'],
                    'pred_avg_reward': metrics['pred_avg_reward'],
                    'prey_avg_reward': metrics['prey_avg_reward'],
                    'balance_score': metrics['balance_score'],
                    'is_ood': metrics['is_ood']
                }
            else:
                json_results[pred_algo][prey_algo] = None
    
    json_path = run_dir / "results_summary.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    print(f"  ✓ JSON摘要: {json_path}")
    
    # 计算自适应性得分
    print(f"\n{'='*70}")
    print("📊 计算自适应性得分...")
    print(f"{'='*70}\n")
    
    # 过滤掉None结果
    valid_results = {}
    for pred_algo, prey_dict in results_matrix.items():
        if pred_algo == 'RANDOM':
            continue
        valid_results[pred_algo] = {}
        for prey_algo, metrics in prey_dict.items():
            if metrics is not None:
                valid_results[pred_algo][prey_algo] = metrics
    
    if valid_results:
        adaptability_scores = MetricsCalculator.compute_adaptability_scores(valid_results)
        ranking = MetricsCalculator.compute_ranking(adaptability_scores)
        
        # 打印排名
        print(f"{'='*80}")
        print("Algorithm Adaptability Ranking (Predator)")
        print(f"{'='*80}")
        print(f"{'Rank':<6} {'Algorithm':<10} {'In-Dist':<10} {'OOD Avg':<10} "
              f"{'Adapt':<8} {'Drop':<8} {'Std':<8}")
        print("-"*80)
        
        for scores in ranking:
            print(f"{scores['rank']:<6} {scores['algorithm']:<10} "
                  f"{scores['in_dist_performance']:<10.3f} "
                  f"{scores['ood_avg_performance']:<10.3f} "
                  f"{scores['adaptability_score']:<8.3f} "
                  f"{scores['performance_drop']:<8.3f} "
                  f"{scores['ood_std']:<8.3f}")
        
        print(f"{'='*80}\n")
        
        # 保存自适应性得分
        adapt_path = run_dir / "adaptability_scores.json"
        with open(adapt_path, 'w', encoding='utf-8') as f:
            json.dump({
                'scores': adaptability_scores,
                'ranking': ranking
            }, f, indent=2, ensure_ascii=False)
        print(f"  ✓ 自适应性得分: {adapt_path}")
    
    # 生成性能矩阵（CSV格式）
    print(f"\n{'='*70}")
    print("📋 生成性能矩阵...")
    print(f"{'='*70}\n")
    
    # Catch Rate矩阵
    catch_rate_matrix = []
    header = ['Predator\\Prey'] + algos
    catch_rate_matrix.append(header)
    
    for pred_algo in algos:
        row = [pred_algo]
        for prey_algo in algos:
            metrics = results_matrix[pred_algo].get(prey_algo)
            if metrics:
                row.append(f"{metrics['catch_rate']:.3f}")
            else:
                row.append("N/A")
        catch_rate_matrix.append(row)
    
    # 保存为CSV
    csv_path = run_dir / "catch_rate_matrix.csv"
    import csv
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerows(catch_rate_matrix)
    print(f"  ✓ Catch Rate矩阵: {csv_path}")
    
    # 打印矩阵预览
    print("\n  Catch Rate Matrix Preview:")
    print("  " + "-" * 60)
    for row in catch_rate_matrix[:6]:  # 只显示前6行
        print("  " + " | ".join(f"{cell:>12}" for cell in row))
    print("  " + "-" * 60)
    
    # 保存配置快照
    config_snapshot = {
        'mode': args.mode,
        'algorithms': algos,
        'n_episodes': n_episodes,
        'timestamp': timestamp,
        'model_paths': {
            'predators': {k: str(v) if v else None for k, v in predator_models.items()},
            'preys': {k: str(v) if v else None for k, v in prey_models.items()}
        }
    }
    
    config_path = run_dir / "eval_config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_snapshot, f, indent=2, ensure_ascii=False)
    print(f"  ✓ 配置快照: {config_path}")
    
    # 完成
    print(f"\n{'='*70}")
    print("✅ 交叉评估完成！")
    print(f"{'='*70}")
    print(f"\n📁 结果保存在: {run_dir}")
    print("\n下一步：")
    print(f"  1. 查看结果: cat {json_path}")
    print(f"  2. 生成可视化: python scripts/analysis/plot_results.py --input {run_dir}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/evaluation/eval_random_baseline.py
================================================================================
"""
Random Baseline 评估脚本
在训练前先评估随机策略的性能，建立基线
"""

import argparse
import sys
import json
from pathlib import Path
from datetime import datetime
import numpy as np

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config


def parse_args():
    """解析命令行参数"""
    parser = argparse.ArgumentParser(description='Random Baseline 评估')
    
    parser.add_argument('--stage', type=str, required=True,
                        choices=['1.1', '1.2'],
                        help='训练阶段（1.1=Prey预热, 1.2=Predator引导）')
    
    parser.add_argument('--n-episodes', type=int, default=20,
                        help='评估episode数')
    
    parser.add_argument('--env-config', type=str, default='waterworld_standard',
                        help='环境配置')
    
    parser.add_argument('--device', type=str, default='cpu',
                        help='计算设备')
    
    parser.add_argument('--output-dir', type=str, default='outputs/baselines',
                        help='结果保存目录')
    
    return parser.parse_args()


def evaluate_random_baseline(stage: str, args, stage_config: dict):
    """
    评估 Random Baseline
    
    Args:
        stage: 训练阶段
        args: 命令行参数
        stage_config: 阶段配置
    """
    
    print(f"\n{'='*70}")
    print(f"🎲 Random Baseline 评估 - Stage {stage}")
    print(f"{'='*70}\n")
    
    # 根据阶段确定训练方和对手
    if stage == '1.1':
        train_side = 'prey'
        opponent_config = {
            'type': 'algorithm',
            'side': 'predator',
            'algorithm': 'RANDOM',
            'freeze': True
        }
        scenario = "Random Prey vs Random Predator"
    
    elif stage == '1.2':
        train_side = 'predator'
        prey_pool = 'outputs/fixed_pools/prey_pool_v1'
        
        # 检查 prey_pool 是否存在
        if not Path(prey_pool).exists():
            print(f"❌ Prey池不存在: {prey_pool}")
            print(f"   请先运行 Stage 1.1 训练")
            sys.exit(1)
        
        opponent_config = {
            'type': 'mixed_pool',
            'side': 'prey',
            'pool_path': prey_pool,
            'mix_strategy': {
                'fixed_ratio': 0.7,
                'sampling': 'uniform'
            },
            'freeze': True
        }
        scenario = "Random Predator vs prey_pool_v1"
    
    else:
        raise ValueError(f"未知的阶段: {stage}")
    
    print(f"场景: {scenario}")
    print(f"评估Episodes: {args.n_episodes}\n")
    
    # 创建训练器（使用 RANDOM 算法）
    trainer = MultiAgentTrainer(
        train_side=train_side,
        train_algo='RANDOM',
        opponent_config=opponent_config,
        experiment_name=f"random_baseline_stage{stage}",
        stage_name=f"baseline_stage{stage}",
        generation=0,
        version='baseline',
        run_mode='dryrun',  # 使用 dryrun 模式
        total_timesteps=0,  # 不训练，只评估
        device=args.device
    )
    
    # 设置环境
    trainer.setup()
    
    print(f"开始评估...")
    print(f"{'-'*70}\n")
    
    # 评估
    eval_results = trainer.evaluate(n_episodes=args.n_episodes)
    
    # 清理
    trainer.cleanup()
    
    return eval_results, scenario


def save_baseline_results(stage: str, scenario: str, results: dict, output_dir: Path):
    """保存基线结果"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"random_baseline_stage{stage}_{timestamp}.json"
    filepath = output_dir / filename
    
    # 构建完整记录
    record = {
        'timestamp': datetime.now().isoformat(),
        'stage': stage,
        'scenario': scenario,
        'baseline_type': 'RANDOM',
        'results': results
    }
    
    # 保存
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(record, f, indent=2, ensure_ascii=False)
    
    print(f"\n💾 基线结果已保存: {filepath}")
    
    return filepath


def print_baseline_summary(stage: str, scenario: str, results: dict):
    """打印基线摘要"""
    print(f"\n{'='*70}")
    print(f"📊 Random Baseline 结果 - Stage {stage}")
    print(f"{'='*70}")
    print(f"\n场景: {scenario}")
    print(f"\n性能指标:")
    print(f"  平均奖励: {results['mean_reward']:.2f} ± {results['std_reward']:.2f}")
    print(f"  最大奖励: {results['max_reward']:.2f}")
    print(f"  最小奖励: {results['min_reward']:.2f}")
    print(f"  平均Episode长度: {results['mean_length']:.0f} ± {results['std_length']:.0f}")
    print(f"  评估Episodes: {results['n_episodes']}")
    print(f"\n{'='*70}")
    print(f"\n💡 提示:")
    print(f"   - 训练后的算法应该显著超过这个基线")
    print(f"   - 如果训练后性能 < 基线，说明训练可能有问题")
    print(f"   - 建议提升幅度: > 20% 为良好，> 50% 为优秀")
    print(f"{'='*70}\n")


def main():
    """主函数"""
    args = parse_args()
    
    # 加载阶段配置
    if args.stage == '1.1':
        stage_config = get_training_config('stage1_1_prey_warmup')
    elif args.stage == '1.2':
        stage_config = get_training_config('stage1_2_pred_guided')
    else:
        raise ValueError(f"未知的阶段: {args.stage}")
    
    # 评估
    eval_results, scenario = evaluate_random_baseline(args.stage, args, stage_config)
    
    # 保存结果
    save_baseline_results(args.stage, scenario, eval_results, args.output_dir)
    
    # 打印摘要
    print_baseline_summary(args.stage, scenario, eval_results)


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/evaluation/eval_single_matchup.py
================================================================================
"""
单次对战评估
评估一对 Predator vs Prey 的性能
"""

import sys
from pathlib import Path
import numpy as np
from typing import Dict, Any

# 添加项目根目录到路径
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.environment import WaterworldEnvManager, create_training_env
from src.core.opponent_pool import create_opponent_policies
from src.core.agent_manager import AgentManager
from src.utils.config_loader import get_env_config


def evaluate_single_matchup(
    predator_model_path: Path,
    prey_model_path: Path,
    predator_algo: str,
    prey_algo: str,
    env_config_name: str = "waterworld_fast",
    n_episodes: int = 20,
    deterministic: bool = True,
    verbose: int = 1
) -> Dict[str, Any]:
    """
    评估单次Predator vs Prey对战
    
    Args:
        predator_model_path: Predator模型路径
        prey_model_path: Prey模型路径
        predator_algo: Predator算法名称
        prey_algo: Prey算法名称
        env_config_name: 环境配置名称
        n_episodes: 评估episode数
        deterministic: 是否使用确定性策略
        verbose: 详细程度
    
    Returns:
        评估指标字典
    """
    
    if verbose > 0:
        print(f"\n{'='*70}")
        print(f"Evaluating: {predator_algo}_pred vs {prey_algo}_prey")
        print(f"{'='*70}")
        # ✅ 修复：处理 None 的情况
        pred_model_name = predator_model_path.name if predator_model_path else "RANDOM (no model)"
        prey_model_name = prey_model_path.name if prey_model_path else "RANDOM (no model)"
        print(f"  Predator Model: {pred_model_name}")
        print(f"  Prey Model:     {prey_model_name}")
        print(f"  Episodes:       {n_episodes}")
    
    # 1. 加载环境配置
    env_config = get_env_config(env_config_name)
    env_manager = WaterworldEnvManager(env_config)
    env_manager.create_env()
    
    # 2. 获取空间信息
    pred_obs_space = env_manager.get_observation_space('predator')
    pred_action_space = env_manager.get_action_space('predator')
    
    prey_obs_space = env_manager.get_observation_space('prey')
    prey_action_space = env_manager.get_action_space('prey')
    
    # 3. 加载模型
    try:
        if predator_algo == 'RANDOM':
            predator_model = AgentManager.create_random_agent(
                pred_obs_space, pred_action_space
            )
        else:
            predator_model = AgentManager.load_agent(
                predator_model_path, pred_obs_space, pred_action_space
            )
        
        if prey_algo == 'RANDOM':
            prey_model = AgentManager.create_random_agent(
                prey_obs_space, prey_action_space
            )
        else:
            prey_model = AgentManager.load_agent(
                prey_model_path, prey_obs_space, prey_action_space
            )
    
    except Exception as e:
        print(f"❌ 模型加载失败: {e}")
        return None
    
    # 4. 创建评估环境（单环境，便于精确控制）
    # 注意：这里我们需要一个特殊的评估环境，双方都用固定策略
    
    # 5. 运行评估
    episode_results = []
    
    for ep in range(n_episodes):
        obs, info = env_manager.reset(seed=42 + ep)
        done = False
        step = 0
        
        ep_data = {
            'predator_rewards': [],
            'prey_rewards': [],
            'predator_dones': [],
            'prey_dones': [],
            'episode_length': 0
        }
        
        while not done and step < 500:  # 最多500步
            # 获取所有智能体的动作
            actions = {}
            
            for agent_id in env_manager.env.agents:
                if 'predator' in agent_id:
                    obs_agent = obs.get(agent_id, np.zeros(pred_obs_space.shape))
                    action, _ = predator_model.predict(obs_agent, deterministic=deterministic)
                    actions[agent_id] = action
                
                elif 'prey' in agent_id:
                    obs_agent = obs.get(agent_id, np.zeros(prey_obs_space.shape))
                    action, _ = prey_model.predict(obs_agent, deterministic=deterministic)
                    actions[agent_id] = action
            
            # 执行环境步进
            obs, rewards, terminations, truncations, infos = env_manager.env.step(actions)
            
            # 记录数据
            pred_rewards = [rewards.get(a, 0) for a in env_manager.env.possible_agents if 'predator' in a]
            prey_rewards = [rewards.get(a, 0) for a in env_manager.env.possible_agents if 'prey' in a]
            
            ep_data['predator_rewards'].extend(pred_rewards)
            ep_data['prey_rewards'].extend(prey_rewards)
            
            # 记录死亡事件
            pred_dones = [terminations.get(a, False) for a in env_manager.env.possible_agents if 'predator' in a]
            prey_dones = [terminations.get(a, False) for a in env_manager.env.possible_agents if 'prey' in a]
            
            ep_data['predator_dones'].extend(pred_dones)
            ep_data['prey_dones'].extend(prey_dones)
            
            step += 1
            
            # 检查是否所有智能体都结束
            if len(env_manager.env.agents) == 0:
                done = True
        
        ep_data['episode_length'] = step
        episode_results.append(ep_data)
        
        if verbose > 1:
            print(f"  Episode {ep+1}/{n_episodes}: "
                  f"Length={step}, "
                  f"PredReward={np.mean(ep_data['predator_rewards']):.2f}, "
                  f"PreyReward={np.mean(ep_data['prey_rewards']):.2f}")
    
    # 6. 计算聚合指标
    from metrics_calculator import MetricsCalculator
    
    episode_metrics = [
        MetricsCalculator.compute_episode_metrics(ep_data)
        for ep_data in episode_results
    ]
    
    aggregated_metrics = MetricsCalculator.aggregate_metrics(episode_metrics)
    
    # 7. 添加元数据
    aggregated_metrics['predator_algo'] = predator_algo
    aggregated_metrics['prey_algo'] = prey_algo
    aggregated_metrics['is_ood'] = (predator_algo != prey_algo) and (prey_algo != 'RANDOM')
    
    # 8. 清理
    env_manager.close()
    
    if verbose > 0:
        print(f"\n  Results:")
        print(f"    Catch Rate:     {aggregated_metrics['catch_rate']:.3f}")
        print(f"    Survival Rate:  {aggregated_metrics['survival_rate']:.3f}")
        print(f"    Pred Reward:    {aggregated_metrics['pred_avg_reward']:+.2f}")
        print(f"    Prey Reward:    {aggregated_metrics['prey_avg_reward']:+.2f}")
        print(f"    Balance Score:  {aggregated_metrics['balance_score']:.3f}")
        print(f"    Is OOD:         {aggregated_metrics['is_ood']}")
        print(f"{'='*70}\n")
    
    return aggregated_metrics


if __name__ == "__main__":
    # 测试单次评估
    import argparse
    
    parser = argparse.ArgumentParser(description='单次对战评估')
    parser.add_argument('--pred-model', type=str, required=True, help='Predator模型路径')
    parser.add_argument('--prey-model', type=str, required=True, help='Prey模型路径')
    parser.add_argument('--pred-algo', type=str, required=True, help='Predator算法')
    parser.add_argument('--prey-algo', type=str, required=True, help='Prey算法')
    parser.add_argument('--n-episodes', type=int, default=20, help='评估episode数')
    
    args = parser.parse_args()
    
    result = evaluate_single_matchup(
        predator_model_path=Path(args.pred_model),
        prey_model_path=Path(args.prey_model),
        predator_algo=args.pred_algo,
        prey_algo=args.prey_algo,
        n_episodes=args.n_episodes
    )
    
    print("\n最终结果:")
    for key, value in result.items():
        print(f"  {key}: {value}")

================================================================================
FILE: scripts/evaluation/metrics_calculator.py
================================================================================
"""
指标计算器
计算交叉评估中的各种性能指标
"""

import numpy as np
from typing import Dict, List, Any


class MetricsCalculator:
    """评估指标计算器"""
    
    @staticmethod
    def compute_episode_metrics(episode_data: Dict) -> Dict[str, float]:
        """
        计算单个episode的指标
        
        Args:
            episode_data: {
                'predator_rewards': [...],
                'prey_rewards': [...],
                'predator_dones': [...],
                'prey_dones': [...],
                'episode_length': int
            }
        
        Returns:
            指标字典
        """
        metrics = {}
        
        # Predator指标
        pred_rewards = np.array(episode_data['predator_rewards'])
        pred_dones = np.array(episode_data['predator_dones'])
        
        metrics['pred_total_reward'] = float(np.sum(pred_rewards))
        metrics['pred_avg_reward'] = float(np.mean(pred_rewards))
        
        # 捕获事件（假设prey死亡时predator获得正奖励）
        prey_dones = np.array(episode_data['prey_dones'])
        metrics['n_catches'] = int(np.sum(prey_dones))
        
        # Prey指标
        prey_rewards = np.array(episode_data['prey_rewards'])
        metrics['prey_total_reward'] = float(np.sum(prey_rewards))
        metrics['prey_avg_reward'] = float(np.mean(prey_rewards))
        
        # Episode级指标
        metrics['episode_length'] = episode_data['episode_length']
        metrics['reward_gap'] = metrics['pred_total_reward'] - metrics['prey_total_reward']
        
        return metrics
    
    @staticmethod
    def aggregate_metrics(episode_metrics_list: List[Dict]) -> Dict[str, Any]:
        """
        聚合多个episode的指标
        
        Args:
            episode_metrics_list: 多个episode的指标列表
        
        Returns:
            聚合后的指标
        """
        n_episodes = len(episode_metrics_list)
        
        # 提取各指标的数组
        pred_rewards = [m['pred_avg_reward'] for m in episode_metrics_list]
        prey_rewards = [m['prey_avg_reward'] for m in episode_metrics_list]
        n_catches = [m['n_catches'] for m in episode_metrics_list]
        episode_lengths = [m['episode_length'] for m in episode_metrics_list]
        
        # 聚合统计
        aggregated = {
            'n_episodes': n_episodes,
            
            # Predator指标
            'pred_avg_reward': float(np.mean(pred_rewards)),
            'pred_std_reward': float(np.std(pred_rewards)),
            'pred_min_reward': float(np.min(pred_rewards)),
            'pred_max_reward': float(np.max(pred_rewards)),
            
            # Prey指标
            'prey_avg_reward': float(np.mean(prey_rewards)),
            'prey_std_reward': float(np.std(prey_rewards)),
            'prey_min_reward': float(np.min(prey_rewards)),
            'prey_max_reward': float(np.max(prey_rewards)),
            
            # 对战指标
            'catch_rate': float(np.mean(n_catches)) / 10.0,  # 假设10个prey
            'avg_catches': float(np.mean(n_catches)),
            'std_catches': float(np.std(n_catches)),
            
            'survival_rate': 1.0 - (float(np.mean(n_catches)) / 10.0),
            
            'avg_episode_length': float(np.mean(episode_lengths)),
            'std_episode_length': float(np.std(episode_lengths)),
            
            # 平衡度指标
            'reward_gap': float(np.mean([m['reward_gap'] for m in episode_metrics_list])),
            'balance_score': MetricsCalculator._compute_balance_score(n_catches)
        }
        
        # 额外指标
        aggregated['energy_efficiency'] = (
            aggregated['pred_avg_reward'] / (aggregated['avg_episode_length'] + 1e-6)
        )
        
        aggregated['escape_success'] = aggregated['survival_rate']
        
        # 首次捕获时间（简化版，假设第一次捕获发生在episode中间）
        aggregated['first_catch_time'] = aggregated['avg_episode_length'] / 2.0
        
        # Prey平均寿命
        aggregated['prey_avg_lifespan'] = aggregated['avg_episode_length'] * aggregated['survival_rate']
        
        return aggregated
    
    @staticmethod
    def _compute_balance_score(n_catches_list: List[int]) -> float:
        """
        计算平衡度分数
        
        0.5 = 完美平衡（catch_rate = 50%）
        0.0 = 极度不平衡
        
        Args:
            n_catches_list: 每个episode的捕获数
        
        Returns:
            平衡度分数 [0, 1]
        """
        avg_catches = np.mean(n_catches_list)
        catch_rate = avg_catches / 10.0  # 假设10个prey
        
        # 距离0.5越近，平衡度越高
        deviation = abs(catch_rate - 0.5)
        balance_score = 1.0 - (deviation * 2.0)  # 归一化到[0, 1]
        
        return float(max(0.0, balance_score))
    
    @staticmethod
    def compute_adaptability_scores(results_matrix: Dict) -> Dict[str, Dict]:
        """
        计算所有算法的自适应性得分
        
        Args:
            results_matrix: {
                'PPO': {'PPO': {...}, 'A2C': {...}, ...},
                'A2C': {...},
                ...
            }
        
        Returns:
            自适应性得分字典
        """
        adaptability_scores = {}
        
        for pred_algo in results_matrix.keys():
            if pred_algo == 'RANDOM':
                continue
            
            # In-Distribution性能（对角线）
            in_dist_perf = results_matrix[pred_algo][pred_algo]['catch_rate']
            
            # Out-of-Distribution性能（非对角线，排除RANDOM）
            ood_perfs = []
            for prey_algo in results_matrix[pred_algo].keys():
                if prey_algo != pred_algo and prey_algo != 'RANDOM':
                    ood_perfs.append(results_matrix[pred_algo][prey_algo]['catch_rate'])
            
            ood_avg = float(np.mean(ood_perfs))
            ood_std = float(np.std(ood_perfs))
            
            # 自适应性得分 = OOD保持率
            adaptability = ood_avg / (in_dist_perf + 1e-6)
            
            adaptability_scores[pred_algo] = {
                'algorithm': pred_algo,
                'in_dist_performance': float(in_dist_perf),
                'ood_avg_performance': ood_avg,
                'ood_std': ood_std,
                'adaptability_score': float(adaptability),
                'performance_drop': float(in_dist_perf - ood_avg),
                'ood_performances': ood_perfs  # 原始数据
            }
        
        return adaptability_scores
    
    @staticmethod
    def compute_ranking(adaptability_scores: Dict) -> List[Dict]:
        """
        根据自适应性得分排名
        
        Returns:
            排序后的列表
        """
        scores_list = list(adaptability_scores.values())
        scores_list.sort(key=lambda x: x['adaptability_score'], reverse=True)
        
        # 添加排名
        for i, scores in enumerate(scores_list):
            scores['rank'] = i + 1
        
        return scores_list

================================================================================
FILE: scripts/evaluation/run_cross_eval.py
================================================================================


================================================================================
FILE: src/__init__.py
================================================================================


================================================================================
FILE: src/core/__init__.py
================================================================================
"""
核心模块初始化
"""

from .environment import WaterworldEnvManager, SingleAgentWrapper, create_training_env
from .opponent_pool import OpponentPool, MixedOpponentSampler, create_opponent_policies
from .trainer import MultiAgentTrainer
from .agent_manager import AgentManager


__all__ = [
    'WaterworldEnvManager',
    'SingleAgentWrapper',
    'create_training_env',
    'OpponentPool',
    'MixedOpponentSampler',
    'create_opponent_policies',
    'MultiAgentTrainer',
    'AgentManager'
]

================================================================================
FILE: src/core/agent_manager.py
================================================================================
"""
智能体管理
加载和管理训练好的智能体
"""

from pathlib import Path
from typing import Dict, Any, Optional
import gymnasium as gym

from src.algorithms import create_algorithm
from src.utils.config_loader import get_algo_config


class AgentManager:
    """智能体管理器"""
    
    @staticmethod
    def load_agent(
        model_path: Path,
        observation_space: gym.Space,
        action_space: gym.Space,
        device: str = "auto"
    ):
        """
        加载训练好的智能体
        
        Args:
            model_path: 模型文件路径
            observation_space: 观察空间
            action_space: 动作空间
            device: 计算设备
        
        Returns:
            加载的算法实例
        """
        # 从文件名解析算法名称
        filename = model_path.stem
        parts = filename.split('_')
        
        # 移除可能的前缀（DEBUG_, DRYRUN_）
        if parts[0] in ['DEBUG', 'DRYRUN']:
            parts = parts[1:]
        
        algo_name = parts[0]
        
        # 加载算法配置
        algo_config = get_algo_config(algo_name)
        
        # 创建算法实例
        algorithm = create_algorithm(
            algo_name=algo_name,
            observation_space=observation_space,
            action_space=action_space,
            config=algo_config,
            device=device
        )
        
        # 加载模型权重
        algorithm.load(str(model_path))
        
        return algorithm
    
    @staticmethod
    def create_random_agent(
        observation_space: gym.Space,
        action_space: gym.Space
    ):
        """
        创建随机智能体
        
        Args:
            observation_space: 观察空间
            action_space: 动作空间
        
        Returns:
            随机策略实例
        """
        algo_config = get_algo_config('RANDOM')
        
        algorithm = create_algorithm(
            algo_name='RANDOM',
            observation_space=observation_space,
            action_space=action_space,
            config=algo_config,
            device='cpu'
        )
        
        algorithm.create_model(None)
        
        return algorithm

================================================================================
FILE: src/core/environment.py
================================================================================
"""
环境创建与管理 - 使用 SuperSuit 方案
"""

from typing import Dict, Any, Optional, List
from pettingzoo.sisl import waterworld_v4
import gymnasium as gym
import numpy as np
import supersuit as ss
from stable_baselines3.common.vec_env import VecEnv


class WaterworldEnvManager:
    """Waterworld环境管理器"""
    
    def __init__(self, env_config: Dict[str, Any]):
        """
        初始化环境管理器
        
        Args:
            env_config: 环境配置字典
        """
        self.config = env_config['environment']
        self.env = None
    
    def create_env(self, render_mode: Optional[str] = None):
        """
        创建Waterworld环境
        
        Args:
            render_mode: 渲染模式（None/"rgb_array"/"human"）
        
        Returns:
            PettingZoo环境实例
        """
        if render_mode is None:
            render_mode = self.config.get('render_mode', None)
        
        self.env = waterworld_v4.parallel_env(
            n_predators=self.config.get('n_predators', 5),
            n_preys=self.config.get('n_preys', 10),
            n_evaders=self.config.get('n_evaders', 90),
            n_poisons=self.config.get('n_poisons', 10),
            n_obstacles=self.config.get('n_obstacles', 2),
            obstacle_coord=self.config.get('obstacle_coord', [[0.2, 0.2], [0.8, 0.2]]),
            
            predator_speed=self.config.get('predator_speed', 0.06),
            prey_speed=self.config.get('prey_speed', 0.001),
            
            sensor_range=self.config.get('sensor_range', 0.8),
            thrust_penalty=self.config.get('thrust_penalty', 0.0),
            
            max_cycles=self.config.get('max_cycles', 3000),
            
            render_mode=render_mode
        )
        
        return self.env
    
    def get_observation_space(self, agent_type: str) -> gym.Space:
        """
        获取观察空间
        
        Args:
            agent_type: 智能体类型（predator/prey）
        
        Returns:
            观察空间
        """
        if self.env is None:
            self.create_env()
        
        for agent in self.env.possible_agents:
            if agent_type in agent:
                return self.env.observation_space(agent)
        
        raise ValueError(f"未找到类型为 {agent_type} 的智能体")
    
    def get_action_space(self, agent_type: str) -> gym.Space:
        """
        获取动作空间
        
        Args:
            agent_type: 智能体类型（predator/prey）
        
        Returns:
            动作空间
        """
        if self.env is None:
            self.create_env()
        
        for agent in self.env.possible_agents:
            if agent_type in agent:
                return self.env.action_space(agent)
        
        raise ValueError(f"未找到类型为 {agent_type} 的智能体")
    
    def get_agents_by_type(self, agent_type: str) -> List[str]:
        """
        获取指定类型的所有智能体ID
        
        Args:
            agent_type: 智能体类型（predator/prey）
        
        Returns:
            智能体ID列表
        """
        if self.env is None:
            self.create_env()
        
        return [agent for agent in self.env.possible_agents if agent_type in agent]
    
    def reset(self, seed: Optional[int] = None):
        """重置环境"""
        if self.env is None:
            self.create_env()
        
        return self.env.reset(seed=seed)
    
    def close(self):
        """关闭环境"""
        if self.env is not None:
            self.env.close()
            self.env = None


class MixedAgentVecEnv(VecEnv):
    """
    混合智能体向量化环境
    基于参考代码的成功实现
    """
    
    def __init__(self, venv, train_agent_indices: List[int], opponent_policies: Dict[str, Any]):
        """
        初始化
        
        Args:
            venv: SuperSuit转换后的向量化环境
            train_agent_indices: 训练智能体的索引列表
            opponent_policies: 对手策略字典 {agent_name: policy}
        """
        self.venv = venv
        self.train_indices = train_agent_indices
        self.opponent_policies = opponent_policies
        
        self.n_training = len(train_agent_indices)
        self.n_total = venv.num_envs
        
        # 获取训练智能体的空间
        super().__init__(
            num_envs=self.n_training,
            observation_space=venv.observation_space,
            action_space=venv.action_space
        )
        
        self.latest_obs = None
        
        print(f"  MixedAgentVecEnv: training {self.n_training}/{self.n_total} agents")
    
    def reset(self):
        """重置环境"""
        obs = self.venv.reset()
        self.latest_obs = obs
        
        # 重置对手策略
        for policy in self.opponent_policies.values():
            if hasattr(policy, 'reset'):
                policy.reset()
        
        # 返回训练智能体的观察
        return obs[self.train_indices]
    
    def step_async(self, actions):
        """异步步进"""
        # 构建完整动作数组
        full_actions = np.zeros((self.n_total, self.action_space.shape[0]), dtype=np.float32)
        
        # 填充训练智能体的动作
        for i, train_idx in enumerate(self.train_indices):
            full_actions[train_idx] = actions[i]
        
        # 填充对手智能体的动作
        agent_names = list(self.opponent_policies.keys())
        for i, agent_name in enumerate(agent_names):
            if i not in self.train_indices:
                policy = self.opponent_policies[agent_name]
                obs = self.latest_obs[i] if self.latest_obs is not None else None
                if obs is not None:
                    full_actions[i] = policy.predict(obs, deterministic=False)[0]
                else:
                    full_actions[i] = self.action_space.sample()
        
        self.venv.step_async(full_actions)
    
    def step_wait(self):
        """等待步进结果"""
        obs, rewards, dones, infos = self.venv.step_wait()
        self.latest_obs = obs
        
        # 提取训练智能体的数据
        train_obs = obs[self.train_indices]
        train_rewards = rewards[self.train_indices]
        train_dones = dones[self.train_indices]
        train_infos = [infos[i] for i in self.train_indices]
        
        return train_obs, train_rewards, train_dones, train_infos
    
    def close(self):
        """关闭环境"""
        return self.venv.close()
    
    def get_attr(self, attr_name, indices=None):
        return self.venv.get_attr(attr_name, indices)
    
    def set_attr(self, attr_name, value, indices=None):
        return self.venv.set_attr(attr_name, value, indices)
    
    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):
        return self.venv.env_method(method_name, *method_args, indices=indices, **method_kwargs)
    
    def env_is_wrapped(self, wrapper_class, indices=None):
        """检查环境是否被包装"""
        return self.venv.env_is_wrapped(wrapper_class, indices)


# 简化占位符
class SingleAgentWrapper:
    pass


def create_training_env(
    env_config: Dict[str, Any],
    train_side: str,
    opponent_policies: Dict[str, Any],
    n_envs: int = 1
):
    """
    创建训练环境 - 使用 SuperSuit 方案
    
    Args:
        env_config: 环境配置
        train_side: 训练方（predator/prey）
        opponent_policies: 对手策略字典
        n_envs: 并行环境数量（目前仅支持1）
    
    Returns:
        训练环境
    """
    # 1. 创建基础环境
    env_manager = WaterworldEnvManager(env_config)
    pz_env = env_manager.create_env()
    
    # 2. 使用 SuperSuit 转换（参考成功代码）
    # black_death: 死亡的智能体返回零观察和零奖励
    env = ss.black_death_v3(pz_env)
    
    # 转换为向量化环境
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    
    # 拼接环境（这里 n_envs 总是1）
    env = ss.concat_vec_envs_v1(
        env, 
        num_vec_envs=1, 
        num_cpus=1, 
        base_class='stable_baselines3'
    )
    
    # 3. 确定训练智能体
    all_agents = pz_env.possible_agents
    train_agents = [agent for agent in all_agents if train_side in agent]
    
    if not train_agents:
        raise ValueError(f"没有找到 {train_side} 类型的智能体")
    
    # 使用第一个作为训练智能体
    train_agent_name = train_agents[0]
    train_agent_idx = all_agents.index(train_agent_name)
    
    print(f"  训练智能体: {train_agent_name} (index={train_agent_idx})")
    
    # 4. 应用混合智能体包装器
    env = MixedAgentVecEnv(
        venv=env,
        train_agent_indices=[train_agent_idx],
        opponent_policies=opponent_policies
    )
    
    return env


================================================================================
FILE: src/core/opponent_pool.py
================================================================================
"""
对手池管理
负责固定对手的加载、采样和维护
"""

import os
import json
import random
from pathlib import Path
from typing import Dict, Any, List, Optional
import numpy as np
import gymnasium as gym  # ← 添加这一行！
from src.algorithms import create_algorithm
from src.utils.config_loader import get_algo_config


class OpponentPool:
    """对手池管理器"""
    
    def __init__(self, pool_dir: Optional[Path] = None):
        """
        初始化对手池
        
        Args:
            pool_dir: 池目录路径（如果为None则创建空池）
        """
        self.pool_dir = pool_dir
        self.opponents = []  # 对手列表：[{name, path, policy, metadata}, ...]
        self.metadata = {}
        
        if pool_dir and pool_dir.exists():
            self.load_pool()
    
    def load_pool(self):
        """从目录加载对手池"""
        if not self.pool_dir.exists():
            print(f"⚠️  对手池目录不存在: {self.pool_dir}")
            return
        
        # 加载元数据
        metadata_path = self.pool_dir / "metadata.json"
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        
        # 加载所有模型
        for model_file in self.pool_dir.glob("*.zip"):
            opponent_info = self._load_opponent(model_file)
            if opponent_info:
                self.opponents.append(opponent_info)
        
        print(f"✅ 从 {self.pool_dir} 加载了 {len(self.opponents)} 个对手")
    
    def _load_opponent(self, model_path: Path) -> Optional[Dict[str, Any]]:
        """
        加载单个对手
        
        Args:
            model_path: 模型文件路径
        
        Returns:
            对手信息字典，加载失败返回None
        """
        try:
            # 从文件名解析信息
            filename = model_path.stem  # 去掉.zip
            
            # 假设文件名格式: ALGO_side_version
            parts = filename.split('_')
            if len(parts) < 3:
                print(f"⚠️  无法解析文件名: {filename}")
                return None
            
            algo_name = parts[0]
            side = parts[1]
            version = parts[2]
            
            # 从metadata中获取详细信息
            model_metadata = {}
            for model_info in self.metadata.get('models', []):
                if model_info.get('name') == filename:
                    model_metadata = model_info
                    break
            
            # 加载算法配置
            try:
                algo_config = get_algo_config(algo_name)
            except:
                print(f"⚠️  无法加载算法配置: {algo_name}")
                return None
            
            # 创建算法实例（暂不加载模型，延迟加载）
            opponent_info = {
                'name': filename,
                'algo': algo_name,
                'side': side,
                'version': version,
                'path': model_path,
                'policy': None,  # 延迟加载
                'config': algo_config,
                'metadata': model_metadata
            }
            
            return opponent_info
        
        except Exception as e:
            print(f"❌ 加载对手失败 {model_path}: {e}")
            return None
    
    def get_opponent_policy(
        self, 
        opponent_info: Dict[str, Any], 
        device: str = "auto",
        observation_space: gym.Space = None,
        action_space: gym.Space = None
    ):
        """
        获取对手策略（延迟加载）
        
        Args:
            opponent_info: 对手信息
            device: 计算设备
        
        Returns:
            加载的策略
        """
        # 如果已经加载过，直接返回
        if opponent_info['policy'] is not None:
            return opponent_info['policy']
        
        # ✅ 创建算法实例（使用传入的空间信息）
        algo = create_algorithm(
            algo_name=opponent_info['algo'],
            observation_space=observation_space,  # 使用传入的参数
            action_space=action_space,             # 使用传入的参数
            config=opponent_info['config'],
            device=device
        )
        
        # 加载模型权重
        algo.load(str(opponent_info['path']))
        
        # 缓存
        opponent_info['policy'] = algo
        
        return algo
    
    def sample_opponents(
        self,
        n: int,
        strategy: str = "uniform",
        exclude: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        从池中采样对手
        
        Args:
            n: 采样数量
            strategy: 采样策略（uniform/weighted）
            exclude: 要排除的对手名称列表
        
        Returns:
            对手信息列表
        """
        if not self.opponents:
            print("⚠️  对手池为空，无法采样")
            return []
        
        # 过滤要排除的对手
        available = self.opponents
        if exclude:
            available = [opp for opp in self.opponents if opp['name'] not in exclude]
        
        if not available:
            print("⚠️  没有可用的对手")
            return []
        
        # 采样
        if strategy == "uniform":
            # 均匀采样（可重复）
            sampled = random.choices(available, k=min(n, len(available)))
        
        elif strategy == "weighted":
            # 按性能加权采样（性能越好权重越高）
            weights = []
            for opp in available:
                # 从metadata中获取性能指标
                perf = opp['metadata'].get('eval_metrics', {})
                # 使用平均奖励作为权重
                weight = perf.get('avg_reward', 1.0) + 5.0  # +5确保权重为正
                weights.append(max(weight, 0.1))
            
            sampled = random.choices(available, weights=weights, k=min(n, len(available)))
        
        else:
            raise ValueError(f"未知的采样策略: {strategy}")
        
        return sampled
    
    def add_opponent(
        self,
        model_path: Path,
        metadata: Dict[str, Any]
    ):
        """
        添加新对手到池中
        
        Args:
            model_path: 模型文件路径
            metadata: 对手元数据
        """
        opponent_info = self._load_opponent(model_path)
        if opponent_info:
            opponent_info['metadata'] = metadata
            self.opponents.append(opponent_info)
            
            # 更新池的metadata文件
            self._update_metadata()
            
            print(f"✅ 添加对手到池: {opponent_info['name']}")
    
    def _update_metadata(self):
        """更新池的metadata文件"""
        if not self.pool_dir:
            return
        
        self.metadata['models'] = []
        for opp in self.opponents:
            self.metadata['models'].append({
                'name': opp['name'],
                'algo': opp['algo'],
                'side': opp['side'],
                'version': opp['version'],
                'path': str(opp['path']),
                'metadata': opp['metadata']
            })
        
        metadata_path = self.pool_dir / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
    
    def size(self) -> int:
        """获取池大小"""
        return len(self.opponents)
    
    def get_all_opponents(self) -> List[Dict[str, Any]]:
        """获取所有对手"""
        return self.opponents.copy()
    
    def clear(self):
        """清空对手池（仅清空内存，不删除文件）"""
        self.opponents = []


class MixedOpponentSampler:
    """混合对手采样器（固定池 + 随机）"""
    
    def __init__(
        self,
        fixed_pool: OpponentPool,
        random_policy_creator,
        fixed_ratio: float = 0.7,
        sampling_strategy: str = "uniform"
    ):
        """
        初始化混合采样器
        
        Args:
            fixed_pool: 固定对手池
            random_policy_creator: 创建随机策略的函数
            fixed_ratio: 固定对手占比
            sampling_strategy: 采样策略
        """
        self.fixed_pool = fixed_pool
        self.random_policy_creator = random_policy_creator
        self.fixed_ratio = fixed_ratio
        self.sampling_strategy = sampling_strategy
    
    def sample(self, n: int) -> List[Dict[str, Any]]:
        """
        采样混合对手
        
        Args:
            n: 总对手数量
        
        Returns:
            对手列表（包含固定对手和随机对手）
        """
        n_fixed = int(n * self.fixed_ratio)
        n_random = n - n_fixed
        
        opponents = []
        
        # 从固定池采样
        if n_fixed > 0 and self.fixed_pool.size() > 0:
            fixed_opponents = self.fixed_pool.sample_opponents(
                n=n_fixed,
                strategy=self.sampling_strategy
            )
            opponents.extend(fixed_opponents)
        
        # 添加随机对手
        for i in range(n_random):
            random_opp = {
                'name': f'RANDOM_{i}',
                'algo': 'RANDOM',
                'policy': self.random_policy_creator(),
                'is_random': True
            }
            opponents.append(random_opp)
        
        return opponents


def create_opponent_policies(
    opponent_config: Dict[str, Any],
    env_manager,
    device: str = "auto"
) -> Dict[str, Any]:
    """
    根据配置创建对手策略
    
    Args:
        opponent_config: 对手配置
        env_manager: 环境管理器（用于获取空间信息）
        device: 计算设备
    
    Returns:
        对手策略字典 {agent_id: policy}
    """
    opp_type = opponent_config.get('type', 'algorithm')
    opp_side = opponent_config.get('side', 'predator')
    
    # 获取对手智能体列表
    opponent_agents = env_manager.get_agents_by_type(opp_side)
    
    # 获取空间信息
    obs_space = env_manager.get_observation_space(opp_side)
    action_space = env_manager.get_action_space(opp_side)
    
    policies = {}
    
    if opp_type == "algorithm":
        # 使用指定算法（通常是RANDOM）
        algo_name = opponent_config.get('algorithm', 'RANDOM')
        algo_config = get_algo_config(algo_name)
        
        # 为所有对手创建相同的策略
        policy = create_algorithm(
            algo_name=algo_name,
            observation_space=obs_space,
            action_space=action_space,
            config=algo_config,
            device=device
        )
        
        # 如果是RANDOM，需要创建模型
        if algo_name == 'RANDOM':
            policy.create_model(None)
        
        for agent in opponent_agents:
            policies[agent] = policy
    
    elif opp_type == "fixed_model":
        # 加载固定模型
        model_path = opponent_config.get('path')
        if not model_path or not os.path.exists(model_path):
            raise FileNotFoundError(f"对手模型不存在: {model_path}")
        
        # 从路径解析算法名称
        filename = Path(model_path).stem
        algo_name = filename.split('_')[0]
        algo_config = get_algo_config(algo_name)
        
        policy = create_algorithm(
            algo_name=algo_name,
            observation_space=obs_space,
            action_space=action_space,
            config=algo_config,
            device=device
        )
        policy.load(model_path)
        
        for agent in opponent_agents:
            policies[agent] = policy
    
    elif opp_type == "mixed_pool":
        # 从混合池采样
        pool_path = Path(opponent_config.get('pool_path', ''))
        
        if not pool_path.exists():
            print(f"⚠️  对手池不存在: {pool_path}，使用RANDOM策略")
            # 回退到RANDOM
            algo_config = get_algo_config('RANDOM')
            policy = create_algorithm(
                algo_name='RANDOM',
                observation_space=obs_space,
                action_space=action_space,
                config=algo_config,
                device=device
            )
            policy.create_model(None)
            
            for agent in opponent_agents:
                policies[agent] = policy
        else:
            # 加载池
            pool = OpponentPool(pool_path)
            
            # 创建混合采样器
            def create_random():
                algo_config = get_algo_config('RANDOM')
                policy = create_algorithm(
                    algo_name='RANDOM',
                    observation_space=obs_space,
                    action_space=action_space,
                    config=algo_config,
                    device=device
                )
                policy.create_model(None)
                return policy
            
            mix_strategy = opponent_config.get('mix_strategy', {})
            sampler = MixedOpponentSampler(
                fixed_pool=pool,
                random_policy_creator=create_random,
                fixed_ratio=mix_strategy.get('fixed_ratio', 0.7),
                sampling_strategy=mix_strategy.get('sampling', 'uniform')
            )
            
            # 采样对手
            n_opponents = len(opponent_agents)
            sampled_opponents = sampler.sample(n_opponents)
            
            # 分配给智能体
            for i, agent in enumerate(opponent_agents):
                opp = sampled_opponents[i % len(sampled_opponents)]
                
                if opp.get('is_random', False):
                    # 随机对手
                    policies[agent] = opp['policy']
                else:
                    # ✅ 固定池对手（传入空间信息）
                    loaded_policy = pool.get_opponent_policy(
                        opp, 
                        device,
                        obs_space,
                        action_space
                    )
                    policies[agent] = loaded_policy
    
    else:
        raise ValueError(f"未知的对手类型: {opp_type}")
    
    return policies

================================================================================
FILE: src/core/trainer.py
================================================================================
"""
核心训练器
整合所有模块，提供统一的训练接口
"""

import os
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

from src.core.environment import WaterworldEnvManager, create_training_env
from src.core.opponent_pool import create_opponent_policies
from src.algorithms import create_algorithm
from src.callbacks import create_callbacks
from src.utils.config_loader import get_mode_config, get_env_config, get_algo_config
from src.utils.path_manager import PathManager
from src.utils.naming import FileNaming
from src.utils.logger import create_logger
from src.utils.banner import print_mode_banner, print_training_start, print_training_complete
from src.utils.config_snapshot import save_config_snapshot, save_training_summary
from src.utils.config_validator import validator
from src.utils.cleanup import cleanup_debug


class MultiAgentTrainer:
    """多智能体训练器"""
    
    def __init__(
        self,
        # 核心配置
        train_side: str,
        train_algo: str,
        opponent_config: Dict[str, Any],
        
        # 实验元数据
        experiment_name: str,
        stage_name: str,
        generation: int = 0,
        version: str = "v1",
        
        # 运行模式
        run_mode: str = "prod",
        
        # 环境配置
        env_config: Optional[Dict[str, Any]] = None,
        
        # 算法配置
        algo_config: Optional[Dict[str, Any]] = None,
        
        # 训练配置（覆盖模式默认值）
        total_timesteps: Optional[int] = None,
        n_envs: Optional[int] = None,
        eval_freq: Optional[int] = None,
        checkpoint_freq: Optional[int] = None,
        n_eval_episodes: Optional[int] = None,
        
        # 其他
        device: str = "auto",
        seed: Optional[int] = None,
        notes: str = ""
    ):
        """
        初始化训练器
        
        Args:
            train_side: 训练方（predator/prey）
            train_algo: 训练算法（PPO/A2C/SAC/TD3）
            opponent_config: 对手配置
            experiment_name: 实验名称
            stage_name: 训练阶段名称（stage1.1_prey_warmup等）
            generation: 代数
            version: 版本号
            run_mode: 运行模式（debug/dryrun/prod）
            env_config: 环境配置（None则使用默认）
            algo_config: 算法配置（None则使用默认）
            total_timesteps: 总训练步数（None则使用模式默认）
            n_envs: 并行环境数（None则使用模式默认）
            eval_freq: 评估频率（None则使用模式默认）
            checkpoint_freq: 检查点频率（None则使用模式默认）
            n_eval_episodes: 评估episode数（None则使用模式默认）
            device: 计算设备
            seed: 随机种子
            notes: 实验备注
        """
        
        # =====================================================================
        # 1. 基本配置
        # =====================================================================
        self.train_side = train_side
        self.train_algo = train_algo.upper()
        self.opponent_config = opponent_config
        self.experiment_name = experiment_name
        self.stage_name = stage_name
        self.generation = generation
        self.version = version
        self.run_mode = run_mode
        self.device = device
        self.seed = seed
        self.notes = notes
        
        # =====================================================================
        # 2. 加载配置
        # =====================================================================
        
        # 加载运行模式配置
        self.mode_config = get_mode_config(run_mode)
        
        # 加载环境配置
        if env_config is None:
            env_config = get_env_config("waterworld_standard")
        self.env_config = env_config
        
        # 加载算法配置
        if algo_config is None:
            algo_config = get_algo_config(self.train_algo)
        self.algo_config = algo_config
        
        # 合并训练配置（用户指定 > 模式默认）
        self.training_config = {
            'total_timesteps': total_timesteps or self.mode_config.get('total_timesteps', 1000000),
            'n_envs': n_envs or self.mode_config.get('n_envs', 1),
            'eval_freq': eval_freq if eval_freq is not None else self.mode_config.get('eval_freq', 10000),
            'checkpoint_freq': checkpoint_freq if checkpoint_freq is not None else self.mode_config.get('checkpoint_freq', 100000),
            'n_eval_episodes': n_eval_episodes or self.mode_config.get('n_eval_episodes', 10),
            'save_checkpoints': self.mode_config.get('save_checkpoints', True),
            'save_final_model': self.mode_config.get('save_final_model', True),
            'tensorboard_enabled': self.mode_config.get('tensorboard_enabled', True),
            'verbose': self.mode_config.get('verbose', 1),
            'deterministic_eval': self.mode_config.get('deterministic_eval', True),
            'show_progress': True,
            'check_freeze': False  # 在训练后手动检查
        }
        
        # =====================================================================
        # 3. 验证配置
        # =====================================================================
        full_config = {
            'run_mode': run_mode,
            'train_side': train_side,
            'train_algo': train_algo,
            'experiment_name': experiment_name,
            **self.training_config
        }
        
        if not validator.validate_run_mode(run_mode, full_config):
            validator.print_results()
            raise ValueError("配置验证失败")
        # test 模式不需要确认
        if run_mode not in ['debug', 'test']:  # ✅ 添加 'test'
            if not validator.require_confirmation():
                raise KeyboardInterrupt("用户取消训练")
        if not validator.require_confirmation():
            raise KeyboardInterrupt("用户取消训练")
        
        # =====================================================================
        # 4. 路径管理
        # =====================================================================
        self.path_manager = PathManager(run_mode, experiment_name)
        
        # 各种输出路径
        self.model_dir = self.path_manager.get_model_dir(stage_name)
        self.checkpoint_dir = self.path_manager.get_checkpoint_dir(stage_name)
        self.tensorboard_dir = self.path_manager.get_tensorboard_dir(stage_name)
        self.experiment_dir = self.path_manager.get_experiment_dir(stage_name)
        
        # =====================================================================
        # 5. 日志系统
        # =====================================================================
        self.naming = FileNaming()
        log_filename = self.naming.generate_log_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        self.logger = create_logger(
            name=f"{self.train_algo}_{self.train_side}",
            log_dir=self.experiment_dir,
            log_level=self.mode_config.get('log_level', 'INFO')
        )
        
        # =====================================================================
        # 6. 环境和模型（延迟初始化）
        # =====================================================================
        self.env_manager = None
        self.train_env = None
        self.eval_env = None
        self.algorithm = None
        self.opponent_policies = None
        
        # =====================================================================
        # 7. 训练统计
        # =====================================================================
        self.training_start_time = None
        self.training_end_time = None
        self.total_training_time = None
        self.final_model_path = None
        
        # =====================================================================
        # 8. 打印横幅
        # =====================================================================
        print_mode_banner(run_mode, self.mode_config)
        
        # =====================================================================
        # 9. 清理调试数据（如果需要）
        # =====================================================================
        if run_mode == "debug":
            cleanup_debug(self.mode_config)
        
        # =====================================================================
        # 10. 保存配置快照
        # =====================================================================
        self._save_config_snapshot()
    
    def _save_config_snapshot(self):
        """保存配置快照"""
        snapshot = {
            'run_mode': self.run_mode,
            'train_side': self.train_side,
            'train_algo': self.train_algo,
            'opponent_config': self.opponent_config,
            'experiment_name': self.experiment_name,
            'stage_name': self.stage_name,
            'generation': self.generation,
            'version': self.version,
            'device': self.device,
            'seed': self.seed,
            'notes': self.notes,
            'env_config': self.env_config,
            'algo_config': self.algo_config,
            'training_config': self.training_config,
            'mode_config': self.mode_config
        }
        
        config_filename = self.naming.generate_config_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        save_config_snapshot(
            config=snapshot,
            save_dir=self.experiment_dir,
            name=config_filename.replace('.yaml', '')
        )
    
    def setup(self):
        """
        设置训练环境和模型
        在训练前必须调用此方法
        """
        self.logger.log_banner("🔧 设置训练环境", "=")
        
        # =====================================================================
        # 1. 创建环境管理器
        # =====================================================================
        self.logger.info("创建环境管理器...")
        self.env_manager = WaterworldEnvManager(self.env_config)
        
        # =====================================================================
        # 2. 创建对手策略
        # =====================================================================
        self.logger.info(f"创建对手策略 (类型: {self.opponent_config.get('type')})...")
        self.opponent_policies = create_opponent_policies(
            opponent_config=self.opponent_config,
            env_manager=self.env_manager,
            device=self.device
        )
        
        self.logger.info(f"  ✓ 创建了 {len(self.opponent_policies)} 个对手策略")
        
        # =====================================================================
        # 3. 创建训练环境
        # =====================================================================
        self.logger.info(f"创建训练环境 (并行数: {self.training_config['n_envs']})...")
        self.train_env = create_training_env(
            env_config=self.env_config,
            train_side=self.train_side,
            opponent_policies=self.opponent_policies,
            n_envs=self.training_config['n_envs']
        )
        
        # =====================================================================
        # 4. 创建评估环境
        # =====================================================================
        if self.training_config['eval_freq'] > 0:
            self.logger.info("创建评估环境...")
            self.eval_env = create_training_env(
                env_config=self.env_config,
                train_side=self.train_side,
                opponent_policies=self.opponent_policies,
                n_envs=1  # 评估用单环境
            )
        
        # =====================================================================
        # 5. 创建算法
        # =====================================================================
        self.logger.info(f"创建算法: {self.train_algo}...")
        
        # 获取空间信息
        obs_space = self.env_manager.get_observation_space(self.train_side)
        action_space = self.env_manager.get_action_space(self.train_side)
        
        # 创建算法实例
        self.algorithm = create_algorithm(
            algo_name=self.train_algo,
            observation_space=obs_space,
            action_space=action_space,
            config=self.algo_config,
            device=self.device
        )
        
        # 创建模型
        tensorboard_log = str(self.tensorboard_dir) if self.training_config['tensorboard_enabled'] else None
        
        self.algorithm.create_model(
            env=self.train_env,
            tensorboard_log=tensorboard_log,
            verbose=self.training_config['verbose']
        )
        
        self.logger.info("  ✓ 算法创建完成")
        
        # =====================================================================
        # 6. 记录配置
        # =====================================================================
        self.logger.log_config({
            '训练方': self.train_side,
            '训练算法': self.train_algo,
            '版本': self.version,
            '总步数': self.training_config['total_timesteps'],
            '并行环境': self.training_config['n_envs'],
            '评估频率': self.training_config['eval_freq'],
            '检查点频率': self.training_config['checkpoint_freq'],
            '设备': self.device,
            '随机种子': self.seed
        }, title="训练配置")
        
        self.logger.log_banner("✅ 环境设置完成", "=")
    
    def train(self):
        """执行训练"""
        
        # =====================================================================
        # 1. 检查是否已设置
        # =====================================================================
        if self.algorithm is None:
            raise RuntimeError("请先调用 setup() 方法设置环境和模型")
        
        # =====================================================================
        # 2. 打印训练开始信息
        # =====================================================================
        opponent_info = self.naming.format_opponent_info(self.opponent_config)
        print_training_start(
            algo=self.train_algo,
            side=self.train_side,
            version=self.version,
            opponent_info=opponent_info
        )
        
        self.logger.log_banner(f"🚀 开始训练 {self.train_algo}_{self.train_side}_{self.version}", "=")
        
        # =====================================================================
        # 3. 创建回调
        # =====================================================================
        callbacks = create_callbacks(
            train_side=self.train_side,
            checkpoint_path=self.checkpoint_dir,
            eval_env=self.eval_env,
            config=self.training_config,
            on_freeze=None  # 可以添加冻结回调
        )
        
        # =====================================================================
        # 4. 执行训练
        # =====================================================================
        self.training_start_time = time.time()
        
        try:
            self.algorithm.train(
                env=self.train_env,
                total_timesteps=self.training_config['total_timesteps'],
                callback=callbacks
            )
            
            self.training_end_time = time.time()
            self.total_training_time = self.training_end_time - self.training_start_time
            
        except KeyboardInterrupt:
            self.logger.warning("\n⚠️  训练被用户中断")
            self.training_end_time = time.time()
            self.total_training_time = self.training_end_time - self.training_start_time
            raise
        
        except Exception as e:
            self.logger.error(f"\n❌ 训练过程中发生错误: {e}")
            raise
        
        # =====================================================================
        # 5. 打印训练完成信息
        # =====================================================================
        print_training_complete(
            algo=self.train_algo,
            side=self.train_side,
            total_steps=self.training_config['total_timesteps'],
            time_elapsed=self.total_training_time
        )
        
        self.logger.log_banner("✅ 训练完成", "=")
    
    def save_model(self, save_to_pool: bool = False, pool_name: Optional[str] = None):
        """
        保存最终模型
        
        Args:
            save_to_pool: 是否保存到固定池
            pool_name: 池名称（如 prey_pool_v1）
        """
        
        if not self.training_config['save_final_model']:
            self.logger.info("配置禁用了模型保存，跳过")
            return
        
        self.logger.log_banner("💾 保存模型", "-")
        
        # =====================================================================
        # 1. 保存到模型目录
        # =====================================================================
        opponent_info = self.naming.format_opponent_info(self.opponent_config)
        
        model_filename = self.naming.generate_model_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version,
            opponent_info=opponent_info,
            run_mode=self.run_mode
        )
        
        model_path = self.model_dir / model_filename
        self.algorithm.save(str(model_path))
        self.final_model_path = model_path
        
        self.logger.info(f"✓ 模型已保存: {model_path}")
        
        # =====================================================================
        # 2. 保存到固定池（如果需要）
        # =====================================================================
        if save_to_pool and pool_name:
            pool_dir = self.path_manager.get_fixed_pool_dir(pool_name)
            
            pool_model_filename = f"{self.train_algo}_{self.train_side}_{self.version}.zip"
            pool_model_path = pool_dir / pool_model_filename
            
            # 复制模型到池
            import shutil
            shutil.copy(str(model_path), str(pool_model_path))
            
            self.logger.info(f"✓ 模型已加入固定池: {pool_model_path}")
            
            # 更新池的metadata
            self._update_pool_metadata(pool_dir, pool_model_filename)
        
        self.logger.log_banner("", "-")
    
    def _update_pool_metadata(self, pool_dir: Path, model_filename: str):
        """更新固定池的metadata"""
        metadata_path = pool_dir / "metadata.json"
        
        # 加载现有metadata
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {
                'pool_name': pool_dir.name,
                'created_at': datetime.now().isoformat(),
                'models': []
            }
        
        # 添加新模型信息
        model_info = {
            'name': model_filename.replace('.zip', ''),
            'path': model_filename,
            'algorithm': self.train_algo,
            'training_steps': self.training_config['total_timesteps'],
            'trained_against': self.naming.format_opponent_info(self.opponent_config),
            'added_at': datetime.now().isoformat(),
            'eval_metrics': {}  # 可以在评估后填充
        }
        
        # 检查是否已存在
        existing = [m for m in metadata['models'] if m['name'] == model_info['name']]
        if existing:
            # 更新现有条目
            idx = metadata['models'].index(existing[0])
            metadata['models'][idx] = model_info
        else:
            # 添加新条目
            metadata['models'].append(model_info)
        
        # 保存metadata
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def evaluate(self, n_episodes: Optional[int] = None) -> Dict[str, Any]:
        """
        评估模型
        
        Args:
            n_episodes: 评估episode数（None则使用配置默认值）
        
        Returns:
            评估结果字典
        """
        from stable_baselines3.common.evaluation import evaluate_policy
        
        if self.eval_env is None:
            self.logger.warning("评估环境未创建，无法评估")
            return {}
        
        n_episodes = n_episodes or self.training_config['n_eval_episodes']
        
        self.logger.log_banner(f"📊 评估模型 ({n_episodes} episodes)", "-")
        
        episode_rewards, episode_lengths = evaluate_policy(
            self.algorithm.model,
            self.eval_env,
            n_eval_episodes=n_episodes,
            deterministic=self.training_config['deterministic_eval'],
            return_episode_rewards=True
        )
        
        results = {
            'mean_reward': float(np.mean(episode_rewards)),
            'std_reward': float(np.std(episode_rewards)),
            'mean_length': float(np.mean(episode_lengths)),
            'std_length': float(np.std(episode_lengths)),
            'min_reward': float(np.min(episode_rewards)),
            'max_reward': float(np.max(episode_rewards)),
            'n_episodes': n_episodes
        }
        
        self.logger.log_config(results, title="评估结果")
        self.logger.log_banner("", "-")
        
        return results
    
    def check_freeze_criteria(self, freeze_criteria: Dict[str, Any]) -> bool:
        """
        检查是否达到冻结条件
        
        Args:
            freeze_criteria: 冻结条件字典
        
        Returns:
            是否达到冻结条件
        """
        # 先评估模型
        eval_results = self.evaluate()
        
        if not eval_results:
            return False
        
        # 检查最低奖励
        min_reward = freeze_criteria.get('min_avg_reward', -np.inf)
        if eval_results['mean_reward'] < min_reward:
            self.logger.info(f"❌ 未达到最低奖励: {eval_results['mean_reward']:.2f} < {min_reward:.2f}")
            return False
        
        # 角色特定检查（简化版，完整版需要在回调中实现）
        if self.train_side == "predator":
            min_catch_rate = freeze_criteria.get('min_catch_rate', 0.0)
            # 这里需要从评估中获取catch_rate，暂时用奖励代替
            self.logger.info(f"✓ 达到最低奖励: {eval_results['mean_reward']:.2f} >= {min_reward:.2f}")
        
        elif self.train_side == "prey":
            min_survival_rate = freeze_criteria.get('min_survival_rate', 0.0)
            # 这里需要从评估中获取survival_rate，暂时用奖励代替
            self.logger.info(f"✓ 达到最低奖励: {eval_results['mean_reward']:.2f} >= {min_reward:.2f}")
        
        return True
    
    def save_training_summary(self):
        """保存训练摘要"""
        summary = {
            'experiment_name': self.experiment_name,
            'stage_name': self.stage_name,
            'train_side': self.train_side,
            'train_algo': self.train_algo,
            'version': self.version,
            'generation': self.generation,
            'run_mode': self.run_mode,
            'opponent': self.naming.format_opponent_info(self.opponent_config),
            'training_config': self.training_config,
            'training_time_seconds': self.total_training_time,
            'final_model_path': str(self.final_model_path) if self.final_model_path else None,
            'notes': self.notes
        }
        
        summary_filename = self.naming.generate_summary_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        save_training_summary(
            summary=summary,
            save_dir=self.experiment_dir,
            name=summary_filename.replace('.json', '')
        )
    
    def cleanup(self):
        """清理资源"""
        self.logger.info("清理资源...")
        
        if self.train_env:
            self.train_env.close()
        
        if self.eval_env:
            self.eval_env.close()
        
        if self.env_manager:
            self.env_manager.close()
        
        self.logger.info("✓ 资源清理完成")
    
    def run(
        self,
        save_to_pool: bool = False,
        pool_name: Optional[str] = None,
        check_freeze: bool = False,
        freeze_criteria: Optional[Dict[str, Any]] = None
    ):
        """
        完整训练流程（setup → train → evaluate → save）
        
        Args:
            save_to_pool: 是否保存到固定池
            pool_name: 池名称
            check_freeze: 是否检查冻结条件
            freeze_criteria: 冻结条件
        """
        try:
            # 1. 设置
            self.setup()
            
            # 2. 训练
            self.train()
            
            # 3. 最终评估
            final_eval = self.evaluate()
            
            # 4. 检查冻结条件
            can_freeze = True
            if check_freeze and freeze_criteria:
                can_freeze = self.check_freeze_criteria(freeze_criteria)
                
                if can_freeze:
                    self.logger.log_banner("❄️  模型达到冻结标准", "=")
                else:
                    self.logger.log_banner("⚠️  模型未达到冻结标准", "=")
            
            # 5. 保存模型
            should_save_to_pool = save_to_pool and (not check_freeze or can_freeze)
            self.save_model(save_to_pool=should_save_to_pool, pool_name=pool_name)
            
            # 6. 保存训练摘要
            self.save_training_summary()
            
            return final_eval
        
        finally:
            # 7. 清理
            self.cleanup()


# 导入numpy（前面忘记了）
import numpy as np

================================================================================
FILE: src/analysis/__init__.py
================================================================================


================================================================================
FILE: src/callbacks/__init__.py
================================================================================
"""
回调模块初始化
"""

from stable_baselines3.common.callbacks import CallbackList

from .tensorboard_logger import MultiAgentTensorBoardCallback
from .checkpoint_callback import CheckpointCallback
from .eval_callback import EvalCallback
from .freeze_callback import FreezeCallback
from .progress_callback import ProgressBarCallback


def create_callbacks(
    train_side: str,
    checkpoint_path,
    eval_env,
    config: dict,
    on_freeze=None
) -> CallbackList:
    """
    创建回调列表
    
    Args:
        train_side: 训练方
        checkpoint_path: 检查点保存路径
        eval_env: 评估环境
        config: 配置字典
        on_freeze: 冻结回调函数
    
    Returns:
        回调列表
    """
    callbacks = []
    
    # 1. TensorBoard日志
    tb_callback = MultiAgentTensorBoardCallback(
        train_side=train_side,
        verbose=config.get('verbose', 1)
    )
    callbacks.append(tb_callback)
    
    # 2. 检查点保存
    if config.get('save_checkpoints', True):
        checkpoint_freq = config.get('checkpoint_freq', 100000)
        if checkpoint_freq > 0:
            checkpoint_callback = CheckpointCallback(
                save_freq=checkpoint_freq,
                save_path=checkpoint_path,
                name_prefix="checkpoint",
                verbose=config.get('verbose', 1)
            )
            callbacks.append(checkpoint_callback)
    
    # 3. 评估
    if config.get('eval_freq', -1) > 0:
        eval_callback = EvalCallback(
            eval_env=eval_env,
            train_side=train_side,
            eval_freq=config.get('eval_freq', 10000),
            n_eval_episodes=config.get('n_eval_episodes', 10),
            deterministic=config.get('deterministic_eval', True),
            verbose=config.get('verbose', 1),
            best_model_save_path=checkpoint_path
        )
        callbacks.append(eval_callback)
        
        # 4. 冻结条件检查
        if config.get('check_freeze', False):
            freeze_criteria = config.get('freeze_criteria', {})
            freeze_callback = FreezeCallback(
                eval_callback=eval_callback,
                train_side=train_side,
                freeze_criteria=freeze_criteria,
                on_freeze=on_freeze,
                verbose=config.get('verbose', 1)
            )
            callbacks.append(freeze_callback)
    
    # 5. 进度条
    if config.get('show_progress', True):
        progress_callback = ProgressBarCallback(
            total_timesteps=config.get('total_timesteps', 1000000),
            verbose=config.get('verbose', 1)
        )
        callbacks.append(progress_callback)
    
    return CallbackList(callbacks)


__all__ = [
    'MultiAgentTensorBoardCallback',
    'CheckpointCallback',
    'EvalCallback',
    'FreezeCallback',
    'ProgressBarCallback',
    'create_callbacks'
]

================================================================================
FILE: src/callbacks/checkpoint_callback.py
================================================================================
"""
检查点保存回调
定期保存训练检查点
"""

import os
from pathlib import Path
from typing import Optional
from stable_baselines3.common.callbacks import BaseCallback


class CheckpointCallback(BaseCallback):
    """检查点保存回调"""
    
    def __init__(
        self,
        save_freq: int,
        save_path: Path,
        name_prefix: str = "checkpoint",
        save_replay_buffer: bool = False,
        save_vecnormalize: bool = False,
        verbose: int = 0
    ):
        """
        初始化回调
        
        Args:
            save_freq: 保存频率（步数）
            save_path: 保存路径
            name_prefix: 文件名前缀
            save_replay_buffer: 是否保存replay buffer（SAC/TD3）
            save_vecnormalize: 是否保存VecNormalize统计
            verbose: 详细程度
        """
        super().__init__(verbose)
        self.save_freq = save_freq
        self.save_path = Path(save_path)
        self.name_prefix = name_prefix
        self.save_replay_buffer = save_replay_buffer
        self.save_vecnormalize = save_vecnormalize
        
        # 创建保存目录
        self.save_path.mkdir(parents=True, exist_ok=True)
    
    def _on_step(self) -> bool:
        """每步调用"""
        if self.save_freq > 0 and self.n_calls % self.save_freq == 0:
            self._save_checkpoint()
        
        return True
    
    def _save_checkpoint(self):
        """保存检查点"""
        # 构建文件名
        checkpoint_name = f"{self.name_prefix}_step_{self.n_calls}.zip"
        checkpoint_path = self.save_path / checkpoint_name
        
        # 保存模型
        self.model.save(checkpoint_path)
        
        if self.verbose > 0:
            print(f"💾 保存检查点: {checkpoint_path}")
        
        # 保存replay buffer（如果适用）
        if self.save_replay_buffer and hasattr(self.model, 'replay_buffer'):
            if self.model.replay_buffer is not None:
                buffer_path = self.save_path / f"{self.name_prefix}_replay_buffer_step_{self.n_calls}.pkl"
                self.model.save_replay_buffer(buffer_path)
                
                if self.verbose > 0:
                    print(f"💾 保存replay buffer: {buffer_path}")
        
        # 保存VecNormalize统计（如果适用）
        if self.save_vecnormalize:
            from stable_baselines3.common.vec_env import VecNormalize
            if isinstance(self.training_env, VecNormalize):
                vecnorm_path = self.save_path / f"{self.name_prefix}_vecnormalize_step_{self.n_calls}.pkl"
                self.training_env.save(vecnorm_path)
                
                if self.verbose > 0:
                    print(f"💾 保存VecNormalize: {vecnorm_path}")

================================================================================
FILE: src/callbacks/eval_callback.py
================================================================================
"""
评估回调
定期评估模型性能
"""

import numpy as np
from pathlib import Path
from typing import Optional, Callable, Dict, Any
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy


class EvalCallback(BaseCallback):
    """评估回调"""
    
    def __init__(
        self,
        eval_env,
        train_side: str,
        eval_freq: int = 10000,
        n_eval_episodes: int = 10,
        deterministic: bool = True,
        render: bool = False,
        verbose: int = 1,
        best_model_save_path: Optional[Path] = None,
        log_path: Optional[Path] = None
    ):
        """
        初始化回调
        
        Args:
            eval_env: 评估环境
            train_side: 训练方（predator/prey）
            eval_freq: 评估频率（步数）
            n_eval_episodes: 评估episode数
            deterministic: 是否使用确定性策略
            render: 是否渲染
            verbose: 详细程度
            best_model_save_path: 最佳模型保存路径
            log_path: 日志保存路径
        """
        super().__init__(verbose)
        self.eval_env = eval_env
        self.train_side = train_side
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.deterministic = deterministic
        self.render = render
        self.best_model_save_path = best_model_save_path
        self.log_path = log_path
        
        # 最佳性能跟踪
        self.best_mean_reward = -np.inf
        self.last_mean_reward = -np.inf
        
        # 评估历史
        self.evaluations_timesteps = []
        self.evaluations_results = []
        self.evaluations_length = []
        
        # 创建保存目录
        if self.best_model_save_path:
            self.best_model_save_path = Path(self.best_model_save_path)
            self.best_model_save_path.mkdir(parents=True, exist_ok=True)
    
    def _on_step(self) -> bool:
        """每步调用"""
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            self._evaluate()
        
        return True
    
    def _evaluate(self):
        """执行评估"""
        if self.verbose > 0:
            print(f"\n{'='*70}")
            print(f"📊 评估 (步数: {self.n_calls})")
            print(f"{'='*70}")
        
        # 评估模型
        episode_rewards, episode_lengths = evaluate_policy(
            self.model,
            self.eval_env,
            n_eval_episodes=self.n_eval_episodes,
            render=self.render,
            deterministic=self.deterministic,
            return_episode_rewards=True
        )
        
        mean_reward = np.mean(episode_rewards)
        std_reward = np.std(episode_rewards)
        mean_length = np.mean(episode_lengths)
        
        self.last_mean_reward = mean_reward
        
        # 记录结果
        self.evaluations_timesteps.append(self.n_calls)
        self.evaluations_results.append(episode_rewards)
        self.evaluations_length.append(episode_lengths)
        
        # 记录到TensorBoard
        self.logger.record('eval/mean_reward', mean_reward)
        self.logger.record('eval/std_reward', std_reward)
        self.logger.record('eval/mean_ep_length', mean_length)
        
        if self.verbose > 0:
            print(f"  平均奖励: {mean_reward:.2f} +/- {std_reward:.2f}")
            print(f"  平均长度: {mean_length:.0f}")
        
        # 保存最佳模型
        if mean_reward > self.best_mean_reward:
            if self.verbose > 0:
                print(f"  🎉 新的最佳模型! (旧: {self.best_mean_reward:.2f}, 新: {mean_reward:.2f})")
            
            self.best_mean_reward = mean_reward
            
            if self.best_model_save_path:
                best_model_path = self.best_model_save_path / "best_model.zip"
                self.model.save(best_model_path)
                
                if self.verbose > 0:
                    print(f"  💾 保存最佳模型: {best_model_path}")
        
        if self.verbose > 0:
            print(f"{'='*70}\n")
    
    def get_best_mean_reward(self) -> float:
        """获取最佳平均奖励"""
        return self.best_mean_reward
    
    def get_last_mean_reward(self) -> float:
        """获取最近一次平均奖励"""
        return self.last_mean_reward

================================================================================
FILE: src/callbacks/freeze_callback.py
================================================================================
"""
冻结条件检查回调
检查模型是否达到冻结标准，可以加入固定池
"""

from pathlib import Path
from typing import Dict, Any, Optional, Callable
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np


class FreezeCallback(BaseCallback):
    """冻结条件检查回调"""
    
    def __init__(
        self,
        eval_callback: BaseCallback,
        train_side: str,
        freeze_criteria: Dict[str, Any],
        on_freeze: Optional[Callable] = None,
        verbose: int = 1
    ):
        """
        初始化回调
        
        Args:
            eval_callback: 评估回调（用于获取评估结果）
            train_side: 训练方（predator/prey）
            freeze_criteria: 冻结条件
            on_freeze: 达到冻结条件时的回调函数
            verbose: 详细程度
        """
        super().__init__(verbose)
        self.eval_callback = eval_callback
        self.train_side = train_side
        self.freeze_criteria = freeze_criteria
        self.on_freeze = on_freeze
        
        # 冻结状态
        self.is_frozen = False
        self.freeze_timestep = None
    
    def _on_step(self) -> bool:
        """每步调用"""
        # 只在评估后检查
        if not hasattr(self.eval_callback, 'last_mean_reward'):
            return True
        
        # 如果已经冻结，不再检查
        if self.is_frozen:
            return True
        
        # 检查是否达到冻结条件
        if self._check_freeze_criteria():
            self._freeze_model()
        
        return True
    
    def _check_freeze_criteria(self) -> bool:
        """
        检查是否达到冻结条件
        
        Returns:
            是否达到冻结条件
        """
        # 获取评估结果
        last_mean_reward = self.eval_callback.get_last_mean_reward()
        
        # 获取对应角色的冻结标准
        criteria = self.freeze_criteria
        
        # 检查最低奖励
        min_reward = criteria.get('min_avg_reward', -np.inf)
        if last_mean_reward < min_reward:
            if self.verbose > 1:
                print(f"  ❌ 奖励不足: {last_mean_reward:.2f} < {min_reward:.2f}")
            return False
        
        # 检查角色特定指标
        if self.train_side == "predator":
            # Predator需要检查捕获率
            min_catch_rate = criteria.get('min_catch_rate', 0.0)
            # 这里需要从评估结果中获取catch_rate
            # 暂时简化处理
            if self.verbose > 1:
                print(f"  ✓ 奖励达标: {last_mean_reward:.2f} >= {min_reward:.2f}")
        
        elif self.train_side == "prey":
            # Prey需要检查生存率
            min_survival_rate = criteria.get('min_survival_rate', 0.0)
            # 这里需要从评估结果中获取survival_rate
            # 暂时简化处理
            if self.verbose > 1:
                print(f"  ✓ 奖励达标: {last_mean_reward:.2f} >= {min_reward:.2f}")
        
        # 检查评估episode数
        min_episodes = criteria.get('min_episodes', 0)
        n_eval_episodes = self.eval_callback.n_eval_episodes
        if n_eval_episodes < min_episodes:
            if self.verbose > 1:
                print(f"  ❌ 评估episode不足: {n_eval_episodes} < {min_episodes}")
            return False
        
        return True
    
    def _freeze_model(self):
        """冻结模型"""
        self.is_frozen = True
        self.freeze_timestep = self.n_calls
        
        if self.verbose > 0:
            print(f"\n{'='*70}")
            print(f"❄️  模型达到冻结条件")
            print(f"{'='*70}")
            print(f"  训练步数: {self.freeze_timestep}")
            print(f"  平均奖励: {self.eval_callback.get_last_mean_reward():.2f}")
            print(f"{'='*70}\n")
        
        # 调用自定义回调
        if self.on_freeze:
            self.on_freeze(self.model, self.freeze_timestep)
    
    def is_model_frozen(self) -> bool:
        """模型是否已冻结"""
        return self.is_frozen

================================================================================
FILE: src/callbacks/progress_callback.py
================================================================================
"""
进度显示回调
显示训练进度条和统计信息
"""

from typing import Optional
from stable_baselines3.common.callbacks import BaseCallback
from tqdm import tqdm


class ProgressBarCallback(BaseCallback):
    """进度条回调"""
    
    def __init__(
        self,
        total_timesteps: int,
        verbose: int = 1
    ):
        """
        初始化回调
        
        Args:
            total_timesteps: 总训练步数
            verbose: 详细程度
        """
        super().__init__(verbose)
        self.total_timesteps = total_timesteps
        self.pbar: Optional[tqdm] = None
    
    def _on_training_start(self) -> None:
        """训练开始时创建进度条"""
        if self.verbose > 0:
            self.pbar = tqdm(
                total=self.total_timesteps,
                desc="训练进度",
                unit="步"
            )
    
    def _on_step(self) -> bool:
        """每步更新进度条"""
        if self.pbar:
            self.pbar.update(1)
            
            # 更新进度条后缀信息
            if hasattr(self, 'locals') and 'infos' in self.locals:
                for info in self.locals['infos']:
                    if 'episode' in info:
                        ep_reward = info['episode']['r']
                        ep_length = info['episode']['l']
                        self.pbar.set_postfix({
                            'reward': f'{ep_reward:.2f}',
                            'length': f'{ep_length:.0f}'
                        })
                        break
        
        return True
    
    def _on_training_end(self) -> None:
        """训练结束时关闭进度条"""
        if self.pbar:
            self.pbar.close()
            self.pbar = None

================================================================================
FILE: src/callbacks/tensorboard_logger.py
================================================================================
"""
自定义TensorBoard日志回调
记录多智能体特定的指标
"""

from typing import Dict, Any, Optional
import numpy as np
from stable_baselines3.common.callbacks import BaseCallback


class MultiAgentTensorBoardCallback(BaseCallback):
    """多智能体TensorBoard日志回调"""
    
    def __init__(
        self,
        train_side: str,
        verbose: int = 0
    ):
        """
        初始化回调
        
        Args:
            train_side: 训练方（predator/prey）
            verbose: 详细程度
        """
        super().__init__(verbose)
        self.train_side = train_side
        
        # 统计信息
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_counts = 0
        
        # 角色特定指标
        self.catch_rates = []  # Predator
        self.survival_rates = []  # Prey
    
    def _on_step(self) -> bool:
        """每步调用"""
        # 检查是否有episode结束
        for info in self.locals.get('infos', []):
            if 'episode' in info:
                # 记录基本指标
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                self.episode_counts += 1
                
                # 记录到TensorBoard
                self.logger.record('rollout/ep_rew_mean', episode_reward)
                self.logger.record('rollout/ep_len_mean', episode_length)
                
                # 角色特定指标
                if self.train_side == "predator":
                    # Predator指标：捕获率
                    if 'catch_rate' in info:
                        catch_rate = info['catch_rate']
                        self.catch_rates.append(catch_rate)
                        self.logger.record('metrics/catch_rate', catch_rate)
                    
                    if 'first_catch_time' in info:
                        self.logger.record('metrics/first_catch_time', info['first_catch_time'])
                    
                    if 'energy_efficiency' in info:
                        self.logger.record('metrics/energy_efficiency', info['energy_efficiency'])
                
                elif self.train_side == "prey":
                    # Prey指标：生存率
                    if 'survival_rate' in info:
                        survival_rate = info['survival_rate']
                        self.survival_rates.append(survival_rate)
                        self.logger.record('metrics/survival_rate', survival_rate)
                    
                    if 'avg_lifespan' in info:
                        self.logger.record('metrics/avg_lifespan', info['avg_lifespan'])
                    
                    if 'escape_success' in info:
                        self.logger.record('metrics/escape_success', info['escape_success'])
                
                # 对战级指标
                if 'reward_gap' in info:
                    self.logger.record('metrics/reward_gap', info['reward_gap'])
                
                if 'balance_score' in info:
                    self.logger.record('metrics/balance_score', info['balance_score'])
        
        return True
    
    def _on_training_end(self) -> None:
        """训练结束时调用"""
        if self.episode_counts > 0:
            # 记录汇总统计
            self.logger.record('summary/total_episodes', self.episode_counts)
            self.logger.record('summary/mean_episode_reward', np.mean(self.episode_rewards))
            self.logger.record('summary/mean_episode_length', np.mean(self.episode_lengths))
            
            if self.train_side == "predator" and self.catch_rates:
                self.logger.record('summary/mean_catch_rate', np.mean(self.catch_rates))
            
            if self.train_side == "prey" and self.survival_rates:
                self.logger.record('summary/mean_survival_rate', np.mean(self.survival_rates))

================================================================================
FILE: src/algorithms/__init__.py
================================================================================
"""
算法模块初始化
提供算法工厂函数
"""

from typing import Dict, Any
import gymnasium as gym

from .base_algorithm import BaseAlgorithm
from .ppo_wrapper import PPOWrapper
from .a2c_wrapper import A2CWrapper
from .sac_wrapper import SACWrapper
from .td3_wrapper import TD3Wrapper
from .random_policy import RandomPolicy


# 算法映射
ALGORITHM_MAP = {
    'PPO': PPOWrapper,
    'A2C': A2CWrapper,
    'SAC': SACWrapper,
    'TD3': TD3Wrapper,
    'RANDOM': RandomPolicy
}


def create_algorithm(
    algo_name: str,
    observation_space: gym.Space,
    action_space: gym.Space,
    config: Dict[str, Any],
    device: str = "auto"
) -> BaseAlgorithm:
    """
    创建算法实例
    
    Args:
        algo_name: 算法名称（PPO/A2C/SAC/TD3/RANDOM）
        observation_space: 观察空间
        action_space: 动作空间
        config: 算法配置
        device: 计算设备
    
    Returns:
        算法实例
    """
    algo_name = algo_name.upper()
    
    if algo_name not in ALGORITHM_MAP:
        raise ValueError(
            f"未知的算法: {algo_name}. "
            f"支持的算法: {list(ALGORITHM_MAP.keys())}"
        )
    
    algo_class = ALGORITHM_MAP[algo_name]
    return algo_class(observation_space, action_space, config, device)


__all__ = [
    'BaseAlgorithm',
    'PPOWrapper',
    'A2CWrapper',
    'SACWrapper',
    'TD3Wrapper',
    'RandomPolicy',
    'create_algorithm',
    'ALGORITHM_MAP'
]

================================================================================
FILE: src/algorithms/a2c_wrapper.py
================================================================================
"""
A2C算法封装
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym
from stable_baselines3 import A2C
import torch

from .base_algorithm import BaseAlgorithm


class A2CWrapper(BaseAlgorithm):
    """A2C算法包装器"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        super().__init__(observation_space, action_space, config, device)
        
        hyperparams = config.get('hyperparameters', {})
        policy_kwargs = hyperparams.get('policy_kwargs', {}).copy()
        
        if 'activation_fn' in policy_kwargs:
            activation_str = policy_kwargs['activation_fn']
            if activation_str == "torch.nn.ReLU":
                policy_kwargs['activation_fn'] = torch.nn.ReLU
            elif activation_str == "torch.nn.Tanh":
                policy_kwargs['activation_fn'] = torch.nn.Tanh
        
        self.hyperparams = {
            'policy': hyperparams.get('policy', 'MlpPolicy'),
            'learning_rate': hyperparams.get('learning_rate', 7e-4),
            'n_steps': hyperparams.get('n_steps', 5),
            'gamma': hyperparams.get('gamma', 0.99),
            'gae_lambda': hyperparams.get('gae_lambda', 1.0),
            'ent_coef': hyperparams.get('ent_coef', 0.01),
            'vf_coef': hyperparams.get('vf_coef', 0.25),
            'max_grad_norm': hyperparams.get('max_grad_norm', 0.5),
            'rms_prop_eps': hyperparams.get('rms_prop_eps', 1e-5),
            'normalize_advantage': hyperparams.get('normalize_advantage', False),
            'policy_kwargs': policy_kwargs,
            'device': device,
            'seed': config.get('seed', None)
        }
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """创建A2C模型"""
        self.model = A2C(
            env=env,
            tensorboard_log=tensorboard_log,
            verbose=verbose,
            **self.hyperparams
        )
        return self.model
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """训练A2C"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """预测动作"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    def save(self, path: str):
        """保存模型"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        self.model.save(path)
    
    def load(self, path: str):
        """加载模型"""
        self.model = A2C.load(path, device=self.device)
        return self.model


================================================================================
FILE: src/algorithms/base_algorithm.py
================================================================================
"""
算法基类
提供统一的接口
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym


class BaseAlgorithm(ABC):
    """算法基类"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        """
        初始化算法
        
        Args:
            observation_space: 观察空间
            action_space: 动作空间
            config: 算法配置
            device: 计算设备
        """
        self.observation_space = observation_space
        self.action_space = action_space
        self.config = config
        self.device = device
        self.model = None
    
    @abstractmethod
    def train(
        self,
        env,
        total_timesteps: int,
        callback=None,
        **kwargs
    ):
        """
        训练算法
        
        Args:
            env: 训练环境
            total_timesteps: 总训练步数
            callback: 回调函数
            **kwargs: 其他参数
        """
        pass
    
    @abstractmethod
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """
        预测动作
        
        Args:
            observation: 观察
            deterministic: 是否使用确定性策略
        
        Returns:
            (action, state) 元组
        """
        pass
    
    @abstractmethod
    def save(self, path: str):
        """保存模型"""
        pass
    
    @abstractmethod
    def load(self, path: str):
        """加载模型"""
        pass
    
    @abstractmethod
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """创建模型"""
        pass
    
    def get_name(self) -> str:
        """获取算法名称"""
        return self.config.get('algorithm', {}).get('name', 'UNKNOWN')


================================================================================
FILE: src/algorithms/ppo_wrapper.py
================================================================================
"""
PPO算法封装
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym
from stable_baselines3 import PPO
import torch

from .base_algorithm import BaseAlgorithm


class PPOWrapper(BaseAlgorithm):
    """PPO算法包装器"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        super().__init__(observation_space, action_space, config, device)
        
        # 提取超参数
        hyperparams = config.get('hyperparameters', {})
        
        # 处理policy_kwargs
        policy_kwargs = hyperparams.get('policy_kwargs', {}).copy()
        if 'activation_fn' in policy_kwargs:
            activation_str = policy_kwargs['activation_fn']
            if activation_str == "torch.nn.ReLU":
                policy_kwargs['activation_fn'] = torch.nn.ReLU
            elif activation_str == "torch.nn.Tanh":
                policy_kwargs['activation_fn'] = torch.nn.Tanh
        
        self.hyperparams = {
            'policy': hyperparams.get('policy', 'MlpPolicy'),
            'learning_rate': hyperparams.get('learning_rate', 3e-4),
            'n_steps': hyperparams.get('n_steps', 2048),
            'batch_size': hyperparams.get('batch_size', 64),
            'n_epochs': hyperparams.get('n_epochs', 10),
            'gamma': hyperparams.get('gamma', 0.99),
            'gae_lambda': hyperparams.get('gae_lambda', 0.95),
            'clip_range': hyperparams.get('clip_range', 0.2),
            'clip_range_vf': hyperparams.get('clip_range_vf', None),
            'ent_coef': hyperparams.get('ent_coef', 0.01),
            'vf_coef': hyperparams.get('vf_coef', 0.5),
            'max_grad_norm': hyperparams.get('max_grad_norm', 0.5),
            'normalize_advantage': hyperparams.get('normalize_advantage', True),
            'target_kl': hyperparams.get('target_kl', None),
            'policy_kwargs': policy_kwargs,
            'device': device,
            'seed': config.get('seed', None)
        }
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """创建PPO模型 - 关键修复：直接传入env，让SB3自动获取空间"""
        self.model = PPO(
            env=env,  # SB3会从env自动获取observation_space和action_space
            tensorboard_log=tensorboard_log,
            verbose=verbose,
            **self.hyperparams
        )
        return self.model
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """训练PPO"""
        if self.model is None:
            raise ValueError("模型未初始化，请先调用 create_model()")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """预测动作"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    def save(self, path: str):
        """保存模型"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        self.model.save(path)
    
    def load(self, path: str):
        """加载模型"""
        self.model = PPO.load(path, device=self.device)
        return self.model


================================================================================
FILE: src/algorithms/random_policy.py
================================================================================
"""
Random策略（Baseline）
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym

from .base_algorithm import BaseAlgorithm


class RandomPolicy(BaseAlgorithm):
    """随机策略（Baseline）"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "cpu"
    ):
        super().__init__(observation_space, action_space, config, device)
        self.model = self  # Random策略自己就是模型
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """Random策略不需要训练"""
        print("⚠️  Random策略不需要训练，跳过训练步骤")
        pass
    
    def predict(
        self,
        observation: np.ndarray,
        state: Optional[np.ndarray] = None,
        episode_start: Optional[np.ndarray] = None,  # ← 新增参数
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        随机采样动作
        
        Args:
            observation: 观察（未使用）
            state: RNN 状态（Random策略不使用）
            episode_start: Episode开始标志（Random策略不使用）
            deterministic: 是否确定性（未使用）
        
        Returns:
            (action, state) 元组
        """
        action = self.action_space.sample()
        return action, None
    
    def save(self, path: str):
        """Random策略不需要保存"""
        print(f"ℹ️  Random策略不需要保存模型文件: {path}")
        pass
    
    def load(self, path: str):
        """Random策略不需要加载"""
        print(f"ℹ️  Random策略不需要加载模型文件: {path}")
        return self
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """Random策略不需要创建模型"""
        self.model = self
        return self


================================================================================
FILE: src/algorithms/sac_wrapper.py
================================================================================
"""
SAC算法封装
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym
from stable_baselines3 import SAC
import torch

from .base_algorithm import BaseAlgorithm


class SACWrapper(BaseAlgorithm):
    """SAC算法包装器"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        super().__init__(observation_space, action_space, config, device)
        
        hyperparams = config.get('hyperparameters', {})
        policy_kwargs = hyperparams.get('policy_kwargs', {}).copy()
        
        if 'activation_fn' in policy_kwargs:
            activation_str = policy_kwargs['activation_fn']
            if activation_str == "torch.nn.ReLU":
                policy_kwargs['activation_fn'] = torch.nn.ReLU
            elif activation_str == "torch.nn.Tanh":
                policy_kwargs['activation_fn'] = torch.nn.Tanh
        
        self.hyperparams = {
            'policy': hyperparams.get('policy', 'MlpPolicy'),
            'learning_rate': hyperparams.get('learning_rate', 3e-4),
            'buffer_size': hyperparams.get('buffer_size', 1000000),
            'learning_starts': hyperparams.get('learning_starts', 10000),
            'batch_size': hyperparams.get('batch_size', 256),
            'tau': hyperparams.get('tau', 0.005),
            'gamma': hyperparams.get('gamma', 0.99),
            'train_freq': hyperparams.get('train_freq', 1),
            'gradient_steps': hyperparams.get('gradient_steps', 1),
            'ent_coef': hyperparams.get('ent_coef', 'auto'),
            'target_entropy': hyperparams.get('target_entropy', 'auto'),
            'use_sde': hyperparams.get('use_sde', False),
            'sde_sample_freq': hyperparams.get('sde_sample_freq', -1),
            'policy_kwargs': policy_kwargs,
            'device': device,
            'seed': config.get('seed', None)
        }
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """创建SAC模型"""
        self.model = SAC(
            env=env,
            tensorboard_log=tensorboard_log,
            verbose=verbose,
            **self.hyperparams
        )
        return self.model
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """训练SAC"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """预测动作"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    def save(self, path: str):
        """保存模型"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        self.model.save(path)
    
    def load(self, path: str):
        """加载模型"""
        self.model = SAC.load(path, device=self.device)
        return self.model


================================================================================
FILE: src/algorithms/td3_wrapper.py
================================================================================
"""
TD3算法封装
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym
from stable_baselines3 import TD3
from stable_baselines3.common.noise import NormalActionNoise
import torch

from .base_algorithm import BaseAlgorithm


class TD3Wrapper(BaseAlgorithm):
    """TD3算法包装器"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        super().__init__(observation_space, action_space, config, device)
        
        hyperparams = config.get('hyperparameters', {})
        policy_kwargs = hyperparams.get('policy_kwargs', {}).copy()
        
        if 'activation_fn' in policy_kwargs:
            activation_str = policy_kwargs['activation_fn']
            if activation_str == "torch.nn.ReLU":
                policy_kwargs['activation_fn'] = torch.nn.ReLU
            elif activation_str == "torch.nn.Tanh":
                policy_kwargs['activation_fn'] = torch.nn.Tanh
        
        # 处理动作噪声
        action_noise = None
        if hyperparams.get('action_noise') == 'normal':
            noise_kwargs = hyperparams.get('action_noise_kwargs', {})
            n_actions = action_space.shape[0]
            action_noise = NormalActionNoise(
                mean=np.zeros(n_actions) + noise_kwargs.get('mean', 0.0),
                sigma=np.ones(n_actions) * noise_kwargs.get('sigma', 0.1)
            )
        
        self.hyperparams = {
            'policy': hyperparams.get('policy', 'MlpPolicy'),
            'learning_rate': hyperparams.get('learning_rate', 3e-4),
            'buffer_size': hyperparams.get('buffer_size', 1000000),
            'learning_starts': hyperparams.get('learning_starts', 10000),
            'batch_size': hyperparams.get('batch_size', 256),
            'tau': hyperparams.get('tau', 0.005),
            'gamma': hyperparams.get('gamma', 0.99),
            'train_freq': hyperparams.get('train_freq', 1),
            'gradient_steps': hyperparams.get('gradient_steps', 1),
            'policy_delay': hyperparams.get('policy_delay', 2),
            'target_policy_noise': hyperparams.get('target_policy_noise', 0.2),
            'target_noise_clip': hyperparams.get('target_noise_clip', 0.5),
            'action_noise': action_noise,
            'policy_kwargs': policy_kwargs,
            'device': device,
            'seed': config.get('seed', None)
        }
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """创建TD3模型"""
        self.model = TD3(
            env=env,
            tensorboard_log=tensorboard_log,
            verbose=verbose,
            **self.hyperparams
        )
        return self.model
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """训练TD3"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """预测动作"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    def save(self, path: str):
        """保存模型"""
        if self.model is None:
            raise ValueError("模型未初始化")
        
        self.model.save(path)
    
    def load(self, path: str):
        """加载模型"""
        self.model = TD3.load(path, device=self.device)
        return self.model


================================================================================
FILE: src/evaluation/__init__.py
================================================================================


================================================================================
FILE: src/utils/__init__.py
================================================================================


================================================================================
FILE: src/utils/banner.py
================================================================================
"""
运行模式横幅显示
提醒用户当前运行模式
"""

import sys
from typing import Dict, Any


def print_mode_banner(run_mode: str, config: Dict[str, Any]):
    """
    打印运行模式横幅
    
    Args:
        run_mode: 运行模式
        config: 模式配置
    """
    
    if run_mode == "debug":
        print("\n" + "="*70)
        print("🐛 DEBUG MODE - 调试模式")
        print("="*70)
        print(f"  训练步数: {config.get('total_timesteps', 'N/A')}")
        print(f"  并行环境: {config.get('n_envs', 'N/A')}")
        print(f"  保存模型: {'否' if not config.get('save_final_model', False) else '是'}")
        print(f"  输出目录: {config.get('output_base_dir', 'N/A')}")
        print(f"  TensorBoard: {'启用' if config.get('tensorboard_enabled', False) else '禁用'}")
        print("\n  ⚠️  此模式数据将被定期清理，不用于正式实验！")
        print("="*70 + "\n")
    
    elif run_mode == "dryrun":
        print("\n" + "="*70)
        print("🧪 DRYRUN MODE - 预演模式")
        print("="*70)
        print(f"  训练步数: {config.get('total_timesteps', 'N/A')}")
        print(f"  并行环境: {config.get('n_envs', 'N/A')}")
        print(f"  保存模型: 是（标记为DRYRUN）")
        print(f"  输出目录: {config.get('output_base_dir', 'N/A')}")
        print(f"  TensorBoard: {'启用' if config.get('tensorboard_enabled', False) else '禁用'}")
        print("\n  ℹ️  用于验证完整流程，数据保留最近 {config.get('max_runs', 3)} 次")
        print("="*70 + "\n")
    elif run_mode == "test":  # ✅ 新增
        print("\n" + "="*70)
        print("🧪 TEST MODE - 快速流程测试")
        print("="*70)
        print(f"  训练步数: {config.get('total_timesteps', 'N/A')}")
        print(f"  并行环境: {config.get('n_envs', 'N/A')}")
        print(f"  保存模型: 是（标记为TEST）")
        print(f"  输出目录: {config.get('output_base_dir', 'N/A')}")
        print(f"  TensorBoard: {'启用' if config.get('tensorboard_enabled', False) else '禁用'}")
        print("\n  ℹ️  用于测试完整训练流程，数据保留在 test_outputs/")
        print("="*70 + "\n")    
    else:  # production
        print("\n" + "="*70)
        print("✅ PRODUCTION MODE - 生产模式")
        print("="*70)
        print(f"  训练步数: {config.get('total_timesteps', 'N/A')}")
        print(f"  并行环境: {config.get('n_envs', 'N/A')}")
        print(f"  保存模型: 是")
        print(f"  输出目录: {config.get('output_base_dir', 'N/A')}")
        print(f"  TensorBoard: 启用")
        print("\n  🚨 所有数据将被永久保存，请确认配置无误！")
        print("="*70 + "\n")
        
        # 生产模式需要确认
        if config.get("require_confirmation", True):
            response = input("确认开始正式实验？(yes/no): ").strip().lower()
            if response not in ['yes', 'y']:
                print("❌ 已取消")
                sys.exit(0)


def print_training_start(
    algo: str,
    side: str,
    version: str,
    opponent_info: str
):
    """
    打印训练开始信息
    
    Args:
        algo: 算法名称
        side: 训练方（predator/prey）
        version: 版本号
        opponent_info: 对手信息
    """
    print("\n" + "="*70)
    print(f"🚀 开始训练")
    print("="*70)
    print(f"  算法: {algo}")
    print(f"  角色: {side}")
    print(f"  版本: {version}")
    print(f"  对手: {opponent_info}")
    print("="*70 + "\n")


def print_training_complete(
    algo: str,
    side: str,
    total_steps: int,
    time_elapsed: float
):
    """
    打印训练完成信息
    
    Args:
        algo: 算法名称
        side: 训练方
        total_steps: 总训练步数
        time_elapsed: 训练耗时（秒）
    """
    hours = int(time_elapsed // 3600)
    minutes = int((time_elapsed % 3600) // 60)
    seconds = int(time_elapsed % 60)
    
    print("\n" + "="*70)
    print(f"✅ 训练完成！")
    print("="*70)
    print(f"  算法: {algo}")
    print(f"  角色: {side}")
    print(f"  总步数: {total_steps:,}")
    print(f"  耗时: {hours:02d}:{minutes:02d}:{seconds:02d}")
    print("="*70 + "\n")


def print_evaluation_start(n_episodes: int):
    """打印评估开始信息"""
    print("\n" + "-"*70)
    print(f"📊 开始评估 ({n_episodes} episodes)")
    print("-"*70)


def print_evaluation_results(metrics: Dict[str, Any]):
    """
    打印评估结果
    
    Args:
        metrics: 评估指标字典
    """
    print("\n" + "-"*70)
    print("📊 评估结果")
    print("-"*70)
    
    for key, value in metrics.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
        else:
            print(f"  {key}: {value}")
    
    print("-"*70 + "\n")


def print_freeze_decision(
    algo: str,
    side: str,
    is_frozen: bool,
    reason: str = ""
):
    """
    打印冻结决策
    
    Args:
        algo: 算法名称
        side: 训练方
        is_frozen: 是否冻结
        reason: 原因说明
    """
    if is_frozen:
        print("\n" + "="*70)
        print(f"❄️  模型冻结：{algo}_{side}")
        print("="*70)
        print(f"  ✅ 达到冻结标准，已加入固定池")
        if reason:
            print(f"  原因: {reason}")
        print("="*70 + "\n")
    else:
        print("\n" + "="*70)
        print(f"⚠️  模型未冻结：{algo}_{side}")
        print("="*70)
        print(f"  ❌ 未达到冻结标准")
        if reason:
            print(f"  原因: {reason}")
        print("="*70 + "\n")

================================================================================
FILE: src/utils/cleanup.py
================================================================================
"""
清理工具
管理调试和预演数据的自动清理
"""

import shutil
from pathlib import Path
from datetime import datetime
from typing import List
import os


class OutputCleaner:
    """输出清理器"""
    
    @staticmethod
    def cleanup_debug_on_start(config: dict):
        """启动时清理调试目录"""
        if not config.get("clear_on_start", False):
            return
        
        current_dir = Path("debug_outputs/current")
        
        if current_dir.exists():
            print(f"🗑️  清空调试目录: {current_dir}")
            shutil.rmtree(current_dir)
        
        current_dir.mkdir(parents=True, exist_ok=True)
    
    @staticmethod
    def archive_debug_run():
        """归档当前调试会话"""
        current_dir = Path("debug_outputs/current")
        
        if not current_dir.exists() or not any(current_dir.iterdir()):
            print("ℹ️  调试目录为空，跳过归档")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_dir = Path(f"debug_outputs/archive/debug_{timestamp}")
        
        print(f"📦 归档调试数据到: {archive_dir}")
        archive_dir.parent.mkdir(parents=True, exist_ok=True)
        shutil.move(str(current_dir), str(archive_dir))
        
        # 清理旧归档
        OutputCleaner.cleanup_old_debug_archives()
    
    @staticmethod
    def cleanup_old_debug_archives(max_archives: int = 5):
        """删除超出限制的旧调试归档"""
        archive_dir = Path("debug_outputs/archive")
        
        if not archive_dir.exists():
            return
        
        # 获取所有归档目录
        archives = sorted(
            [d for d in archive_dir.iterdir() if d.is_dir() and d.name.startswith("debug_")],
            key=lambda x: x.name,
            reverse=True
        )
        
        # 删除超出的
        for old_archive in archives[max_archives:]:
            print(f"🗑️  删除旧调试归档: {old_archive.name}")
            shutil.rmtree(old_archive)
    
    @staticmethod
    def cleanup_old_dryruns(max_runs: int = 3):
        """清理旧的预演数据"""
        dryrun_dir = Path("dryrun_outputs")
        
        if not dryrun_dir.exists():
            return
        
        # 获取所有运行目录
        runs = sorted(
            [d for d in dryrun_dir.iterdir() if d.is_dir() and d.name.startswith("run_")],
            key=lambda x: x.name,
            reverse=True
        )
        
        # 删除超出的
        for old_run in runs[max_runs:]:
            print(f"🗑️  删除旧预演数据: {old_run.name}")
            shutil.rmtree(old_run)
    
    @staticmethod
    def get_directory_size(path: Path) -> float:
        """
        获取目录大小（MB）
        
        Args:
            path: 目录路径
        
        Returns:
            大小（MB）
        """
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                if os.path.exists(filepath):
                    total_size += os.path.getsize(filepath)
        
        return total_size / (1024 * 1024)  # 转换为MB
    
    @staticmethod
    def print_storage_summary():
        """打印存储空间使用摘要"""
        print("\n" + "="*70)
        print("💾 存储空间使用摘要")
        print("="*70)
        
        directories = {
            "outputs": Path("outputs"),
            "dryrun_outputs": Path("dryrun_outputs"),
            "debug_outputs": Path("debug_outputs")
        }
        
        for name, path in directories.items():
            if path.exists():
                size = OutputCleaner.get_directory_size(path)
                print(f"{name:20s}: {size:>10.2f} MB")
            else:
                print(f"{name:20s}: {'不存在':>10s}")
        
        print("="*70 + "\n")


# 便捷函数
def cleanup_debug(config: dict):
    """清理调试数据"""
    OutputCleaner.cleanup_debug_on_start(config)


def archive_debug():
    """归档调试数据"""
    OutputCleaner.archive_debug_run()


def cleanup_dryrun(max_runs: int = 3):
    """清理预演数据"""
    OutputCleaner.cleanup_old_dryruns(max_runs)

================================================================================
FILE: src/utils/config_loader.py
================================================================================
"""
配置加载工具
"""

import os
import yaml
from typing import Dict, Any, Optional
from pathlib import Path


class ConfigLoader:
    """配置加载器"""
    
    def __init__(self, config_root: str = "configs"):
        self.config_root = Path(config_root)
        
        # 缓存已加载的配置
        self._cache = {}
    
    def load_yaml(self, config_path: str) -> Dict[str, Any]:
        """
        加载YAML配置文件
        
        Args:
            config_path: 相对于config_root的路径，或绝对路径
        
        Returns:
            配置字典
        """
        # 检查缓存
        if config_path in self._cache:
            return self._cache[config_path]
        
        # 构建完整路径
        if os.path.isabs(config_path):
            full_path = Path(config_path)
        else:
            full_path = self.config_root / config_path
        
        # 加载YAML
        if not full_path.exists():
            raise FileNotFoundError(f"配置文件不存在: {full_path}")
        
        with open(full_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # 缓存
        self._cache[config_path] = config
        
        return config
    
    def load_run_mode_config(self, mode: str) -> Dict[str, Any]:
        """加载运行模式配置"""
        config = self.load_yaml("run_modes.yaml")
        
        if mode not in config:
            raise ValueError(f"未知的运行模式: {mode}. 支持的模式: {list(config.keys())}")
        
        return config[mode]
    
    def load_environment_config(self, env_name: str = "waterworld_standard") -> Dict[str, Any]:
        """加载环境配置"""
        return self.load_yaml(f"environments/{env_name}.yaml")
    
    def load_algorithm_config(self, algo_name: str) -> Dict[str, Any]:
        """加载算法配置"""
        return self.load_yaml(f"algorithms/{algo_name.lower()}.yaml")
    
    def load_training_config(self, stage_name: str) -> Dict[str, Any]:
        """加载训练阶段配置"""
        return self.load_yaml(f"training/{stage_name}.yaml")
    
    def get_freeze_criteria(self, side: str) -> Dict[str, Any]:
        """获取冻结条件"""
        config = self.load_yaml("run_modes.yaml")
        return config['freeze_criteria'][side]
    
    def get_fixed_pool_config(self) -> Dict[str, Any]:
        """获取固定池配置"""
        config = self.load_yaml("run_modes.yaml")
        return config['fixed_pool']
    
    def merge_configs(self, *configs: Dict[str, Any]) -> Dict[str, Any]:
        """
        合并多个配置（后面的覆盖前面的）
        
        Args:
            *configs: 多个配置字典
        
        Returns:
            合并后的配置
        """
        merged = {}
        
        for config in configs:
            merged = self._deep_merge(merged, config)
        
        return merged
    
    def _deep_merge(self, base: Dict, update: Dict) -> Dict:
        """深度合并字典"""
        result = base.copy()
        
        for key, value in update.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        
        return result


# 全局配置加载器实例
config_loader = ConfigLoader()


# 便捷函数
def load_config(config_path: str) -> Dict[str, Any]:
    """加载配置的便捷函数"""
    return config_loader.load_yaml(config_path)


def get_mode_config(mode: str) -> Dict[str, Any]:
    """获取运行模式配置"""
    return config_loader.load_run_mode_config(mode)


def get_env_config(env_name: str = "waterworld_standard") -> Dict[str, Any]:
    """获取环境配置"""
    return config_loader.load_environment_config(env_name)


def get_algo_config(algo_name: str) -> Dict[str, Any]:
    """获取算法配置"""
    return config_loader.load_algorithm_config(algo_name)


def get_training_config(stage_name: str) -> Dict[str, Any]:
    """获取训练配置"""
    return config_loader.load_training_config(stage_name)




# from src.utils.config_loader import get_mode_config, get_env_config, get_algo_config

# # 加载运行模式配置
# debug_config = get_mode_config("debug")
# print(f"Debug模式训练步数: {debug_config['total_timesteps']}")

# # 加载环境配置
# env_config = get_env_config("waterworld_standard")
# print(f"Predator数量: {env_config['environment']['n_predators']}")

# # 加载算法配置
# ppo_config = get_algo_config("PPO")
# print(f"PPO学习率: {ppo_config['hyperparameters']['learning_rate']}")

# # 合并配置
# from src.utils.config_loader import config_loader

# final_config = config_loader.merge_configs(
#     debug_config,
#     {"total_timesteps": 5000}  # 覆盖默认值
# )
# print(f"最终训练步数: {final_config['total_timesteps']}")

================================================================================
FILE: src/utils/config_snapshot.py
================================================================================
"""
配置快照工具
保存每次训练的完整配置，确保可复现
"""

import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, Any


def make_json_serializable(obj):
    """将对象转换为JSON可序列化的格式"""
    if isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [make_json_serializable(v) for v in obj]
    elif isinstance(obj, tuple):
        return tuple(make_json_serializable(v) for v in obj)
    elif isinstance(obj, type):
        # 将类型对象转换为字符串
        return f"{obj.__module__}.{obj.__name__}"
    elif hasattr(obj, '__class__') and obj.__class__.__module__ not in ['builtins', '__builtin__']:
        # 复杂对象转为字符串表示
        return str(obj)
    else:
        return obj


class ConfigSnapshot:
    """配置快照管理器"""
    
    @staticmethod
    def save_snapshot(config: Dict[str, Any], save_dir: Path, filename: str):
        save_dir.mkdir(parents=True, exist_ok=True)
        
        snapshot = {
            'timestamp': datetime.now().isoformat(),
            'config': make_json_serializable(config)
        }
        
        # 保存为YAML
        yaml_path = save_dir / f"{filename}.yaml"
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(snapshot, f, default_flow_style=False, allow_unicode=True)
        
        # 尝试保存为JSON
        json_path = save_dir / f"{filename}.json"
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(snapshot, f, indent=2, ensure_ascii=False)
        except TypeError as e:
            print(f"⚠️  JSON序列化失败，仅保存YAML: {e}")
        
        print(f"💾 配置快照已保存: {yaml_path}")
    
    @staticmethod
    def load_snapshot(snapshot_path: Path) -> Dict[str, Any]:
        if snapshot_path.suffix == '.yaml':
            with open(snapshot_path, 'r', encoding='utf-8') as f:
                snapshot = yaml.safe_load(f)
        elif snapshot_path.suffix == '.json':
            with open(snapshot_path, 'r', encoding='utf-8') as f:
                snapshot = json.load(f)
        else:
            raise ValueError(f"不支持的文件格式: {snapshot_path.suffix}")
        
        return snapshot.get('config', {})
    
    @staticmethod
    def save_training_summary(summary: Dict[str, Any], save_dir: Path, filename: str):
        save_dir.mkdir(parents=True, exist_ok=True)
        
        summary['saved_at'] = datetime.now().isoformat()
        serializable_summary = make_json_serializable(summary)
        
        json_path = save_dir / f"{filename}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_summary, f, indent=2, ensure_ascii=False)
        
        print(f"📝 训练摘要已保存: {json_path}")


def save_config_snapshot(config: Dict, save_dir: Path, name: str):
    ConfigSnapshot.save_snapshot(config, save_dir, name)


def save_training_summary(summary: Dict, save_dir: Path, name: str):
    ConfigSnapshot.save_training_summary(summary, save_dir, name)


================================================================================
FILE: src/utils/config_validator.py
================================================================================
"""
配置验证工具
防止配置错误导致训练失败
"""

import sys
from typing import Dict, Any, List
from src.utils.config_loader import config_loader


class ConfigValidator:
    """配置验证器"""
    
    def __init__(self):
        self.errors = []
        self.warnings = []
    
    def validate_run_mode(self, mode: str, config: Dict[str, Any]) -> bool:
        """
        验证运行模式配置
        
        Args:
            mode: 运行模式 (debug/dryrun/prod)
            config: 配置字典
        
        Returns:
            是否通过验证
        """
        self.errors = []
        self.warnings = []
        
        # 加载验证规则
        validation_rules = config_loader.load_yaml("run_modes.yaml").get("validation", {})
        
        if mode == "prod":
            self._validate_production_mode(config, validation_rules.get("prod", {}))
        elif mode == "debug":
            self._validate_debug_mode(config, validation_rules.get("debug", {}))
        
        return len(self.errors) == 0
    
    def _validate_production_mode(self, config: Dict, rules: Dict):
        """验证生产模式"""
        
        # 检查训练步数
        min_timesteps = rules.get("min_timesteps", 500000)
        if config.get("total_timesteps", 0) < min_timesteps:
            self.errors.append(
                f"⚠️  生产模式训练步数过低: {config['total_timesteps']} "
                f"(建议至少 {min_timesteps} 步)"
            )
        
        # 检查实验名称
        if "experiment_name" in config:
            exp_name = config["experiment_name"].lower()
            forbidden = rules.get("forbidden_keywords", [])
            
            for keyword in forbidden:
                if keyword in exp_name:
                    self.warnings.append(
                        f"⚠️  实验名称 '{config['experiment_name']}' 包含 '{keyword}' "
                        "字样，确认这是正式实验吗？"
                    )
        
        # 检查是否启用了保存
        if not config.get("save_final_model", True):
            self.warnings.append(
                "⚠️  生产模式未启用模型保存，这可能不是你想要的！"
            )
    
    def _validate_debug_mode(self, config: Dict, rules: Dict):
        """验证调试模式"""
        
        # 检查训练步数是否过多
        max_timesteps = rules.get("max_timesteps", 10000)
        if config.get("total_timesteps", 0) > max_timesteps:
            self.warnings.append(
                f"💡 调试模式训练步数较多 ({config['total_timesteps']})，"
                "可能耗时较长，考虑降低步数？"
            )
    
    def validate_environment_config(self, config: Dict[str, Any]) -> bool:
        """验证环境配置"""
        self.errors = []
        self.warnings = []
        
        env_config = config.get("environment", {})
        
        # 必需字段
        required_fields = [
            "n_predators", "n_preys", "max_cycles",
            "predator_speed", "prey_speed"
        ]
        
        for field in required_fields:
            if field not in env_config:
                self.errors.append(f"❌ 环境配置缺少必需字段: {field}")
        
        # 合理性检查
        if env_config.get("n_predators", 0) > env_config.get("n_preys", 0):
            self.warnings.append(
                "⚠️  Predator数量多于Prey，可能导致训练不平衡"
            )
        
        if env_config.get("predator_speed", 0) < env_config.get("prey_speed", 0):
            self.warnings.append(
                "⚠️  Predator速度慢于Prey，可能难以捕获"
            )
        
        return len(self.errors) == 0
    
    def validate_algorithm_config(self, algo_name: str, config: Dict[str, Any]) -> bool:
        """验证算法配置"""
        self.errors = []
        self.warnings = []
        
        hyperparams = config.get("hyperparameters", {})
        
        # 检查学习率
        lr = hyperparams.get("learning_rate")
        if lr is not None:
            if lr > 0.01:
                self.warnings.append(
                    f"⚠️  {algo_name} 学习率较高 ({lr})，可能导致训练不稳定"
                )
            elif lr < 1e-6:
                self.warnings.append(
                    f"⚠️  {algo_name} 学习率过低 ({lr})，可能学习缓慢"
                )
        
        return len(self.errors) == 0
    
    def validate_training_config(self, stage_config: Dict[str, Any]) -> bool:
        """验证训练阶段配置"""
        self.errors = []
        self.warnings = []
        
        # 检查对手配置
        opponent = stage_config.get("opponent", {})
        if not opponent:
            self.errors.append("❌ 缺少对手配置")
        else:
            opp_type = opponent.get("type")
            if opp_type == "mixed_pool":
                pool_path = opponent.get("pool_path")
                if not pool_path:
                    self.errors.append("❌ mixed_pool模式需要指定pool_path")
        
        # 检查训练算法列表
        algos = stage_config.get("algorithms_to_train", [])
        if not algos:
            self.errors.append("❌ 未指定要训练的算法")
        
        return len(self.errors) == 0
    
    def print_results(self):
        """打印验证结果"""
        if self.errors:
            print("\n❌ 配置错误:")
            for err in self.errors:
                print(f"  {err}")
        
        if self.warnings:
            print("\n⚠️  配置警告:")
            for warn in self.warnings:
                print(f"  {warn}")
    
    def require_confirmation(self) -> bool:
        """
        如果有警告，要求用户确认
        
        Returns:
            用户是否确认继续
        """
        if not self.warnings:
            return True
        
        self.print_results()
        response = input("\n继续吗？(yes/no): ").strip().lower()
        return response in ['yes', 'y']


# 全局验证器实例
validator = ConfigValidator()


def validate_config(mode: str, config: Dict[str, Any]) -> bool:
    """
    验证配置的便捷函数
    
    Args:
        mode: 运行模式
        config: 配置字典
    
    Returns:
        是否通过验证
    """
    if not validator.validate_run_mode(mode, config):
        validator.print_results()
        return False
    
    return True

================================================================================
FILE: src/utils/logger.py
================================================================================
"""
日志工具
提供统一的日志接口
"""

import logging
import sys
from pathlib import Path
from typing import Optional
from datetime import datetime


class TrainingLogger:
    """训练日志器"""
    
    def __init__(
        self,
        name: str,
        log_file: Optional[Path] = None,
        log_level: str = "INFO",
        console_output: bool = True
    ):
        """
        初始化日志器
        
        Args:
            name: 日志器名称
            log_file: 日志文件路径（None则不写文件）
            log_level: 日志级别（DEBUG/INFO/WARNING/ERROR）
            console_output: 是否输出到控制台
        """
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        # 清除已有的handlers
        self.logger.handlers = []
        
        # 日志格式
        formatter = logging.Formatter(
            '[%(asctime)s] [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # 控制台输出
        if console_output:
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setLevel(getattr(logging, log_level.upper()))
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
        
        # 文件输出
        if log_file:
            log_file.parent.mkdir(parents=True, exist_ok=True)
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(getattr(logging, log_level.upper()))
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def debug(self, msg: str):
        """Debug级别日志"""
        self.logger.debug(msg)
    
    def info(self, msg: str):
        """Info级别日志"""
        self.logger.info(msg)
    
    def warning(self, msg: str):
        """Warning级别日志"""
        self.logger.warning(msg)
    
    def error(self, msg: str):
        """Error级别日志"""
        self.logger.error(msg)
    
    def critical(self, msg: str):
        """Critical级别日志"""
        self.logger.critical(msg)
    
    def log_config(self, config: dict, title: str = "Configuration"):
        """记录配置信息"""
        self.info(f"\n{'='*70}")
        self.info(f"{title}")
        self.info(f"{'='*70}")
        
        for key, value in config.items():
            if isinstance(value, dict):
                self.info(f"{key}:")
                for sub_key, sub_value in value.items():
                    self.info(f"  {sub_key}: {sub_value}")
            else:
                self.info(f"{key}: {value}")
        
        self.info(f"{'='*70}\n")
    
    def log_banner(self, text: str, char: str = "="):
        """打印横幅"""
        banner = char * 70
        self.info(f"\n{banner}")
        self.info(text)
        self.info(f"{banner}\n")


def create_logger(
    name: str,
    log_dir: Optional[Path] = None,
    log_level: str = "INFO"
) -> TrainingLogger:
    """
    创建日志器的便捷函数
    
    Args:
        name: 日志器名称
        log_dir: 日志目录（None则不写文件）
        log_level: 日志级别
    
    Returns:
        日志器实例
    """
    if log_dir:
        log_file = log_dir / f"{name}.log"
    else:
        log_file = None
    
    return TrainingLogger(name, log_file, log_level)

================================================================================
FILE: src/utils/naming.py
================================================================================
"""
文件命名规范工具
确保所有文件名都包含必要信息且格式一致
"""

from datetime import datetime
from typing import Optional


class FileNaming:
    """文件命名工具"""
    
    @staticmethod
    def generate_model_filename(
        train_algo: str,
        train_side: str,
        version: str,
        opponent_info: str,
        run_mode: str,
        extension: str = "zip"
    ) -> str:
        """
        生成模型文件名
        
        Args:
            train_algo: 训练算法（如 PPO）
            train_side: 训练方（predator/prey）
            version: 版本号（如 v1）
            opponent_info: 对手信息（如 RANDOM_pred 或 MIX_pool_v1）
            run_mode: 运行模式（debug/dryrun/prod）
            extension: 文件扩展名
        
        Returns:
            文件名
        
        Example:
            PPO_prey_v1_vs_RANDOM_pred_20251013_143022.zip
            DRYRUN_SAC_pred_v2_vs_MIX_pool_v1_20251013_151033.zip
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 添加模式前缀
        if run_mode == "debug":
            prefix = "DEBUG_"
        elif run_mode == "dryrun":
            prefix = "DRYRUN_"
        else:
            prefix = ""
        
        # 构建文件名
        filename = (
            f"{prefix}{train_algo}_{train_side}_{version}"
            f"_vs_{opponent_info}_{timestamp}.{extension}"
        )
        
        return filename
    
    @staticmethod
    def generate_checkpoint_filename(
        train_algo: str,
        train_side: str,
        step: int,
        extension: str = "zip"
    ) -> str:
        """
        生成检查点文件名
        
        Args:
            train_algo: 训练算法
            train_side: 训练方
            step: 训练步数
            extension: 文件扩展名
        
        Returns:
            文件名
        
        Example:
            PPO_prey_step_500000.zip
        """
        return f"{train_algo}_{train_side}_step_{step}.{extension}"
    
    @staticmethod
    def generate_config_filename(
        train_algo: str,
        train_side: str,
        version: str,
        extension: str = "yaml"
    ) -> str:
        """
        生成配置快照文件名
        
        Example:
            PPO_prey_v1_config.yaml
        """
        return f"{train_algo}_{train_side}_{version}_config.{extension}"
    
    @staticmethod
    def generate_log_filename(
        train_algo: str,
        train_side: str,
        version: str,
        extension: str = "log"
    ) -> str:
        """
        生成日志文件名
        
        Example:
            PPO_prey_v1_training.log
        """
        return f"{train_algo}_{train_side}_{version}_training.{extension}"
    
    @staticmethod
    def generate_summary_filename(
        train_algo: str,
        train_side: str,
        version: str,
        extension: str = "json"
    ) -> str:
        """
        生成训练摘要文件名
        
        Example:
            PPO_prey_v1_summary.json
        """
        return f"{train_algo}_{train_side}_{version}_summary.{extension}"
    
    @staticmethod
    def parse_model_filename(filename: str) -> dict:
        """
        解析模型文件名，提取信息
        
        Args:
            filename: 模型文件名
        
        Returns:
            包含解析信息的字典
        
        Example:
            Input: "PPO_prey_v1_vs_RANDOM_pred_20251013_143022.zip"
            Output: {
                'algo': 'PPO',
                'side': 'prey',
                'version': 'v1',
                'opponent': 'RANDOM_pred',
                'timestamp': '20251013_143022',
                'is_debug': False,
                'is_dryrun': False
            }
        """
        # 去除扩展名
        name = filename.rsplit('.', 1)[0]
        
        # 检查模式前缀
        is_debug = name.startswith("DEBUG_")
        is_dryrun = name.startswith("DRYRUN_")
        
        if is_debug:
            name = name[6:]  # 去除 "DEBUG_"
        elif is_dryrun:
            name = name[7:]  # 去除 "DRYRUN_"
        
        # 分割字段
        parts = name.split('_')
        
        # 基本解析（假设格式正确）
        if len(parts) >= 6:
            return {
                'algo': parts[0],
                'side': parts[1],
                'version': parts[2],
                'opponent': '_'.join(parts[4:-2]),  # vs之后到时间戳之前
                'timestamp': '_'.join(parts[-2:]),
                'is_debug': is_debug,
                'is_dryrun': is_dryrun
            }
        else:
            return {}
    
    @staticmethod
    def format_opponent_info(opponent_config: dict) -> str:
        """
        根据对手配置生成对手信息字符串
        
        Args:
            opponent_config: 对手配置字典
        
        Returns:
            对手信息字符串
        
        Example:
            {"type": "algorithm", "algorithm": "RANDOM", "side": "predator"}
            -> "RANDOM_pred"
            
            {"type": "mixed_pool", "pool_path": "outputs/fixed_pools/prey_pool_v1"}
            -> "MIX_prey_pool_v1"
        """
        opp_type = opponent_config.get("type", "unknown")
        
        if opp_type == "algorithm":
            algo = opponent_config.get("algorithm", "UNKNOWN")
            side = opponent_config.get("side", "unknown")
            side_abbr = "pred" if side == "predator" else "prey"
            return f"{algo}_{side_abbr}"
        
        elif opp_type == "fixed_model":
            # 从路径提取模型名
            path = opponent_config.get("path", "")
            model_name = path.split('/')[-1].replace('.zip', '')
            return model_name
        
        elif opp_type == "mixed_pool":
            # 从池路径提取池名
            pool_path = opponent_config.get("pool_path", "")
            pool_name = pool_path.split('/')[-1]
            return f"MIX_{pool_name}"
        
        else:
            return "UNKNOWN"


# 全局命名工具实例
naming = FileNaming()


# 便捷函数
def generate_model_name(
    algo: str, side: str, version: str,
    opponent: dict, mode: str
) -> str:
    """生成模型文件名的便捷函数"""
    opponent_info = naming.format_opponent_info(opponent)
    return naming.generate_model_filename(algo, side, version, opponent_info, mode)

================================================================================
FILE: src/utils/path_manager.py
================================================================================
"""
路径管理工具
根据运行模式和实验配置生成正确的输出路径
"""

import os
from pathlib import Path
from datetime import datetime
from typing import Optional


class PathManager:
    """路径管理器"""
    
    def __init__(self, run_mode: str, experiment_name: str):
        """
        初始化路径管理器
        
        Args:
            run_mode: 运行模式 (debug/dryrun/prod)
            experiment_name: 实验名称
        """
        self.run_mode = run_mode
        self.experiment_name = experiment_name
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 根据模式设置基础目录
        if run_mode == "debug":
            self.base_dir = Path("debug_outputs/current")
        elif run_mode == "dryrun":
            self.base_dir = Path(f"dryrun_outputs/run_{self.timestamp}")
        else:  # prod
            self.base_dir = Path("outputs")
        
        # 创建基础目录
        self.base_dir.mkdir(parents=True, exist_ok=True)
    
    def get_model_dir(self, stage_name: Optional[str] = None) -> Path:
        """
        获取模型保存目录
        
        Args:
            stage_name: 训练阶段名称（如 stage1.1_prey_warmup）
        
        Returns:
            模型目录路径
        """
        if stage_name:
            path = self.base_dir / "saved_models" / stage_name
        else:
            path = self.base_dir / "saved_models" / self.experiment_name
        
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    def get_checkpoint_dir(self, stage_name: Optional[str] = None) -> Path:
        """获取检查点目录"""
        if stage_name:
            path = self.base_dir / "checkpoints" / stage_name / self.experiment_name
        else:
            path = self.base_dir / "checkpoints" / self.experiment_name
        
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    def get_tensorboard_dir(self, stage_name: Optional[str] = None) -> Path:
        """获取TensorBoard日志目录"""
        if stage_name:
            # 同一 stage 的所有实验共享一个目录
            path = self.base_dir / "tensorboard_logs" / stage_name
        else:
            path = self.base_dir / "tensorboard_logs" / self.experiment_name
        
        path.mkdir(parents=True, exist_ok=True)
        return path
    def get_experiment_dir(self, stage_name: Optional[str] = None) -> Path:
        """获取实验记录目录"""
        if stage_name:
            path = self.base_dir / "experiments" / stage_name
        else:
            path = self.base_dir / "experiments" / self.experiment_name
        
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    def get_fixed_pool_dir(self, pool_name: str) -> Path:
        """
        获取固定池目录
        
        Args:
            pool_name: 池名称（如 prey_pool_v1）
        """
        # 固定池只在正式输出目录
        path = Path("outputs/fixed_pools") / pool_name
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    def get_evaluation_dir(self) -> Path:
        """获取评估结果目录"""
        path = Path("outputs/evaluation_results")
        path.mkdir(parents=True, exist_ok=True)
        return path


def create_path_manager(run_mode: str, experiment_name: str) -> PathManager:
    """创建路径管理器的便捷函数"""
    return PathManager(run_mode, experiment_name)

================================================================================
FILE: dryrun_outputs/run_20251013_172831/experiments/baseline_stage1.1/RANDOM_prey_baseline_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: custom
      name: RANDOM
    device: cpu
    hyperparameters: {}
    seed: null
  device: cpu
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 3000
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: random_baseline_stage1.1
  generation: 0
  mode_config:
    auto_cleanup: true
    checkpoint_freq: 20000
    deterministic_eval: true
    eval_freq: 10000
    filename_prefix: DRYRUN_
    log_level: INFO
    max_runs: 3
    n_envs: 4
    n_eval_episodes: 10
    output_base_dir: dryrun_outputs
    save_checkpoints: true
    save_final_model: true
    seed: null
    tensorboard_enabled: true
    total_timesteps: 50000
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: dryrun
  seed: null
  stage_name: baseline_stage1.1
  train_algo: RANDOM
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 20000
    deterministic_eval: true
    eval_freq: 10000
    n_envs: 4
    n_eval_episodes: 10
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 50000
    verbose: 1
  version: baseline
timestamp: '2025-10-13T17:28:31.146911'


================================================================================
FILE: configs/run_modes.yaml
================================================================================
# ============================================================================
# 运行模式配置
# ============================================================================
# 三种模式：debug（快速调试）、dryrun（流程预演）、prod（正式实验）

debug:
  # 训练配置
  total_timesteps: 1000              # 只训练1000步
  n_envs: 1                          # 单环境
  n_eval_episodes: 3                 # 评估时只跑3个episode
  
  # 保存配置
  save_checkpoints: false            # 不保存检查点
  save_final_model: false            # 不保存最终模型
  checkpoint_freq: -1                # 禁用检查点
  
  # 评估配置
  eval_freq: -1                      # 禁用定期评估
  
  # 日志配置
  tensorboard_enabled: false         # 关闭TensorBoard（可选true写入debug目录）
  log_level: "DEBUG"                 # 详细日志
  verbose: 2                         # SB3详细输出
  
  # 输出管理
  output_base_dir: "debug_outputs/current"
  clear_on_start: true               # 启动时清空目录
  archive_on_exit: true              # 退出时归档到archive/
  max_archives: 5                    # 最多保留5个历史归档
  
  # 文件命名
  filename_prefix: "DEBUG_"          # 文件名前缀
  
  # 其他
  deterministic_eval: true           # 评估时使用确定性策略
  seed: 42                           # 固定随机种子（便于调试）


dryrun:
  # 训练配置
  total_timesteps: 50000             # 5万步快速验证
  n_envs: 4                          # 4个并行环境
  n_eval_episodes: 10                # 评估10个episode
  
  # 保存配置
  save_checkpoints: true             # 保存检查点（但标记为dryrun）
  save_final_model: true             # 保存最终模型
  checkpoint_freq: 20000             # 每2万步保存
  
  # 评估配置
  eval_freq: 10000                   # 每1万步评估
  
  # 日志配置
  tensorboard_enabled: true          # 启用TensorBoard
  log_level: "INFO"                  # 标准日志
  verbose: 1                         # SB3适中输出
  
  # 输出管理
  output_base_dir: "dryrun_outputs"  # 会自动添加时间戳子目录
  auto_cleanup: true                 # 自动清理旧数据
  max_runs: 3                        # 最多保留3次运行
  
  # 文件命名
  filename_prefix: "DRYRUN_"
  
  # 其他
  deterministic_eval: true
  seed: null                         # 不固定随机种子（更真实）


prod:
  # 训练配置
  total_timesteps: 1000000           # 完整训练（1M步，各阶段可覆盖）
  n_envs: 8                          # 8个并行环境
  n_eval_episodes: 100               # 评估100个episode
  
  # 保存配置
  save_checkpoints: true             # 保存检查点
  save_final_model: true             # 保存最终模型
  checkpoint_freq: 100000            # 每10万步保存
  
  # 评估配置
  eval_freq: 50000                   # 每5万步评估
  
  # 日志配置
  tensorboard_enabled: true          # 启用TensorBoard
  log_level: "INFO"                  # 标准日志
  verbose: 1                         # SB3适中输出
  
  # 输出管理
  output_base_dir: "outputs"         # 正式输出目录
  require_confirmation: true         # 启动前需要确认
  auto_backup: false                 # 不自动清理（手动管理）
  
  # 文件命名
  filename_prefix: ""                # 无前缀
  
  # 其他
  deterministic_eval: true           # 评估时确定性
  seed: null                         # 不固定种子（更鲁棒）
# ============================================================================
# TEST MODE - 快速流程测试（10000步完整流程）
# ============================================================================
test:
  # 训练配置 - 极短步数，测试完整流程
  total_timesteps: 500             # 只训练500步
  n_envs: 4                          # 4个并行环境（加快速度）
  n_eval_episodes: 2                 # 评估2个episode
  
  # 保存配置
  save_checkpoints: true             # 测试保存逻辑
  save_final_model: true             # 测试模型保存
  checkpoint_freq: 500               # 每100步保存（测试检查点）
  
  # 评估配置
  eval_freq: 500                     # 每100步评估（测试评估）

  # 日志配置
  tensorboard_enabled: true          # 测试TensorBoard
  log_level: "INFO"
  verbose: 1
  
  # 输出管理
  output_base_dir: "test_outputs"    # 专门的测试输出目录
  clear_on_start: true               # 每次测试前清空
  auto_cleanup: false                # 不自动清理（便于检查）
  
  # 文件命名
  filename_prefix: "TEST_"
  
  # 其他
  deterministic_eval: true
  seed: 42                           # 固定种子（便于复现）
  require_confirmation: false        # 不需要确认（快速启动）


# 更新验证规则，添加test模式
validation:
  prod:
    min_timesteps: 500000
    forbidden_keywords:
      - "test"
      - "debug"
      - "tmp"
    require_notes: false
  
  debug:
    max_timesteps: 10000
    warn_if_exceed: true
  
  test:  # 新增
    max_timesteps: 50000             # 测试模式最多5万步
    warn_if_exceed: true

# ============================================================================
# 模式验证规则
# ============================================================================
validation:
  prod:
    min_timesteps: 500000            # 生产模式最少50万步
    forbidden_keywords:              # 实验名称不能包含这些词
      - "test"
      - "debug"
      - "tmp"
    require_notes: false             # 是否要求填写实验备注
  
  debug:
    max_timesteps: 10000             # 调试模式最多1万步（警告）
    warn_if_exceed: true


# ============================================================================
# 冻结条件（什么时候将模型加入固定池）
# ============================================================================
freeze_criteria:
  prey:
    min_survival_rate: 0.65          # 最低生存率
    min_avg_reward: 2.0              # 最低平均奖励
    min_episodes: 100                # 至少评估100个episode
    
  predator:
    min_catch_rate: 0.55             # 最低捕获率
    min_avg_reward: 2.0              # 最低平均奖励
    min_episodes: 100


# ============================================================================
# 固定池管理
# ============================================================================
fixed_pool:
  max_versions: 5                    # 每个算法最多保留5个版本
  similarity_threshold: 0.8          # KL散度>0.8认为相似，替换旧版本
  auto_maintain: true                # 自动维护池大小

================================================================================
FILE: configs/environments/waterworld_fast.yaml
================================================================================
action_space:
  dtype: float32
  high: 1.0
  low: -1.0
  shape:
  - 2
  type: Box
environment:
  evader_speed: 0.01
  local_ratio: 0.5
  max_cycles: 500
  n_evaders: 90
  n_obstacles: 2
  n_poisons: 10
  n_predators: 5
  n_preys: 10
  name: waterworld_v4
  obstacle_coord:
  - - 0.2
    - 0.2
  - - 0.8
    - 0.2
  poison_speed: 0.01
  predator_speed: 0.06
  prey_speed: 0.001
  render_mode: null
  sensor_range: 0.8
  static_food: true
  static_poison: true
  thrust_penalty: 0.0
observation_space:
  dtype: float32
  shape:
  - 212
  type: Box


================================================================================
FILE: configs/environments/waterworld_standard.yaml
================================================================================
# ============================================================================
# Waterworld 标准环境配置（用于正式实验）
# ============================================================================

environment:
  name: "waterworld_v4"
  
  # 智能体数量
  n_predators: 5
  n_preys: 10
  n_evaders: 90                      # 食物数量
  n_poisons: 10
  n_obstacles: 2
  
  # 障碍物位置（固定）
  obstacle_coord:
    - [0.2, 0.2]
    - [0.8, 0.2]
  
  # 速度配置
  predator_speed: 0.06               # Predator速度
  prey_speed: 0.001                  # Prey速度（更慢，容易被抓）
  evader_speed: 0.01                 # 食物速度
  poison_speed: 0.01                 # 毒药速度
  
  # 传感器配置
  sensor_range: 0.8                  # 传感器范围（增大以便智能体感知更多）
  
  # 奖励配置
  thrust_penalty: 0.0                # 移动惩罚（0=无惩罚）
  local_ratio: 0.5                   # 局部奖励比例
  
  # 环境设置
  max_cycles: 3000                   # 最大步数
  static_food: true                  # 食物静态（不移动）
  static_poison: true                # 毒药静态
  
  # 渲染设置
  render_mode: null                  # 训练时不渲染（null），评估时可设为"rgb_array"

# 观察和动作空间（自动从环境获取，这里仅文档用途）
observation_space:
  type: "Box"
  shape: [212]                       # Waterworld默认观察维度
  dtype: "float32"

action_space:
  type: "Box"
  shape: [2]                         # [Δx, Δy]
  low: -1.0
  high: 1.0
  dtype: "float32"

================================================================================
FILE: configs/training/stage1_1_prey_warmup.yaml
================================================================================
# ============================================================================
# Stage 1.1: Prey 预热训练配置
# ============================================================================

stage:
  name: "stage1.1_prey_warmup"
  description: "训练Prey对抗RANDOM Predator，建立初始Prey池"
  generation: 0

# 训练方配置
train_side: "prey"                   # 训练哪一方

# 要训练的算法列表
algorithms_to_train:
  - "PPO"
  - "A2C"
  - "SAC"
  - "TD3"
  # RANDOM不需要训练

# 对手配置
opponent:
  side: "predator"
  type: "algorithm"                  # algorithm / fixed_model / mixed_pool
  algorithm: "RANDOM"                # 使用RANDOM算法
  freeze: true                       # 冻结对手参数

# 环境配置（引用）
environment_config: "configs/environments/waterworld_standard.yaml"

# 训练配置（会覆盖run_mode中的默认值）
training:
  # prod模式下的配置
  prod:
    total_timesteps: 1000000         # 100万步
    eval_freq: 50000
    checkpoint_freq: 100000
  
  # dryrun模式下的配置
  dryrun:
    total_timesteps: 50000
    eval_freq: 10000
    checkpoint_freq: 20000
  
  # debug模式使用默认配置（不覆盖）

# 冻结条件（达到此标准则加入fixed_pool）
freeze_on_success:
  enabled: true
  criteria:
    survival_rate: 0.65
    avg_reward: 2.0
    min_eval_episodes: 100
  save_to_pool: "prey_pool_v1"

# 输出配置
output:
  save_models: true
  save_tensorboard: true
  save_config_snapshot: true         # 保存此配置的快照

================================================================================
FILE: configs/training/stage1_2_pred_guided.yaml
================================================================================
# ============================================================================
# Stage 1.2: Predator 引导训练配置
# ============================================================================

stage:
  name: "stage1.2_pred_guided"
  description: "训练Predator对抗固定的Prey池v1"
  generation: 1

train_side: "predator"

algorithms_to_train:
  - "PPO"
  - "A2C"
  - "SAC"
  - "TD3"

# 对手配置：从固定池采样
opponent:
  side: "prey"
  type: "mixed_pool"                 # 混合池模式
  pool_path: "outputs/fixed_pools/prey_pool_v1"
  mix_strategy:
    fixed_ratio: 0.7                 # 70%从固定池
    random_ratio: 0.3                # 30%用RANDOM
  sampling: "uniform"                # uniform / weighted（按性能加权）
  freeze: true

environment_config: "configs/environments/waterworld_standard.yaml"

training:
  prod:
    total_timesteps: 1000000
    eval_freq: 50000
    checkpoint_freq: 100000
  
  dryrun:
    total_timesteps: 50000
    eval_freq: 10000
    checkpoint_freq: 20000

freeze_on_success:
  enabled: true
  criteria:
    catch_rate: 0.55
    avg_reward: 2.0
    min_eval_episodes: 100
  save_to_pool: "predator_pool_v1"

output:
  save_models: true
  save_tensorboard: true
  save_config_snapshot: true

================================================================================
FILE: configs/training/stage1_3_coevolution.yaml
================================================================================
# ============================================================================
# Stage 1.3: 共进化训练配置
# ============================================================================

stage:
  name: "stage1.3_coevolution"
  description: "Predator和Prey交替训练，共同进化"

# 共进化特定配置
coevolution:
  max_generations: 4                # 最多20代
  start_generation: 2                # 从第2代开始（0,1已在stage1.1和1.2完成）
  
  # 奇偶代交替规则
  alternation:
    even_gen: "predator"             # 偶数代训练predator
    odd_gen: "prey"                  # 奇数代训练prey
  
  # 收敛条件
  convergence:
    enabled: true
    check_last_n_gens: 5             # 检查最近5代
    performance_change_threshold: 0.03  # 性能变化<3%
    balance_threshold: 0.05          # CatchRate接近0.5±0.05
  
  # 初始对手池
  initial_pools:
    prey_pool: "outputs/fixed_pools/prey_pool_v1"
    predator_pool: "outputs/fixed_pools/predator_pool_v1"

algorithms_to_train:
  - "PPO"
  - "A2C"
  - "SAC"
  - "TD3"

# 每代的对手配置（动态）
opponent:
  type: "mixed_pool"
  mix_strategy:
    fixed_ratio: 0.7
    random_ratio: 0.3
  sampling: "uniform"
  freeze: true

environment_config: "configs/environments/waterworld_standard.yaml"

# 每代的训练配置
training:
  prod:
    total_timesteps: 400000          # 每代40万步（总共20代=800万步）
    eval_freq: 50000
    checkpoint_freq: 100000
  
  dryrun:
    total_timesteps: 20000           # 每代2万步
    eval_freq: 5000
    checkpoint_freq: 10000

# 冻结条件（每代）
freeze_on_success:
  enabled: true
  criteria:
    # Prey
    prey:
      survival_rate: 0.45            # 共进化阶段标准可以更宽松
      avg_reward: 1.0
      min_eval_episodes: 50
    # Predator
    predator:
      catch_rate: 0.45
      avg_reward: 1.0
      min_eval_episodes: 50
  
  # 动态池管理
  pool_management:
    max_versions_per_algo: 5         # 每个算法最多5个版本
    auto_prune: true                 # 自动淘汰相似版本

output:
  save_models: true
  save_tensorboard: true
  save_config_snapshot: true
  per_generation_subfolder: true     # 每代一个子文件夹

================================================================================
FILE: configs/training/stage1_3_coevolution_test.yaml
================================================================================
# ============================================================================
# Stage 1.3: 共进化训练配置 - TEST版本（仅用于快速测试流程）
# ============================================================================

stage:
  name: "stage1.3_coevolution"
  description: "Predator和Prey交替训练，共同进化 - 测试版本"

# ✅ 测试版本：只跑1代
coevolution:
  max_generations: 2                # start=2, max=2 → 只跑1代
  start_generation: 2
  
  alternation:
    even_gen: "predator"
    odd_gen: "prey"
  
  convergence:
    enabled: false                  # 测试时不检查收敛
  
  initial_pools:
    prey_pool: "outputs/fixed_pools/prey_pool_v1"
    predator_pool: "outputs/fixed_pools/predator_pool_v1"

# ✅ 测试时可以只跑1-2个算法加快速度
algorithms_to_train:
  - "PPO"
  - "A2C"
  # 测试时注释掉 SAC 和 TD3 以加快速度
  # - "SAC"
  # - "TD3"

opponent:
  type: "mixed_pool"
  mix_strategy:
    fixed_ratio: 0.7
    random_ratio: 0.3
  sampling: "uniform"
  freeze: true

environment_config: "configs/environments/waterworld_standard.yaml"

# ✅ 测试配置（极短步数）
training:
  test:
    total_timesteps: 500            # 每代只跑500步
    eval_freq: 500
    checkpoint_freq: 500
  
  # 不需要其他模式的配置

freeze_on_success:
  enabled: false                    # 测试时不检查冻结条件
  
output:
  save_models: true
  save_tensorboard: true
  save_config_snapshot: true
  per_generation_subfolder: true

================================================================================
FILE: configs/algorithms/a2c.yaml
================================================================================
# ============================================================================
# A2C 算法配置
# ============================================================================

algorithm:
  name: "A2C"
  class: "stable_baselines3.A2C"

hyperparameters:
  learning_rate: 7.0e-4              # A2C通常需要更高学习率
  
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      vf: [256, 256]
    activation_fn: "torch.nn.ReLU"
  
  n_steps: 5                         # A2C每5步更新一次
  gamma: 0.99
  gae_lambda: 1.0                    # A2C通常用1.0
  
  ent_coef: 0.01
  vf_coef: 0.25                      # A2C的vf_coef通常较小
  max_grad_norm: 0.5
  
  rms_prop_eps: 1.0e-5               # RMSprop特定参数
  normalize_advantage: false         # A2C通常不标准化

device: "auto"
seed: null

================================================================================
FILE: configs/algorithms/ppo.yaml
================================================================================
# ============================================================================
# PPO 算法配置
# ============================================================================

algorithm:
  name: "PPO"
  class: "stable_baselines3.PPO"

# 超参数
hyperparameters:
  # 学习率
  learning_rate: 3.0e-4
  
  # 网络架构
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256]                 # Actor网络
      vf: [256, 256]                 # Critic网络
    activation_fn: "torch.nn.ReLU"
  
  # 训练参数
  n_steps: 2048                      # 每次更新收集的步数
  batch_size: 64                     # 小批量大小
  n_epochs: 10                       # 每次更新的epoch数
  gamma: 0.99                        # 折扣因子
  gae_lambda: 0.95                   # GAE lambda
  
  # PPO特定参数
  clip_range: 0.2                    # PPO裁剪范围
  clip_range_vf: null                # Value function裁剪（null=不裁剪）
  
  # 正则化
  ent_coef: 0.01                     # 熵系数（鼓励探索）
  vf_coef: 0.5                       # Value function系数
  max_grad_norm: 0.5                 # 梯度裁剪
  
  # 其他
  normalize_advantage: true          # 标准化优势函数
  target_kl: null                    # 目标KL散度（null=不使用）
  
# 设备配置
device: "auto"                       # auto会自动选择cuda/cpu

# 种子（null表示随机）
seed: null

================================================================================
FILE: configs/algorithms/random.yaml
================================================================================
# ============================================================================
# Random Policy 配置（Baseline）
# ============================================================================

algorithm:
  name: "RANDOM"
  class: "custom"                    # 自定义实现

# Random策略没有超参数
hyperparameters: {}

# 用于统一接口
device: "cpu"
seed: null

================================================================================
FILE: configs/algorithms/sac.yaml
================================================================================
# ============================================================================
# SAC 算法配置
# ============================================================================

algorithm:
  name: "SAC"
  class: "stable_baselines3.SAC"

hyperparameters:
  learning_rate: 3.0e-4
  
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      qf: [256, 256]                 # Q网络
    activation_fn: "torch.nn.ReLU"
  
  buffer_size: 1000000               # Replay buffer大小
  learning_starts: 10000             # 开始学习前的随机步数
  batch_size: 256                    # 从buffer采样的批量大小
  tau: 0.005                         # 软更新系数
  gamma: 0.99
  
  # SAC特定参数
  train_freq: 1                      # 每步都训练
  gradient_steps: 1                  # 每次训练的梯度步数
  
  # 自动调整熵系数
  ent_coef: "auto"                   # 自动调整alpha
  target_entropy: "auto"             # 目标熵（auto=-action_dim）
  
  # 其他
  use_sde: false                     # 是否使用状态依赖探索
  sde_sample_freq: -1

device: "auto"
seed: null

================================================================================
FILE: configs/algorithms/td3.yaml
================================================================================
# ============================================================================
# TD3 算法配置
# ============================================================================

algorithm:
  name: "TD3"
  class: "stable_baselines3.TD3"

hyperparameters:
  learning_rate: 3.0e-4
  
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      qf: [256, 256]
    activation_fn: "torch.nn.ReLU"
  
  buffer_size: 1000000
  learning_starts: 10000
  batch_size: 256                    # TD3推荐256
  tau: 0.005
  gamma: 0.99
  
  # TD3特定参数
  train_freq: 1
  gradient_steps: 1
  policy_delay: 2                    # 延迟策略更新（TD3核心）
  target_policy_noise: 0.2           # 目标策略噪声
  target_noise_clip: 0.5             # 噪声裁剪
  
  # 探索噪声
  action_noise: "normal"             # normal或ornstein-uhlenbeck
  action_noise_kwargs:
    mean: 0.0
    sigma: 0.1

device: "auto"
seed: null

================================================================================
FILE: configs/evaluation/cross_eval.yaml
================================================================================
# ============================================================================
# 交叉评估配置（Cross-Evaluation Configuration）
# ============================================================================

evaluation:
  # 评估模式
  mode: "test"  # test / dryrun / prod
  
  # 模型池路径（根据模式自动选择）
  model_pools:
    test:
      predator_pool: "outputs/fixed_pools/predator_pool_v1"
      prey_pool: "outputs/fixed_pools/prey_pool_v1"
      saved_models_base: "outputs/saved_models"
    
    dryrun:
      predator_pool: "dryrun_outputs/fixed_pools/predator_pool_v1"
      prey_pool: "dryrun_outputs/fixed_pools/prey_pool_v1"
      saved_models_base: "dryrun_outputs/saved_models"
    
    prod:
      predator_pool: "outputs/fixed_pools/predator_pool_v1"
      prey_pool: "outputs/fixed_pools/prey_pool_v1"
      saved_models_base: "outputs/saved_models"
  
  # 评估参数
  n_eval_episodes: 20            # 每个组合评估的episode数
  deterministic: true            # 使用确定性策略
  
  # 环境配置
  env_config: "waterworld_fast"  # 使用快速环境加速评估
  
  # 要评估的算法
  algorithms:
    - "PPO"
    - "A2C"
    - "SAC"
    - "TD3"
    - "RANDOM"
  
  # 输出配置
  output_dir: "evaluation_results"
  save_raw_results: true
  save_processed_results: true
  
  # 指标配置
  metrics:
    predator:
      - "catch_rate"
      - "avg_reward"
      - "energy_efficiency"
      - "first_catch_time"
      - "avg_episode_length"
    
    prey:
      - "survival_rate"
      - "avg_reward"
      - "escape_success"
      - "avg_lifespan"
      - "avg_episode_length"
    
    matchup:
      - "reward_gap"
      - "balance_score"
      - "episode_length"

# ============================================================================
# 可视化配置
# ============================================================================
visualization:
  # 热力图
  heatmap:
    figsize: [10, 8]
    cmap: "RdYlGn"
    annot: true
    fmt: ".3f"
    
  # 泛化曲线
  generalization_curve:
    figsize: [10, 6]
    show_std: true
    
  # 雷达图
  radar_chart:
    figsize: [8, 8]
    metrics:
      - "adaptability_score"
      - "avg_performance"
      - "ood_performance"
      - "stability"

# ============================================================================
# 分析配置
# ============================================================================
analysis:
  # 自适应性计算
  adaptability:
    method: "performance_ratio"  # performance_ratio / performance_drop
    include_random: false         # 是否包含RANDOM在OOD计算中
  
  # 统计检验
  statistical_tests:
    method: "wilcoxon"           # wilcoxon / ttest / mannwhitneyu
    significance_level: 0.05
  
  # 策略距离估算（用于泛化曲线）
  policy_distance:
    # 预设的算法间距离（基于架构相似度）
    distances:
      PPO:
        PPO: 0.0
        A2C: 0.3
        SAC: 0.8
        TD3: 0.6
        RANDOM: 1.0
      A2C:
        PPO: 0.3
        A2C: 0.0
        SAC: 0.7
        TD3: 0.5
        RANDOM: 1.0
      SAC:
        PPO: 0.8
        A2C: 0.7
        SAC: 0.0
        TD3: 0.4
        RANDOM: 1.0
      TD3:
        PPO: 0.6
        A2C: 0.5
        SAC: 0.4
        TD3: 0.0
        RANDOM: 1.0
      RANDOM:
        PPO: 1.0
        A2C: 1.0
        SAC: 1.0
        TD3: 1.0
        RANDOM: 0.5

