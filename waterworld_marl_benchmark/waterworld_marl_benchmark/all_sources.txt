================================================================================
FILE: dump_sources.py
================================================================================
import os
import sys

# æ ¹ç›®å½•ä¸è¾“å‡ºæ–‡ä»¶
root = sys.argv[1] if len(sys.argv) > 1 else "."
outfile = sys.argv[2] if len(sys.argv) > 2 else "all_sources.txt"

# è¦åŒ…å«çš„æ‰©å±•å
exts = {".py", ".yaml", ".yml"}
# è¦æ’é™¤çš„ç›®å½•
exclude_dirs = {".git", ".venv", "venv", "__pycache__", ".ipynb_checkpoints"}

# æ‰“å¼€è¾“å‡ºæ–‡ä»¶
with open(outfile, "w", encoding="utf-8") as out:
    for dirpath, dirnames, filenames in os.walk(root):
        # è¿‡æ»¤ä¸å¸Œæœ›éå†çš„ç›®å½•
        dirnames[:] = [d for d in dirnames if d not in exclude_dirs]

        rel_dir = os.path.relpath(dirpath, root)

        for fn in sorted(filenames):
            # è·å–æ‰©å±•åå¹¶æ£€æŸ¥æ˜¯å¦åœ¨åŒ…å«åˆ—è¡¨ä¸­
            ext = os.path.splitext(fn)[1].lower()
            if ext not in exts:
                continue

            # æ’é™¤ä»¥ fix å¼€å¤´çš„ Python æ–‡ä»¶
            if ext == ".py" and fn.lower().startswith("fix"):
                continue

            path = os.path.join(dirpath, fn)
            rel = os.path.normpath(os.path.join(rel_dir, fn)) if rel_dir != '.' else fn

            out.write("=" * 80 + "\n")
            out.write(f"FILE: {rel}\n")
            out.write("=" * 80 + "\n")

            try:
                with open(path, "r", encoding="utf-8") as f:
                    out.write(f.read())
            except UnicodeDecodeError:
                # é™çº§ä¸ºå®¹é”™è¯»å–
                with open(path, "r", encoding="utf-8", errors="replace") as f:
                    out.write(f.read())
            except Exception as e:
                out.write(f"\n[âš ï¸ ERROR reading file: {e}]\n")

            out.write("\n\n")

print(f"âœ… Done -> {outfile}")


================================================================================
FILE: test_config_system.py
================================================================================
"""
æµ‹è¯•é…ç½®ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

from src.utils.config_loader import (
    get_mode_config,
    get_env_config,
    get_algo_config,
    get_training_config
)
from src.utils.path_manager import PathManager
from src.utils.naming import FileNaming
from src.utils.banner import print_mode_banner

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("="*70)
    print("æµ‹è¯•é…ç½®åŠ è½½")
    print("="*70)
    
    # æµ‹è¯•è¿è¡Œæ¨¡å¼é…ç½®
    print("\n1. åŠ è½½debugæ¨¡å¼é…ç½®...")
    debug_config = get_mode_config("debug")
    print(f"   âœ“ Debugè®­ç»ƒæ­¥æ•°: {debug_config['total_timesteps']}")
    
    # æµ‹è¯•ç¯å¢ƒé…ç½®
    print("\n2. åŠ è½½ç¯å¢ƒé…ç½®...")
    env_config = get_env_config("waterworld_standard")
    print(f"   âœ“ Predatoræ•°é‡: {env_config['environment']['n_predators']}")
    
    # æµ‹è¯•ç®—æ³•é…ç½®
    print("\n3. åŠ è½½ç®—æ³•é…ç½®...")
    ppo_config = get_algo_config("PPO")
    print(f"   âœ“ PPOå­¦ä¹ ç‡: {ppo_config['hyperparameters']['learning_rate']}")
    
    # æµ‹è¯•è®­ç»ƒé…ç½®
    print("\n4. åŠ è½½è®­ç»ƒé…ç½®...")
    stage_config = get_training_config("stage1_1_prey_warmup")
    print(f"   âœ“ Stageåç§°: {stage_config['stage']['name']}")
    
    print("\nâœ… é…ç½®åŠ è½½æµ‹è¯•é€šè¿‡ï¼\n")


def test_path_management():
    """æµ‹è¯•è·¯å¾„ç®¡ç†"""
    print("="*70)
    print("æµ‹è¯•è·¯å¾„ç®¡ç†")
    print("="*70)
    
    for mode in ["debug", "dryrun", "prod"]:
        print(f"\n{mode.upper()} æ¨¡å¼:")
        pm = PathManager(mode, "test_experiment")
        print(f"  æ¨¡å‹ç›®å½•: {pm.get_model_dir()}")
        print(f"  æ—¥å¿—ç›®å½•: {pm.get_tensorboard_dir()}")
    
    print("\nâœ… è·¯å¾„ç®¡ç†æµ‹è¯•é€šè¿‡ï¼\n")


def test_naming():
    """æµ‹è¯•æ–‡ä»¶å‘½å"""
    print("="*70)
    print("æµ‹è¯•æ–‡ä»¶å‘½å")
    print("="*70)
    
    naming = FileNaming()
    
    # æµ‹è¯•æ¨¡å‹æ–‡ä»¶åç”Ÿæˆ
    filename = naming.generate_model_filename(
        train_algo="PPO",
        train_side="prey",
        version="v1",
        opponent_info="RANDOM_pred",
        run_mode="debug"
    )
    print(f"\nDebugæ¨¡å¼æ–‡ä»¶å: {filename}")
    
    filename = naming.generate_model_filename(
        train_algo="SAC",
        train_side="predator",
        version="v2",
        opponent_info="MIX_prey_pool_v1",
        run_mode="prod"
    )
    print(f"Prodæ¨¡å¼æ–‡ä»¶å: {filename}")
    
    print("\nâœ… æ–‡ä»¶å‘½åæµ‹è¯•é€šè¿‡ï¼\n")


def test_banner():
    """æµ‹è¯•æ¨ªå¹…æ˜¾ç¤º"""
    print("="*70)
    print("æµ‹è¯•æ¨ªå¹…æ˜¾ç¤º")
    print("="*70)
    
    for mode in ["debug", "dryrun"]:  # prodéœ€è¦ç¡®è®¤ï¼Œè·³è¿‡
        config = get_mode_config(mode)
        print_mode_banner(mode, config)
    
    print("âœ… æ¨ªå¹…æ˜¾ç¤ºæµ‹è¯•é€šè¿‡ï¼\n")


if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ§ª é…ç½®ç³»ç»Ÿæµ‹è¯•")
    print("="*70 + "\n")
    
    try:
        test_config_loading()
        test_path_management()
        test_naming()
        test_banner()
        
        print("="*70)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é…ç½®ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
        print("="*70 + "\n")
    
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()

================================================================================
FILE: test_training_system.py
================================================================================
"""
è®­ç»ƒç³»ç»Ÿç»¼åˆæµ‹è¯•
éªŒè¯æ‰€æœ‰ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def test_imports():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å—å¯¼å…¥"""
    print("\n" + "="*70)
    print("æµ‹è¯•æ¨¡å—å¯¼å…¥")
    print("="*70)
    
    try:
        # æ ¸å¿ƒæ¨¡å—
        from src.core import (
            WaterworldEnvManager,
            OpponentPool,
            MultiAgentTrainer,
            AgentManager
        )
        print("  âœ“ æ ¸å¿ƒæ¨¡å—")
        
        # ç®—æ³•æ¨¡å—
        from src.algorithms import (
            create_algorithm,
            PPOWrapper,
            A2CWrapper,
            SACWrapper,
            TD3Wrapper,
            RandomPolicy
        )
        print("  âœ“ ç®—æ³•æ¨¡å—")
        
        # å›è°ƒæ¨¡å—
        from src.callbacks import (
            MultiAgentTensorBoardCallback,
            CheckpointCallback,
            EvalCallback,
            FreezeCallback,
            ProgressBarCallback
        )
        print("  âœ“ å›è°ƒæ¨¡å—")
        
        # å·¥å…·æ¨¡å—
        from src.utils.config_loader import get_mode_config, get_env_config, get_algo_config
        from src.utils.path_manager import PathManager
        from src.utils.naming import FileNaming
        from src.utils.logger import create_logger
        from src.utils.config_validator import validator
        print("  âœ“ å·¥å…·æ¨¡å—")
        
        print("\nâœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸï¼\n")
        return True
    
    except Exception as e:
        print(f"\nâŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_config_system():
    """æµ‹è¯•é…ç½®ç³»ç»Ÿ"""
    print("="*70)
    print("æµ‹è¯•é…ç½®ç³»ç»Ÿ")
    print("="*70)
    
    try:
        from src.utils.config_loader import (
            get_mode_config,
            get_env_config,
            get_algo_config,
            get_training_config
        )
        
        # æµ‹è¯•è¿è¡Œæ¨¡å¼é…ç½®
        debug_config = get_mode_config("debug")
        assert 'total_timesteps' in debug_config
        print("  âœ“ è¿è¡Œæ¨¡å¼é…ç½®")
        
        # æµ‹è¯•ç¯å¢ƒé…ç½®
        env_config = get_env_config("waterworld_standard")
        assert 'environment' in env_config
        print("  âœ“ ç¯å¢ƒé…ç½®")
        
        # æµ‹è¯•ç®—æ³•é…ç½®
        ppo_config = get_algo_config("PPO")
        assert 'hyperparameters' in ppo_config
        print("  âœ“ ç®—æ³•é…ç½®")
        
        # æµ‹è¯•è®­ç»ƒé…ç½®
        stage_config = get_training_config("stage1_1_prey_warmup")
        assert 'stage' in stage_config
        print("  âœ“ è®­ç»ƒé˜¶æ®µé…ç½®")
        
        print("\nâœ… é…ç½®ç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼\n")
        return True
    
    except Exception as e:
        print(f"\nâŒ é…ç½®ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_environment_creation():
    """æµ‹è¯•ç¯å¢ƒåˆ›å»º"""
    print("="*70)
    print("æµ‹è¯•ç¯å¢ƒåˆ›å»º")
    print("="*70)
    
    try:
        from src.core import WaterworldEnvManager
        from src.utils.config_loader import get_env_config
        
        # åˆ›å»ºç¯å¢ƒç®¡ç†å™¨
        env_config = get_env_config("waterworld_fast")  # ä½¿ç”¨å¿«é€Ÿç¯å¢ƒ
        env_manager = WaterworldEnvManager(env_config)
        print("  âœ“ ç¯å¢ƒç®¡ç†å™¨åˆ›å»º")
        
        # åˆ›å»ºç¯å¢ƒ
        env = env_manager.create_env()
        print("  âœ“ ç¯å¢ƒåˆ›å»º")
        
        # æµ‹è¯•é‡ç½®
        obs, infos = env.reset()
        print(f"  âœ“ ç¯å¢ƒé‡ç½® (agents: {len(env.agents)})")
        
        # æµ‹è¯•å•æ­¥
        actions = {agent: env.action_space(agent).sample() for agent in env.agents}
        obs, rewards, terminations, truncations, infos = env.step(actions)
        print("  âœ“ ç¯å¢ƒå•æ­¥")
        
        # æ¸…ç†
        env_manager.close()
        print("  âœ“ ç¯å¢ƒæ¸…ç†")
        
        print("\nâœ… ç¯å¢ƒåˆ›å»ºæµ‹è¯•é€šè¿‡ï¼\n")
        return True
    
    except Exception as e:
        print(f"\nâŒ ç¯å¢ƒåˆ›å»ºæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_algorithm_creation():
    """æµ‹è¯•ç®—æ³•åˆ›å»º"""
    print("="*70)
    print("æµ‹è¯•ç®—æ³•åˆ›å»º")
    print("="*70)
    
    try:
        from src.algorithms import create_algorithm
        from src.core import WaterworldEnvManager
        from src.utils.config_loader import get_env_config, get_algo_config
        import gymnasium as gym
        
        # åˆ›å»ºç¯å¢ƒè·å–ç©ºé—´ä¿¡æ¯
        env_config = get_env_config("waterworld_fast")
        env_manager = WaterworldEnvManager(env_config)
        env_manager.create_env()
        
        obs_space = env_manager.get_observation_space("predator")
        action_space = env_manager.get_action_space("predator")
        
        # æµ‹è¯•æ¯ä¸ªç®—æ³•
        for algo_name in ['PPO', 'A2C', 'SAC', 'TD3', 'RANDOM']:
            algo_config = get_algo_config(algo_name)
            algorithm = create_algorithm(
                algo_name=algo_name,
                observation_space=obs_space,
                action_space=action_space,
                config=algo_config,
                device='cpu'
            )
            print(f"  âœ“ {algo_name} ç®—æ³•åˆ›å»º")
        
        env_manager.close()
        
        print("\nâœ… ç®—æ³•åˆ›å»ºæµ‹è¯•é€šè¿‡ï¼\n")
        return True
    
    except Exception as e:
        print(f"\nâŒ ç®—æ³•åˆ›å»ºæµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_trainer_initialization():
    """æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–"""
    print("="*70)
    print("æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–")
    print("="*70)
    
    try:
        from src.core import MultiAgentTrainer
        
        # åˆ›å»ºæœ€å°é…ç½®çš„è®­ç»ƒå™¨
        trainer = MultiAgentTrainer(
            train_side='prey',
            train_algo='PPO',
            opponent_config={
                'type': 'algorithm',
                'side': 'predator',
                'algorithm': 'RANDOM',
                'freeze': True
            },
            experiment_name='test_experiment',
            stage_name='test_stage',
            generation=0,
            version='v1',
            run_mode='debug',
            total_timesteps=100  # æå°‘æ­¥æ•°
        )
        print("  âœ“ è®­ç»ƒå™¨åˆå§‹åŒ–")
        
        # æµ‹è¯•setupï¼ˆä¸å®é™…è¿è¡Œè®­ç»ƒï¼‰
        trainer.setup()
        print("  âœ“ è®­ç»ƒå™¨è®¾ç½®")
        
        # æ¸…ç†
        trainer.cleanup()
        print("  âœ“ è®­ç»ƒå™¨æ¸…ç†")
        
        print("\nâœ… è®­ç»ƒå™¨åˆå§‹åŒ–æµ‹è¯•é€šè¿‡ï¼\n")
        return True
    
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå™¨åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def test_mini_training():
    """æµ‹è¯•å®Œæ•´çš„è¿·ä½ è®­ç»ƒæµç¨‹"""
    print("="*70)
    print("æµ‹è¯•è¿·ä½ è®­ç»ƒæµç¨‹")
    print("="*70)
    print("è¿™å°†è¿è¡Œä¸€ä¸ªè¶…çŸ­çš„è®­ç»ƒï¼ˆ200æ­¥ï¼‰æ¥éªŒè¯å®Œæ•´æµç¨‹")
    print("-"*70)
    
    try:
        from src.core import MultiAgentTrainer
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MultiAgentTrainer(
            train_side='prey',
            train_algo='PPO',
            opponent_config={
                'type': 'algorithm',
                'side': 'predator',
                'algorithm': 'RANDOM',
                'freeze': True
            },
            experiment_name='mini_test',
            stage_name='test_mini_training',
            generation=0,
            version='v1',
            run_mode='debug',
            total_timesteps=200,  # åªè®­ç»ƒ200æ­¥
            n_envs=1,
            eval_freq=-1,  # ç¦ç”¨è¯„ä¼°
            checkpoint_freq=-1  # ç¦ç”¨æ£€æŸ¥ç‚¹
        )
        
        print("\nå¼€å§‹è¿·ä½ è®­ç»ƒ...")
        
        # è®¾ç½®
        trainer.setup()
        
        # è®­ç»ƒ
        trainer.train()
        
        # è¯„ä¼°
        eval_results = trainer.evaluate(n_episodes=2)
        
        # ä¿å­˜ï¼ˆä½†ä¸åŠ å…¥æ± ï¼‰
        trainer.save_model(save_to_pool=False)
        
        # ä¿å­˜æ‘˜è¦
        trainer.save_training_summary()
        
        # æ¸…ç†
        trainer.cleanup()
        
        print("\nâœ… è¿·ä½ è®­ç»ƒæµç¨‹æµ‹è¯•é€šè¿‡ï¼")
        mean_reward = eval_results.get('mean_reward', None)
        if mean_reward is not None:
            print(f"   å¹³å‡å¥–åŠ±: {mean_reward:.2f}")
        else:
            print(f"   å¹³å‡å¥–åŠ±: N/A")
        print()
        return True
    
    except Exception as e:
        print(f"\nâŒ è¿·ä½ è®­ç»ƒæµç¨‹æµ‹è¯•å¤±è´¥: {e}\n")
        import traceback
        traceback.print_exc()
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*70)
    print("ğŸ§ª è®­ç»ƒç³»ç»Ÿç»¼åˆæµ‹è¯•")
    print("="*70 + "\n")
    
    tests = [
        ("æ¨¡å—å¯¼å…¥", test_imports),
        ("é…ç½®ç³»ç»Ÿ", test_config_system),
        ("ç¯å¢ƒåˆ›å»º", test_environment_creation),
        ("ç®—æ³•åˆ›å»º", test_algorithm_creation),
        ("è®­ç»ƒå™¨åˆå§‹åŒ–", test_trainer_initialization),
        ("è¿·ä½ è®­ç»ƒæµç¨‹", test_mini_training),
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å´©æºƒ: {e}")
            results[test_name] = False
    
    # æ‰“å°æ±‡æ€»
    print("="*70)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("="*70)
    
    for test_name, passed in results.items():
        status = "âœ… é€šè¿‡" if passed else "âŒ å¤±è´¥"
        print(f"{test_name:20s}: {status}")
    
    print("="*70)
    
    # ç»Ÿè®¡
    total = len(results)
    passed = sum(results.values())
    failed = total - passed
    
    print(f"\næ€»è®¡: {total} ä¸ªæµ‹è¯•")
    print(f"é€šè¿‡: {passed} ä¸ª")
    print(f"å¤±è´¥: {failed} ä¸ª")
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªï¼")
        return 0
    else:
        print(f"\nâš ï¸  {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)

================================================================================
FILE: debug_outputs/current/experiments/stage1.1_prey_warmup/TD3_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 3000
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_warmup
  generation: 0
  mode_config:
    archive_on_exit: true
    checkpoint_freq: -1
    clear_on_start: true
    deterministic_eval: true
    eval_freq: -1
    filename_prefix: DEBUG_
    log_level: DEBUG
    max_archives: 5
    n_envs: 1
    n_eval_episodes: 3
    output_base_dir: debug_outputs/current
    save_checkpoints: false
    save_final_model: false
    seed: 42
    tensorboard_enabled: false
    total_timesteps: 1000
    verbose: 2
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: debug
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: -1
    deterministic_eval: true
    eval_freq: -1
    n_envs: 1
    n_eval_episodes: 3
    save_checkpoints: false
    save_final_model: false
    show_progress: true
    tensorboard_enabled: false
    total_timesteps: 1000
    verbose: 2
  version: v1
timestamp: '2025-10-13T20:02:21.558108'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/A2C_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T21:03:13.318957'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/PPO_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:56:07.328380'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/RANDOM_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: custom
      name: RANDOM
    device: cpu
    hyperparameters: {}
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: RANDOM_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: RANDOM
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 50000
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:55:12.697257'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/SAC_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T21:06:20.197986'


================================================================================
FILE: outputs/experiments/stage1.2_pred_guided/TD3_predator_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_guided
  generation: 1
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.2_pred_guided
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T21:09:25.024869'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/A2C_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:04:44.754133'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/PPO_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T19:57:46.901723'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/RANDOM_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: custom
      name: RANDOM
    device: cpu
    hyperparameters: {}
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: RANDOM_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: RANDOM
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 5000
    verbose: 1
  version: v1
timestamp: '2025-10-13T19:57:46.809251'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/SAC_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:07:42.682084'


================================================================================
FILE: outputs/experiments/stage1.1_prey_warmup/TD3_prey_v1_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_warmup
  generation: 0
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: test
  seed: null
  stage_name: stage1.1_prey_warmup
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v1
timestamp: '2025-10-13T20:10:39.595390'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_19/A2C_prey_v19_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 19
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_19
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v19
timestamp: '2025-10-14T02:06:13.857992'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_19/PPO_prey_v19_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 19
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_19
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v19
timestamp: '2025-10-14T01:59:02.695531'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_19/SAC_prey_v19_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 19
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_19
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v19
timestamp: '2025-10-14T02:09:24.658605'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_19/TD3_prey_v19_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 19
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_19
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v19
timestamp: '2025-10-14T02:12:32.517904'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_8/A2C_predator_v8_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 8
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_8
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v8
timestamp: '2025-10-13T23:05:26.912540'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_8/PPO_predator_v8_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 8
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_8
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v8
timestamp: '2025-10-13T22:58:14.320165'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_8/SAC_predator_v8_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 8
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_8
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v8
timestamp: '2025-10-13T23:08:33.026354'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_8/TD3_predator_v8_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 8
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_8
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v8
timestamp: '2025-10-13T23:11:40.087757'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_3/A2C_prey_v3_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 3
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_3
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v3
timestamp: '2025-10-13T23:39:06.933738'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_3/PPO_prey_v3_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 3
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_3
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v3
timestamp: '2025-10-13T23:32:52.999775'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_3/SAC_prey_v3_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 3
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_3
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v3
timestamp: '2025-10-13T23:41:20.059762'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_3/TD3_prey_v3_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 3
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_3
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v3
timestamp: '2025-10-13T23:43:30.516345'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_15/A2C_prey_v15_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 15
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_15
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v15
timestamp: '2025-10-14T01:00:05.910424'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_15/PPO_prey_v15_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 15
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_15
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v15
timestamp: '2025-10-14T00:53:07.560176'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_15/SAC_prey_v15_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 15
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_15
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v15
timestamp: '2025-10-14T01:03:08.157110'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_15/TD3_prey_v15_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 15
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_15
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v15
timestamp: '2025-10-14T01:06:09.105872'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_12/A2C_predator_v12_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 12
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_12
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v12
timestamp: '2025-10-14T00:11:07.229136'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_12/PPO_predator_v12_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 12
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_12
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v12
timestamp: '2025-10-14T00:03:57.217370'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_12/SAC_predator_v12_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 12
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_12
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v12
timestamp: '2025-10-14T00:14:15.353443'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_12/TD3_predator_v12_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 12
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_12
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v12
timestamp: '2025-10-14T00:17:22.168337'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_2/A2C_predator_v2_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 2
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_2
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v2
timestamp: '2025-10-13T23:26:06.638104'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_2/PPO_predator_v2_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 2
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_2
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v2
timestamp: '2025-10-13T23:19:38.971214'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_2/SAC_predator_v2_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 2
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_2
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v2
timestamp: '2025-10-13T23:28:22.144806'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_2/TD3_predator_v2_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 2
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_2
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v2
timestamp: '2025-10-13T23:30:36.954869'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_5/A2C_prey_v5_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 5
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_5
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v5
timestamp: '2025-10-13T22:16:39.151667'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_5/PPO_prey_v5_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 5
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_5
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v5
timestamp: '2025-10-13T22:09:40.267268'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_5/SAC_prey_v5_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 5
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_5
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v5
timestamp: '2025-10-13T22:19:41.237049'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_5/TD3_prey_v5_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 5
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_5
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v5
timestamp: '2025-10-13T22:22:41.391525'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_10/A2C_predator_v10_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 10
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_10
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v10
timestamp: '2025-10-13T23:38:24.480207'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_10/PPO_predator_v10_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 10
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_10
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v10
timestamp: '2025-10-13T23:31:06.817019'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_10/SAC_predator_v10_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 10
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_10
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v10
timestamp: '2025-10-13T23:41:36.746970'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_10/TD3_predator_v10_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 10
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_10
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v10
timestamp: '2025-10-13T23:44:48.174923'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_9/A2C_prey_v9_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 9
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_9
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v9
timestamp: '2025-10-13T23:21:52.143509'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_9/PPO_prey_v9_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 9
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_9
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v9
timestamp: '2025-10-13T23:14:48.671874'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_9/SAC_prey_v9_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 9
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_9
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v9
timestamp: '2025-10-13T23:24:58.235728'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_9/TD3_prey_v9_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 9
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_9
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v9
timestamp: '2025-10-13T23:28:02.358081'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_7/A2C_prey_v7_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 7
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_7
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v7
timestamp: '2025-10-13T22:49:14.065508'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_7/PPO_prey_v7_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 7
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_7
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v7
timestamp: '2025-10-13T22:42:14.893699'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_7/SAC_prey_v7_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 7
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_7
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v7
timestamp: '2025-10-13T22:52:15.172643'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_7/TD3_prey_v7_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 7
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_7
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v7
timestamp: '2025-10-13T22:55:14.824515'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_16/A2C_predator_v16_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 16
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_16
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v16
timestamp: '2025-10-14T01:16:22.049686'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_16/PPO_predator_v16_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 16
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_16
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v16
timestamp: '2025-10-14T01:09:07.933363'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_16/SAC_predator_v16_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 16
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_16
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v16
timestamp: '2025-10-14T01:19:30.213901'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_16/TD3_predator_v16_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 16
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_16
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v16
timestamp: '2025-10-14T01:22:40.194781'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_6/A2C_predator_v6_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 6
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_6
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v6
timestamp: '2025-10-13T22:32:52.335549'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_6/PPO_predator_v6_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 6
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_6
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v6
timestamp: '2025-10-13T22:25:41.106256'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_6/SAC_predator_v6_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 6
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_6
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v6
timestamp: '2025-10-13T22:36:00.766592'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_6/TD3_predator_v6_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 6
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_6
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v6
timestamp: '2025-10-13T22:39:07.606228'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_20/PPO_predator_v20_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 20
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_20
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v20
timestamp: '2025-10-14T02:15:35.795579'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_17/A2C_prey_v17_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 17
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_17
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v17
timestamp: '2025-10-14T01:32:58.612310'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_17/PPO_prey_v17_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 17
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_17
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v17
timestamp: '2025-10-14T01:25:50.507483'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_17/SAC_prey_v17_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 17
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_17
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v17
timestamp: '2025-10-14T01:36:04.585738'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_17/TD3_prey_v17_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 17
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_17
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v17
timestamp: '2025-10-14T01:39:08.566699'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_4/A2C_predator_v4_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 4
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_4
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v4
timestamp: '2025-10-13T22:00:18.008891'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_4/PPO_predator_v4_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 4
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_4
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v4
timestamp: '2025-10-13T21:53:03.283044'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_4/SAC_predator_v4_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 4
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_4
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v4
timestamp: '2025-10-13T22:03:27.291306'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_4/TD3_predator_v4_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 4
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_4
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v4
timestamp: '2025-10-13T22:06:33.007575'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_13/A2C_prey_v13_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 13
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_13
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v13
timestamp: '2025-10-14T00:27:29.381684'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_13/PPO_prey_v13_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 13
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_13
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v13
timestamp: '2025-10-14T00:20:30.603820'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_13/SAC_prey_v13_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 13
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_13
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v13
timestamp: '2025-10-14T00:30:30.236340'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_13/TD3_prey_v13_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 13
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_13
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v13
timestamp: '2025-10-14T00:33:31.105186'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_14/A2C_predator_v14_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 14
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_14
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v14
timestamp: '2025-10-14T00:43:46.730877'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_14/PPO_predator_v14_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 14
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_14
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v14
timestamp: '2025-10-14T00:36:31.295427'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_14/SAC_predator_v14_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 14
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_14
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v14
timestamp: '2025-10-14T00:46:55.304911'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_14/TD3_predator_v14_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 14
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_14
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v14
timestamp: '2025-10-14T00:50:02.121000'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_11/A2C_prey_v11_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_prey_coevo
  generation: 11
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_11
  train_algo: A2C
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v11
timestamp: '2025-10-13T23:54:55.981495'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_11/PPO_prey_v11_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_prey_coevo
  generation: 11
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_11
  train_algo: PPO
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v11
timestamp: '2025-10-13T23:47:56.922850'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_11/SAC_prey_v11_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_prey_coevo
  generation: 11
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_11
  train_algo: SAC
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v11
timestamp: '2025-10-13T23:57:57.837789'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_11/TD3_prey_v11_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_prey_coevo
  generation: 11
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/predator_pool_v1
    side: predator
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_11
  train_algo: TD3
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v11
timestamp: '2025-10-14T00:00:57.340484'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_18/A2C_predator_v18_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.A2C
      name: A2C
    device: auto
    hyperparameters:
      ent_coef: 0.01
      gae_lambda: 1.0
      gamma: 0.99
      learning_rate: 0.0007
      max_grad_norm: 0.5
      n_steps: 5
      normalize_advantage: false
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      rms_prop_eps: 1.0e-05
      vf_coef: 0.25
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: A2C_predator_coevo
  generation: 18
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_18
  train_algo: A2C
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v18
timestamp: '2025-10-14T01:49:31.567227'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_18/PPO_predator_v18_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.PPO
      name: PPO
    device: auto
    hyperparameters:
      batch_size: 64
      clip_range: 0.2
      clip_range_vf: null
      ent_coef: 0.01
      gae_lambda: 0.95
      gamma: 0.99
      learning_rate: 0.0003
      max_grad_norm: 0.5
      n_epochs: 10
      n_steps: 2048
      normalize_advantage: true
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          vf:
          - 256
          - 256
      target_kl: null
      vf_coef: 0.5
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: PPO_predator_coevo
  generation: 18
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_18
  train_algo: PPO
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v18
timestamp: '2025-10-14T01:42:11.082890'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_18/SAC_predator_v18_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.SAC
      name: SAC
    device: auto
    hyperparameters:
      batch_size: 256
      buffer_size: 1000000
      ent_coef: auto
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      sde_sample_freq: -1
      target_entropy: auto
      tau: 0.005
      train_freq: 1
      use_sde: false
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: SAC_predator_coevo
  generation: 18
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_18
  train_algo: SAC
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v18
timestamp: '2025-10-14T01:52:41.989882'


================================================================================
FILE: outputs/experiments/stage1.3_coevolution/Gen_18/TD3_predator_v18_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: stable_baselines3.TD3
      name: TD3
    device: auto
    hyperparameters:
      action_noise: normal
      action_noise_kwargs:
        mean: 0.0
        sigma: 0.1
      batch_size: 256
      buffer_size: 1000000
      gamma: 0.99
      gradient_steps: 1
      learning_rate: 0.0003
      learning_starts: 10000
      policy: MlpPolicy
      policy_delay: 2
      policy_kwargs:
        activation_fn: torch.nn.ReLU
        net_arch:
          pi:
          - 256
          - 256
          qf:
          - 256
          - 256
      target_noise_clip: 0.5
      target_policy_noise: 0.2
      tau: 0.005
      train_freq: 1
    seed: null
  device: auto
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 500
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: TD3_predator_coevo
  generation: 18
  mode_config:
    auto_cleanup: false
    checkpoint_freq: 500
    clear_on_start: true
    deterministic_eval: true
    eval_freq: 500
    filename_prefix: TEST_
    log_level: INFO
    n_envs: 4
    n_eval_episodes: 2
    output_base_dir: test_outputs
    require_confirmation: false
    save_checkpoints: true
    save_final_model: true
    seed: 42
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  notes: ''
  opponent_config:
    freeze: true
    mix_strategy:
      fixed_ratio: 0.7
      sampling: uniform
    pool_path: outputs/fixed_pools/prey_pool_v1
    side: prey
    type: mixed_pool
  run_mode: test
  seed: null
  stage_name: stage1.3_coevolution/Gen_18
  train_algo: TD3
  train_side: predator
  training_config:
    check_freeze: false
    checkpoint_freq: 500
    deterministic_eval: true
    eval_freq: 500
    n_envs: 4
    n_eval_episodes: 2
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 500
    verbose: 1
  version: v18
timestamp: '2025-10-14T01:55:52.542682'


================================================================================
FILE: scripts/analysis/analyze_results.py
================================================================================


================================================================================
FILE: scripts/analysis/plot_results.py
================================================================================
"""
å¯è§†åŒ–åˆ†æç»“æœ
ç”Ÿæˆçƒ­åŠ›å›¾ã€æ³›åŒ–æ›²çº¿ã€é›·è¾¾å›¾ç­‰
"""

import sys
import argparse
import json
import pickle
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.utils.config_loader import config_loader


def parse_args():
    parser = argparse.ArgumentParser(description='å¯è§†åŒ–è¯„ä¼°ç»“æœ')
    parser.add_argument('--input', type=str, required=True,
                        help='äº¤å‰è¯„ä¼°ç»“æœç›®å½•')
    parser.add_argument('--output', type=str, default=None,
                        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºinputç›®å½•ï¼‰')
    return parser.parse_args()


def load_results(results_dir: Path):
    """åŠ è½½è¯„ä¼°ç»“æœ"""
    # åŠ è½½åŸå§‹ç»“æœ
    raw_path = results_dir / "raw_results.pkl"
    if raw_path.exists():
        with open(raw_path, 'rb') as f:
            results_matrix = pickle.load(f)
    else:
        print(f"âŒ æœªæ‰¾åˆ°åŸå§‹ç»“æœ: {raw_path}")
        return None
    
    # åŠ è½½è‡ªé€‚åº”æ€§å¾—åˆ†
    adapt_path = results_dir / "adaptability_scores.json"
    if adapt_path.exists():
        with open(adapt_path, 'r') as f:
            adaptability_data = json.load(f)
    else:
        adaptability_data = None
    
    return results_matrix, adaptability_data


def plot_heatmap(results_matrix, output_path, metric='catch_rate'):
    """
    ç»˜åˆ¶çƒ­åŠ›å›¾
    
    Args:
        results_matrix: ç»“æœçŸ©é˜µ
        output_path: è¾“å‡ºè·¯å¾„
        metric: è¦æ˜¾ç¤ºçš„æŒ‡æ ‡
    """
    # æå–ç®—æ³•åˆ—è¡¨
    algos = list(results_matrix.keys())
    
    # æ„å»ºçŸ©é˜µ
    matrix = np.zeros((len(algos), len(algos)))
    
    for i, pred_algo in enumerate(algos):
        for j, prey_algo in enumerate(algos):
            metrics = results_matrix[pred_algo].get(prey_algo)
            if metrics:
                matrix[i, j] = metrics.get(metric, 0)
            else:
                matrix[i, j] = np.nan
    
    # ç»˜åˆ¶çƒ­åŠ›å›¾
    plt.figure(figsize=(10, 8))
    
    # åˆ›å»ºmaskæ¥æ ‡è®°å¯¹è§’çº¿
    mask = np.zeros_like(matrix, dtype=bool)
    
    sns.heatmap(
        matrix,
        annot=True,
        fmt='.3f',
        cmap='RdYlGn',
        xticklabels=algos,
        yticklabels=algos,
        vmin=0, vmax=1,
        cbar_kws={'label': 'Predator Catch Rate'},
        linewidths=0.5,
        linecolor='gray',
        mask=mask
    )
    
    # æ ‡è®°å¯¹è§’çº¿ï¼ˆIn-Distributionï¼‰
    for i in range(len(algos)):
        plt.gca().add_patch(plt.Rectangle((i, i), 1, 1,
                            fill=False, edgecolor='blue', lw=3))
    
    plt.xlabel('Prey Algorithm', fontsize=12, fontweight='bold')
    plt.ylabel('Predator Algorithm', fontsize=12, fontweight='bold')
    plt.title(f'Cross-Algorithm Performance Matrix\n'
              f'Metric: {metric.replace("_", " ").title()}\n'
              f'Blue boxes = In-Distribution (trained together)',
              fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  âœ“ çƒ­åŠ›å›¾: {output_path}")


def plot_generalization_curves(results_matrix, adaptability_data, output_path):
    """ç»˜åˆ¶æ³›åŒ–æ›²çº¿"""
    
    # åŠ è½½ç­–ç•¥è·ç¦»é…ç½®
    eval_config = config_loader.load_yaml('evaluation/cross_eval.yaml')
    policy_distances = eval_config['analysis']['policy_distance']['distances']
    
    plt.figure(figsize=(10, 6))
    
    algos = [a for a in results_matrix.keys() if a != 'RANDOM']
    
    for pred_algo in algos:
        # æ”¶é›†(è·ç¦», æ€§èƒ½)ç‚¹å¯¹
        points = []
        
        for prey_algo in algos:
            if prey_algo in policy_distances.get(pred_algo, {}):
                distance = policy_distances[pred_algo][prey_algo]
                
                metrics = results_matrix[pred_algo].get(prey_algo)
                if metrics:
                    performance = metrics['catch_rate']
                    points.append((distance, performance))
        
        # æŒ‰è·ç¦»æ’åº
        points.sort(key=lambda x: x[0])
        
        if points:
            distances = [p[0] for p in points]
            performances = [p[1] for p in points]
            
            plt.plot(distances, performances, marker='o', linewidth=2,
                    label=f'{pred_algo}', markersize=8)
    
    plt.xlabel('Policy Distance from Training Distribution', fontsize=12)
    plt.ylabel('Catch Rate (Performance)', fontsize=12)
    plt.title('Generalization Curves: Performance vs Opponent Distance',
              fontsize=14, fontweight='bold')
    plt.legend(loc='best')
    plt.grid(alpha=0.3)
    plt.axhline(0.5, color='gray', linestyle='--', alpha=0.5,
                label='Balance Line (50%)')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  âœ“ æ³›åŒ–æ›²çº¿: {output_path}")


def plot_ranking_bars(adaptability_data, output_path):
    """ç»˜åˆ¶æ’åæŸ±çŠ¶å›¾"""
    
    if not adaptability_data:
        print("  âš ï¸  è·³è¿‡æ’åå›¾: æ— è‡ªé€‚åº”æ€§æ•°æ®")
        return
    
    ranking = adaptability_data['ranking']
    
    algos = [r['algorithm'] for r in ranking]
    adapt_scores = [r['adaptability_score'] for r in ranking]
    in_dist = [r['in_dist_performance'] for r in ranking]
    ood_avg = [r['ood_avg_performance'] for r in ranking]
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # å­å›¾1: è‡ªé€‚åº”æ€§å¾—åˆ†
    ax1 = axes[0]
    colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(algos)))
    bars1 = ax1.barh(algos, adapt_scores, color=colors)
    ax1.set_xlabel('Adaptability Score', fontsize=12)
    ax1.set_title('Algorithm Adaptability Ranking', fontsize=13, fontweight='bold')
    ax1.set_xlim(0, 1.1)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, score in zip(bars1, adapt_scores):
        width = bar.get_width()
        ax1.text(width + 0.02, bar.get_y() + bar.get_height()/2,
                f'{score:.3f}', va='center', fontsize=10)
    
    # å­å›¾2: In-Dist vs OOD æ€§èƒ½
    ax2 = axes[1]
    x = np.arange(len(algos))
    width = 0.35
    
    bars1 = ax2.bar(x - width/2, in_dist, width, label='In-Distribution',
                    color='steelblue', alpha=0.8)
    bars2 = ax2.bar(x + width/2, ood_avg, width, label='Out-of-Distribution',
                    color='coral', alpha=0.8)
    
    ax2.set_ylabel('Catch Rate', fontsize=12)
    ax2.set_title('In-Distribution vs OOD Performance', fontsize=13, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels(algos)
    ax2.legend()
    ax2.set_ylim(0, 0.8)
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  âœ“ æ’åå›¾: {output_path}")


def plot_performance_distribution(results_matrix, output_path):
    """ç»˜åˆ¶æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾"""
    
    algos = [a for a in results_matrix.keys() if a != 'RANDOM']
    
    # æ”¶é›†æ¯ä¸ªç®—æ³•çš„OODæ€§èƒ½
    ood_performances = {algo: [] for algo in algos}
    
    for pred_algo in algos:
        for prey_algo in algos:
            if prey_algo != pred_algo and prey_algo != 'RANDOM':
                metrics = results_matrix[pred_algo].get(prey_algo)
                if metrics:
                    ood_performances[pred_algo].append(metrics['catch_rate'])
    
    # ç»˜åˆ¶ç®±çº¿å›¾
    plt.figure(figsize=(10, 6))
    
    data = [ood_performances[algo] for algo in algos]
    positions = range(1, len(algos) + 1)
    
    bp = plt.boxplot(data, positions=positions, labels=algos,
                     patch_artist=True, widths=0.6)
    
    # è®¾ç½®é¢œè‰²
    colors = plt.cm.Set3(np.linspace(0, 1, len(algos)))
    for patch, color in zip(bp['boxes'], colors):
        patch.set_facecolor(color)
        patch.set_alpha(0.7)
    
    plt.ylabel('Catch Rate (OOD Performance)', fontsize=12)
    plt.xlabel('Predator Algorithm', fontsize=12)
    plt.title('Out-of-Distribution Performance Distribution\n'
              '(Performance against unseen opponents)',
              fontsize=14, fontweight='bold')
    plt.grid(axis='y', alpha=0.3)
    plt.axhline(0.5, color='red', linestyle='--', alpha=0.5,
                label='Balance Line')
    plt.legend()
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"  âœ“ æ€§èƒ½åˆ†å¸ƒå›¾: {output_path}")


def main():
    args = parse_args()
    
    results_dir = Path(args.input)
    output_dir = Path(args.output) if args.output else results_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*70)
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–ç»“æœ")
    print("="*70)
    print(f"è¾“å…¥ç›®å½•: {results_dir}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print("="*70 + "\n")
    
    # åŠ è½½ç»“æœ
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    results_data = load_results(results_dir)
    
    if results_data is None:
        print("âŒ åŠ è½½å¤±è´¥")
        return
    
    results_matrix, adaptability_data = results_data
    print("  âœ“ æ•°æ®åŠ è½½å®Œæˆ\n")
    
    # ç”Ÿæˆå„ç§å›¾è¡¨
    print("ğŸ¨ ç”Ÿæˆå›¾è¡¨...")
    
    # 1. çƒ­åŠ›å›¾
    plot_heatmap(
        results_matrix,
        output_dir / "heatmap_catch_rate.png",
        metric='catch_rate'
    )
    
    # 2. æ³›åŒ–æ›²çº¿
    plot_generalization_curves(
        results_matrix,
        adaptability_data,
        output_dir / "generalization_curves.png"
    )
    
    # 3. æ’åæŸ±çŠ¶å›¾
    if adaptability_data:
        plot_ranking_bars(
            adaptability_data,
            output_dir / "ranking_comparison.png"
        )
    
    # 4. æ€§èƒ½åˆ†å¸ƒç®±çº¿å›¾
    plot_performance_distribution(
        results_matrix,
        output_dir / "performance_distribution.png"
    )
    
    print("\n" + "="*70)
    print("âœ… å¯è§†åŒ–å®Œæˆï¼")
    print("="*70)
    print(f"\nğŸ“ å›¾è¡¨ä¿å­˜åœ¨: {output_dir}")
    print("\nç”Ÿæˆçš„å›¾è¡¨:")
    print("  1. heatmap_catch_rate.png         - æ€§èƒ½çƒ­åŠ›å›¾")
    print("  2. generalization_curves.png      - æ³›åŒ–æ›²çº¿")
    print("  3. ranking_comparison.png         - ç®—æ³•æ’åå¯¹æ¯”")
    print("  4. performance_distribution.png   - æ€§èƒ½åˆ†å¸ƒ")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/training/train_single.py
================================================================================
"""
å•æ¬¡è®­ç»ƒè„šæœ¬ï¼ˆé€šç”¨ï¼‰
å¯ç”¨äºè®­ç»ƒä»»æ„é…ç½®çš„å•ä¸ªæ¨¡å‹
"""

import argparse
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_env_config, get_training_config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='å•æ¬¡è®­ç»ƒè„šæœ¬')
    
    # æ ¸å¿ƒå‚æ•°
    parser.add_argument('--train-side', type=str, required=True,
                        choices=['predator', 'prey'],
                        help='è®­ç»ƒæ–¹')
    parser.add_argument('--train-algo', type=str, required=True,
                        choices=['PPO', 'A2C', 'SAC', 'TD3', 'RANDOM'],
                        help='è®­ç»ƒç®—æ³•')
    parser.add_argument('--opponent-type', type=str, required=True,
                        choices=['algorithm', 'fixed_model', 'mixed_pool'],
                        help='å¯¹æ‰‹ç±»å‹')
    
    # å¯¹æ‰‹é…ç½®
    parser.add_argument('--opponent-algo', type=str, default='RANDOM',
                        help='å¯¹æ‰‹ç®—æ³•ï¼ˆå½“opponent-type=algorithmæ—¶ä½¿ç”¨ï¼‰')
    parser.add_argument('--opponent-model', type=str, default=None,
                        help='å¯¹æ‰‹æ¨¡å‹è·¯å¾„ï¼ˆå½“opponent-type=fixed_modelæ—¶ä½¿ç”¨ï¼‰')
    parser.add_argument('--opponent-pool', type=str, default=None,
                        help='å¯¹æ‰‹æ± è·¯å¾„ï¼ˆå½“opponent-type=mixed_poolæ—¶ä½¿ç”¨ï¼‰')
    parser.add_argument('--fixed-ratio', type=float, default=0.7,
                        help='å›ºå®šå¯¹æ‰‹å æ¯”ï¼ˆå½“opponent-type=mixed_poolæ—¶ä½¿ç”¨ï¼‰')
    
    # å®éªŒå…ƒæ•°æ®
    parser.add_argument('--experiment-name', type=str, required=True,
                        help='å®éªŒåç§°')
    parser.add_argument('--stage-name', type=str, required=True,
                        help='è®­ç»ƒé˜¶æ®µåç§°')
    parser.add_argument('--version', type=str, default='v1',
                        help='ç‰ˆæœ¬å·')
    parser.add_argument('--generation', type=int, default=0,
                        help='ä»£æ•°')
    
    # è¿è¡Œæ¨¡å¼
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun','test',  'prod'],
                        help='è¿è¡Œæ¨¡å¼')
    
    # ç¯å¢ƒé…ç½®
    parser.add_argument('--env-config', type=str, default='waterworld_standard',
                        help='ç¯å¢ƒé…ç½®åç§°')
    
    # è®­ç»ƒé…ç½®ï¼ˆè¦†ç›–é»˜è®¤å€¼ï¼‰
    parser.add_argument('--timesteps', type=int, default=None,
                        help='æ€»è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--n-envs', type=int, default=None,
                        help='å¹¶è¡Œç¯å¢ƒæ•°')
    parser.add_argument('--eval-freq', type=int, default=None,
                        help='è¯„ä¼°é¢‘ç‡')
    parser.add_argument('--checkpoint-freq', type=int, default=None,
                        help='æ£€æŸ¥ç‚¹é¢‘ç‡')
    
    # å…¶ä»–
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¡ç®—è®¾å¤‡ï¼ˆauto/cuda/cpuï¼‰')
    parser.add_argument('--seed', type=int, default=None,
                        help='éšæœºç§å­')
    parser.add_argument('--notes', type=str, default='',
                        help='å®éªŒå¤‡æ³¨')
    
    # ä¿å­˜é€‰é¡¹
    parser.add_argument('--save-to-pool', action='store_true',
                        help='æ˜¯å¦ä¿å­˜åˆ°å›ºå®šæ± ')
    parser.add_argument('--pool-name', type=str, default=None,
                        help='å›ºå®šæ± åç§°')
    parser.add_argument('--check-freeze', action='store_true',
                        help='æ˜¯å¦æ£€æŸ¥å†»ç»“æ¡ä»¶')
    
    return parser.parse_args()


def build_opponent_config(args):
    """æ ¹æ®å‚æ•°æ„å»ºå¯¹æ‰‹é…ç½®"""
    
    # å¯¹æ‰‹è§’è‰²ï¼ˆä¸è®­ç»ƒæ–¹ç›¸åï¼‰
    opponent_side = 'prey' if args.train_side == 'predator' else 'predator'
    
    if args.opponent_type == 'algorithm':
        return {
            'type': 'algorithm',
            'side': opponent_side,
            'algorithm': args.opponent_algo,
            'freeze': True
        }
    
    elif args.opponent_type == 'fixed_model':
        if not args.opponent_model:
            raise ValueError("ä½¿ç”¨fixed_modelæ—¶å¿…é¡»æŒ‡å®š--opponent-model")
        
        return {
            'type': 'fixed_model',
            'side': opponent_side,
            'path': args.opponent_model,
            'freeze': True
        }
    
    elif args.opponent_type == 'mixed_pool':
        if not args.opponent_pool:
            raise ValueError("ä½¿ç”¨mixed_poolæ—¶å¿…é¡»æŒ‡å®š--opponent-pool")
        
        return {
            'type': 'mixed_pool',
            'side': opponent_side,
            'pool_path': args.opponent_pool,
            'mix_strategy': {
                'fixed_ratio': args.fixed_ratio,
                'sampling': 'uniform'
            },
            'freeze': True
        }
    
    else:
        raise ValueError(f"æœªçŸ¥çš„å¯¹æ‰‹ç±»å‹: {args.opponent_type}")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # æ„å»ºå¯¹æ‰‹é…ç½®
    opponent_config = build_opponent_config(args)
    
    # åŠ è½½ç¯å¢ƒé…ç½®
    env_config = get_env_config(args.env_config)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiAgentTrainer(
        train_side=args.train_side,
        train_algo=args.train_algo,
        opponent_config=opponent_config,
        experiment_name=args.experiment_name,
        stage_name=args.stage_name,
        generation=args.generation,
        version=args.version,
        run_mode=args.mode,
        env_config=env_config,
        total_timesteps=args.timesteps,
        n_envs=args.n_envs,
        eval_freq=args.eval_freq,
        checkpoint_freq=args.checkpoint_freq,
        device=args.device,
        seed=args.seed,
        notes=args.notes
    )
    
    # è¿è¡Œè®­ç»ƒ
    freeze_criteria = None
    if args.check_freeze:
        from src.utils.config_loader import config_loader
        run_modes_config = config_loader.load_yaml("run_modes.yaml")
        freeze_criteria = run_modes_config['freeze_criteria'][args.train_side]
    
    trainer.run(
        save_to_pool=args.save_to_pool,
        pool_name=args.pool_name,
        check_freeze=args.check_freeze,
        freeze_criteria=freeze_criteria
    )
    
    print("\nâœ… è®­ç»ƒå®Œæˆï¼")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/training/train_stage1_1.py
================================================================================
"""
Stage 1.1: Preyé¢„çƒ­è®­ç»ƒ
è®­ç»ƒæ‰€æœ‰ç®—æ³•çš„Preyå¯¹æŠ—RANDOM Predator
"""

import argparse
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config, get_env_config  # â† åŠ ä¸Š get_env_config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Stage 1.1: Preyé¢„çƒ­è®­ç»ƒ')
    
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun', 'test', 'prod'],
                        help='è¿è¡Œæ¨¡å¼')
    
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3'],
                        help='è¦è®­ç»ƒçš„ç®—æ³•åˆ—è¡¨')
    
    parser.add_argument('--env-config', type=str, default='waterworld_standard',
                        help='ç¯å¢ƒé…ç½®')
    
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¡ç®—è®¾å¤‡')
    
    parser.add_argument('--timesteps', type=int, default=None,
                        help='æ€»è®­ç»ƒæ­¥æ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰')
    
    return parser.parse_args()


def train_one_prey_algo(algo: str, args, stage_config: dict):
    """è®­ç»ƒå•ä¸ªPreyç®—æ³•"""
    
    print(f"\n{'='*70}")
    print(f"{'è®°å½•' if algo == 'RANDOM' else 'è®­ç»ƒ'} {algo}_prey vs RANDOM_predator")
    print(f"{'='*70}\n")
    
    # æ„å»ºå¯¹æ‰‹é…ç½®
    opponent_config = {
        'type': 'algorithm',
        'side': 'predator',
        'algorithm': 'RANDOM',
        'freeze': True
    }
    
    # ç¡®å®šè®­ç»ƒæ­¥æ•°
    if algo == 'RANDOM':
        # RANDOM åªéœ€è¦è¿è¡Œè¶³å¤Ÿçš„ episodes æ¥è®°å½•åŸºçº¿
        # ä¾‹å¦‚è¿è¡Œ 50k æ­¥ï¼ˆå¤§çº¦ 50 ä¸ª episodesï¼‰
        timesteps = 5000
    else:
        # æ­£å¸¸ç®—æ³•çš„è®­ç»ƒæ­¥æ•°
        timesteps = args.timesteps
    
    # åˆ›å»ºè®­ç»ƒå™¨
    # âœ… æ ¹æ®æ¨¡å¼é€‰æ‹©ç¯å¢ƒé…ç½®
    if args.mode == 'test':
        env_config_name = 'waterworld_fast'
        print(f"ğŸƒ æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨å¿«é€Ÿç¯å¢ƒ: max_cycles=500")
    else:
        env_config_name = 'waterworld_standard'
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiAgentTrainer(
        train_side='prey',
        train_algo=algo,
        opponent_config=opponent_config,
        experiment_name=f"{algo}_prey_warmup",
        stage_name=stage_config['stage']['name'],
        generation=stage_config['stage']['generation'],
        version='v1',
        run_mode=args.mode,
        env_config=get_env_config(env_config_name),  # âœ… ä½¿ç”¨åŠ¨æ€ç¯å¢ƒ
        total_timesteps=timesteps,
        device=args.device
    )
    
    # è·å–å†»ç»“æ¡ä»¶
    freeze_config = stage_config.get('freeze_on_success', {})
    freeze_criteria = freeze_config.get('criteria', {})
    
    # è¿è¡Œè®­ç»ƒ
    # RANDOM ä¸éœ€è¦ä¿å­˜åˆ°æ± 
    save_to_pool = freeze_config.get('enabled', False) if algo != 'RANDOM' else False
    
    eval_results = trainer.run(
        save_to_pool=save_to_pool,
        pool_name=freeze_config.get('save_to_pool'),
        check_freeze=freeze_config.get('enabled', False) if algo != 'RANDOM' else False,
        freeze_criteria=freeze_criteria
    )
    
    return eval_results

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½Stage 1.1é…ç½®
    stage_config = get_training_config('stage1_1_prey_warmup')
    
    # è·å–ç®—æ³•åˆ—è¡¨
    algos_to_train = args.algos or stage_config.get('algorithms_to_train', ['PPO', 'A2C', 'SAC', 'TD3'])
    
    print(f"\n{'='*70}")
    print(f"ğŸ¯ Stage 1.1: Preyé¢„çƒ­è®­ç»ƒ")
    print(f"{'='*70}")
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"è®­ç»ƒç®—æ³•: {', '.join(algos_to_train)}")
    print(f"{'='*70}\n")
    
    # ========== æ–°å¢ï¼šå…ˆè¿è¡Œ RANDOM ä½œä¸ºåŸºçº¿ ==========
    print(f"\n{'='*70}")
    print(f"æ­¥éª¤ 0: è®°å½• RANDOM Baseline")
    print(f"{'='*70}\n")
    
    # å°† RANDOM æ·»åŠ åˆ°è®­ç»ƒåˆ—è¡¨çš„æœ€å‰é¢
    all_algos = ['RANDOM'] + algos_to_train
    
    # ========== è®­ç»ƒæ‰€æœ‰ç®—æ³•ï¼ˆåŒ…æ‹¬ RANDOMï¼‰==========
    results = {}
    for algo in all_algos:
        try:
            eval_results = train_one_prey_algo(algo, args, stage_config)
            results[algo] = eval_results
        except KeyboardInterrupt:
            print(f"\nâš ï¸  {algo} è®­ç»ƒè¢«ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ {algo} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ‰“å°æ±‡æ€»ï¼ˆçªå‡ºæ˜¾ç¤ºä¸ RANDOM çš„å¯¹æ¯”ï¼‰
    print(f"\n{'='*70}")
    print(f"âœ… Stage 1.1 å®Œæˆ")
    print(f"{'='*70}")
    
    # æ˜¾ç¤º RANDOM åŸºçº¿
    if 'RANDOM' in results and results['RANDOM']:
        baseline_reward = results['RANDOM'].get('mean_reward', 0)
        print(f"\nğŸ“Š Random Baseline: {baseline_reward:.2f}")
        
        print(f"\nè®­ç»ƒç»“æœï¼ˆç›¸å¯¹äº Randomï¼‰:")
        for algo in algos_to_train:  # åªæ˜¾ç¤ºè®­ç»ƒçš„ç®—æ³•
            if algo in results and results[algo]:
                reward = results[algo].get('mean_reward', 0)
                improvement = ((reward - baseline_reward) / abs(baseline_reward) * 100) if baseline_reward != 0 else 0
                status = "âœ…" if improvement > 0 else "âŒ"
                print(f"  {status} {algo}: {reward:.2f} ({improvement:+.1f}% vs Random)")
    else:
        print(f"\nè®­ç»ƒç»“æœ:")
        for algo, result in results.items():
            if result:
                print(f"  {algo}: å¹³å‡å¥–åŠ± = {result.get('mean_reward', 'N/A'):.2f}")
    
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/training/train_stage1_2.py
================================================================================
"""
Stage 1.2: Predatorå¼•å¯¼è®­ç»ƒ
è®­ç»ƒæ‰€æœ‰ç®—æ³•çš„Predatorå¯¹æŠ—å›ºå®šçš„Preyæ± v1
"""

import argparse
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config, get_env_config  # â† æ·»åŠ  get_env_config

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Stage 1.2: Predatorå¼•å¯¼è®­ç»ƒ')
    
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun','test',  'prod'],
                        help='è¿è¡Œæ¨¡å¼')
    
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3'],
                        help='è¦è®­ç»ƒçš„ç®—æ³•åˆ—è¡¨')
    
    parser.add_argument('--prey-pool', type=str, default='outputs/fixed_pools/prey_pool_v1',
                        help='Preyå›ºå®šæ± è·¯å¾„')
    
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¡ç®—è®¾å¤‡')
    
    parser.add_argument('--timesteps', type=int, default=None,
                        help='æ€»è®­ç»ƒæ­¥æ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰')
    
    return parser.parse_args()


def train_one_predator_algo(algo: str, args, stage_config: dict):
    """è®­ç»ƒå•ä¸ªPredatorç®—æ³•"""
    
    print(f"\n{'='*70}")
    print(f"{'è®°å½•' if algo == 'RANDOM' else 'è®­ç»ƒ'} {algo}_predator vs prey_pool_v1")
    print(f"{'='*70}\n")
    
    # æ„å»ºå¯¹æ‰‹é…ç½®
    opponent_config = {
        'type': 'mixed_pool',
        'side': 'prey',
        'pool_path': args.prey_pool,
        'mix_strategy': {
            'fixed_ratio': 0.7,
            'sampling': 'uniform'
        },
        'freeze': True
    }
    
    # âœ… æ·»åŠ è¿™æ®µï¼šç¡®å®šè®­ç»ƒæ­¥æ•°
    if algo == 'RANDOM':
        # RANDOM åªéœ€è¦è¿è¡Œè¶³å¤Ÿçš„ episodes æ¥è®°å½•åŸºçº¿
        timesteps = 50000  # å¤§çº¦ 50 ä¸ª episodes
    else:
        # æ­£å¸¸ç®—æ³•çš„è®­ç»ƒæ­¥æ•°
        timesteps = args.timesteps
    # âœ… æ ¹æ®æ¨¡å¼é€‰æ‹©ç¯å¢ƒé…ç½®
    if args.mode == 'test':
        env_config_name = 'waterworld_fast'
        print(f"ğŸƒ æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨å¿«é€Ÿç¯å¢ƒ: max_cycles=500")
    else:
        env_config_name = 'waterworld_standard'
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiAgentTrainer(
        train_side='predator',
        train_algo=algo,
        opponent_config=opponent_config,
        experiment_name=f"{algo}_predator_guided",
        env_config=get_env_config(env_config_name),  # âœ… ä½¿ç”¨åŠ¨æ€ç¯å¢ƒ
        stage_name=stage_config['stage']['name'],
        generation=stage_config['stage']['generation'],
        version='v1',
        run_mode=args.mode,
        total_timesteps=timesteps,  # âœ… ä½¿ç”¨ timesteps è€Œä¸æ˜¯ args.timesteps
        device=args.device
    )
    
    # è·å–å†»ç»“æ¡ä»¶
    freeze_config = stage_config.get('freeze_on_success', {})
    freeze_criteria = freeze_config.get('criteria', {})
    
    # è¿è¡Œè®­ç»ƒ
    # RANDOM ä¸éœ€è¦ä¿å­˜åˆ°æ± 
    save_to_pool = freeze_config.get('enabled', False) if algo != 'RANDOM' else False
    
    eval_results = trainer.run(
        save_to_pool=save_to_pool,
        pool_name=freeze_config.get('save_to_pool'),
        check_freeze=freeze_config.get('enabled', False) if algo != 'RANDOM' else False,
        freeze_criteria=freeze_criteria
    )
    
    return eval_results

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # æ£€æŸ¥prey_poolæ˜¯å¦å­˜åœ¨
    prey_pool_path = Path(args.prey_pool)
    if not prey_pool_path.exists():
        print(f"âŒ Preyå›ºå®šæ± ä¸å­˜åœ¨: {prey_pool_path}")
        print(f"è¯·å…ˆè¿è¡Œ Stage 1.1 è®­ç»ƒ")
        sys.exit(1)
    
    # åŠ è½½Stage 1.2é…ç½®
    stage_config = get_training_config('stage1_2_pred_guided')
    
    # è·å–ç®—æ³•åˆ—è¡¨
    algos_to_train = args.algos or stage_config.get('algorithms_to_train', ['PPO', 'A2C', 'SAC', 'TD3'])
    
    print(f"\n{'='*70}")
    print(f"ğŸ¯ Stage 1.2: Predatorå¼•å¯¼è®­ç»ƒ")
    print(f"{'='*70}")
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"è®­ç»ƒç®—æ³•: {', '.join(algos_to_train)}")
    print(f"Preyæ± è·¯å¾„: {args.prey_pool}")
    print(f"{'='*70}\n")
    
    # ========== æ–°å¢ï¼šå…ˆè¿è¡Œ RANDOM ä½œä¸ºåŸºçº¿ ==========
    print(f"\n{'='*70}")
    print(f"æ­¥éª¤ 0: è®°å½• RANDOM Baseline")
    print(f"{'='*70}\n")
    
    # å°† RANDOM æ·»åŠ åˆ°è®­ç»ƒåˆ—è¡¨çš„æœ€å‰é¢
    all_algos = ['RANDOM'] + algos_to_train
    
    # ========== è®­ç»ƒæ‰€æœ‰ç®—æ³•ï¼ˆåŒ…æ‹¬ RANDOMï¼‰==========
    results = {}
    for algo in all_algos:
        try:
            eval_results = train_one_predator_algo(algo, args, stage_config)
            results[algo] = eval_results
        except KeyboardInterrupt:
            print(f"\nâš ï¸  {algo} è®­ç»ƒè¢«ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ {algo} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ========== æ‰“å°æ±‡æ€»ï¼ˆçªå‡ºæ˜¾ç¤ºä¸ RANDOM çš„å¯¹æ¯”ï¼‰==========
    print(f"\n{'='*70}")
    print(f"âœ… Stage 1.2 å®Œæˆ")
    print(f"{'='*70}")
    
    # æ˜¾ç¤º RANDOM åŸºçº¿
    if 'RANDOM' in results and results['RANDOM']:
        baseline_reward = results['RANDOM'].get('mean_reward', 0)
        print(f"\nğŸ“Š Random Baseline: {baseline_reward:.2f}")
        
        print(f"\nè®­ç»ƒç»“æœï¼ˆç›¸å¯¹äº Randomï¼‰:")
        for algo in algos_to_train:  # åªæ˜¾ç¤ºè®­ç»ƒçš„ç®—æ³•
            if algo in results and results[algo]:
                reward = results[algo].get('mean_reward', 0)
                improvement = ((reward - baseline_reward) / abs(baseline_reward) * 100) if baseline_reward != 0 else 0
                status = "âœ…" if improvement > 0 else "âŒ"
                print(f"  {status} {algo}: {reward:.2f} ({improvement:+.1f}% vs Random)")
    else:
        print(f"\nè®­ç»ƒç»“æœ:")
        for algo, result in results.items():
            if result:
                reward = result.get('mean_reward', 0)
                if isinstance(reward, (int, float)):
                    print(f"  {algo}: å¹³å‡å¥–åŠ± = {reward:.2f}")
                else:
                    print(f"  {algo}: å¹³å‡å¥–åŠ± = {reward}")
    
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/training/train_stage1_3.py
================================================================================
"""
Stage 1.3: å…±è¿›åŒ–è®­ç»ƒ
Predatorå’ŒPreyäº¤æ›¿è®­ç»ƒï¼Œå…±åŒè¿›åŒ–
"""

import argparse
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config, get_env_config  # â† åŠ ä¸Š get_env_config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Stage 1.3: å…±è¿›åŒ–è®­ç»ƒ')
    
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun','test',  'prod'],
                        help='è¿è¡Œæ¨¡å¼')
    
    parser.add_argument('--max-generations', type=int, default=20,
                        help='æœ€å¤§ä»£æ•°')
    
    parser.add_argument('--start-generation', type=int, default=2,
                        help='èµ·å§‹ä»£æ•°')
    
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3'],
                        help='è¦è®­ç»ƒçš„ç®—æ³•åˆ—è¡¨')
    
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¡ç®—è®¾å¤‡')
    
    parser.add_argument('--timesteps-per-gen', type=int, default=None,
                        help='æ¯ä»£çš„è®­ç»ƒæ­¥æ•°')
    
    return parser.parse_args()


def train_one_generation(
    generation: int,
    train_side: str,
    opponent_pool_path: str,
    algos: list,
    args,
    stage_config: dict
):
    """è®­ç»ƒä¸€ä»£"""
    
    print(f"\n{'='*70}")
    print(f"ğŸ”„ Generation {generation}: è®­ç»ƒ {train_side.upper()}")
    print(f"{'='*70}\n")
    
    results = {}
    
    for algo in algos:
        print(f"\n{'-'*70}")
        print(f"è®­ç»ƒ {algo}_{train_side} (Gen {generation})")
        print(f"{'-'*70}\n")
        
        # æ„å»ºå¯¹æ‰‹é…ç½®
        opponent_side = 'prey' if train_side == 'predator' else 'predator'
        opponent_config = {
            'type': 'mixed_pool',
            'side': opponent_side,
            'pool_path': opponent_pool_path,
            'mix_strategy': {
                'fixed_ratio': 0.7,
                'sampling': 'uniform'
            },
            'freeze': True
        }
        
        # âœ… æ ¹æ®æ¨¡å¼é€‰æ‹©ç¯å¢ƒé…ç½®
        if args.mode == 'test':
            env_config_name = 'waterworld_fast'
            print(f"ğŸƒ æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨å¿«é€Ÿç¯å¢ƒ: max_cycles=500")
        else:
            env_config_name = 'waterworld_standard'
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = MultiAgentTrainer(
            train_side=train_side,
            train_algo=algo,
            opponent_config=opponent_config,
            experiment_name=f"{algo}_{train_side}_coevo",
            stage_name=f"{stage_config['stage']['name']}/Gen_{generation}",
            generation=generation,
            version=f"v{generation}",
            run_mode=args.mode,
            env_config=get_env_config(env_config_name),  # âœ… ä½¿ç”¨åŠ¨æ€ç¯å¢ƒ
            total_timesteps=args.timesteps_per_gen,
            device=args.device
        )
        
        # è·å–å†»ç»“æ¡ä»¶
        freeze_config = stage_config.get('freeze_on_success', {})
        freeze_criteria = freeze_config.get('criteria', {}).get(train_side, {})
        
        # è¿è¡Œè®­ç»ƒ
        try:
            eval_results = trainer.run(
                save_to_pool=freeze_config.get('enabled', False),
                pool_name=f"{train_side}_pool_v{generation}",
                check_freeze=freeze_config.get('enabled', False),
                freeze_criteria=freeze_criteria
            )
            results[algo] = eval_results
        
        except Exception as e:
            print(f"\nâŒ {algo} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results[algo] = None
    
    return results


def check_convergence(generation_results: list, threshold: float = 0.03) -> bool:
    """
    æ£€æŸ¥æ˜¯å¦æ”¶æ•›
    
    Args:
        generation_results: æœ€è¿‘å‡ ä»£çš„ç»“æœåˆ—è¡¨
        threshold: æ€§èƒ½å˜åŒ–é˜ˆå€¼
    
    Returns:
        æ˜¯å¦æ”¶æ•›
    """
    if len(generation_results) < 5:
        return False
    
    # å–æœ€è¿‘5ä»£çš„å¹³å‡å¥–åŠ±
    recent_rewards = []
    for gen_result in generation_results[-5:]:
        rewards = [r.get('mean_reward', 0) for r in gen_result.values() if r]
        if rewards:
            recent_rewards.append(sum(rewards) / len(rewards))
    
    if len(recent_rewards) < 5:
        return False
    
    # è®¡ç®—å˜åŒ–ç‡
    mean_reward = sum(recent_rewards) / len(recent_rewards)
    max_deviation = max(abs(r - mean_reward) for r in recent_rewards)
    
    change_rate = max_deviation / (abs(mean_reward) + 1e-6)
    
    print(f"\nğŸ“Š æ”¶æ•›æ£€æŸ¥:")
    print(f"  æœ€è¿‘5ä»£å¹³å‡å¥–åŠ±: {recent_rewards}")
    print(f"  å‡å€¼: {mean_reward:.2f}")
    print(f"  æœ€å¤§åå·®: {max_deviation:.2f}")
    print(f"  å˜åŒ–ç‡: {change_rate:.2%}")
    print(f"  é˜ˆå€¼: {threshold:.2%}")
    
    return change_rate < threshold


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    # âœ… æ ¹æ®è¿è¡Œæ¨¡å¼é€‰æ‹©é…ç½®æ–‡ä»¶
    if args.mode == 'test':
        config_name = 'stage1_3_coevolution_test'  # ä½¿ç”¨æµ‹è¯•é…ç½®
        print("ğŸ§ª TESTæ¨¡å¼ï¼šä½¿ç”¨æµ‹è¯•é…ç½®ï¼ˆ1ä»£ï¼Œ2ç®—æ³•ï¼‰")
    else:
        config_name = 'stage1_3_coevolution'        # ä½¿ç”¨æ­£å¼é…ç½®
    # æ£€æŸ¥åˆå§‹æ± æ˜¯å¦å­˜åœ¨
    prey_pool_path = Path("outputs/fixed_pools/prey_pool_v1")
    pred_pool_path = Path("outputs/fixed_pools/predator_pool_v1")
    
    if not prey_pool_path.exists() or not pred_pool_path.exists():
        print("âŒ åˆå§‹å¯¹æ‰‹æ± ä¸å­˜åœ¨")
        print(f"  Preyæ± : {prey_pool_path} - {'âœ“' if prey_pool_path.exists() else 'âœ—'}")
        print(f"  Predatoræ± : {pred_pool_path} - {'âœ“' if pred_pool_path.exists() else 'âœ—'}")
        print("\nè¯·å…ˆè¿è¡Œ Stage 1.1 å’Œ Stage 1.2")
        sys.exit(1)
    
    # åŠ è½½Stage 1.3é…ç½®
    stage_config = get_training_config(config_name)
    
    # è·å–é…ç½®
    coevo_config = stage_config.get('coevolution', {})
    max_generations = args.max_generations or coevo_config.get('max_generations', 20)
    start_generation = args.start_generation or coevo_config.get('start_generation', 2)
    
    algos_to_train = args.algos or stage_config.get('algorithms_to_train', ['PPO', 'A2C', 'SAC', 'TD3'])
    
    print(f"\n{'='*70}")
    print(f"ğŸ¯ Stage 1.3: å…±è¿›åŒ–è®­ç»ƒ")
    print(f"{'='*70}")
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"è®­ç»ƒç®—æ³•: {', '.join(algos_to_train)}")
    print(f"ä»£æ•°èŒƒå›´: {start_generation} - {max_generations}")
    print(f"{'='*70}\n")
    
    # è®°å½•æ‰€æœ‰ä»£çš„ç»“æœ
    all_results = []
    
    # å…±è¿›åŒ–å¾ªç¯
    for generation in range(start_generation, max_generations + 1):
        
        # å¥‡å¶ä»£äº¤æ›¿è®­ç»ƒ
        if generation % 2 == 0:
            # å¶æ•°ä»£ï¼šè®­ç»ƒPredator
            train_side = 'predator'
            opponent_pool = str(prey_pool_path)
        else:
            # å¥‡æ•°ä»£ï¼šè®­ç»ƒPrey
            train_side = 'prey'
            opponent_pool = str(pred_pool_path)
        
        # è®­ç»ƒå½“å‰ä»£
        try:
            gen_results = train_one_generation(
                generation=generation,
                train_side=train_side,
                opponent_pool_path=opponent_pool,
                algos=algos_to_train,
                args=args,
                stage_config=stage_config
            )
            
            all_results.append(gen_results)
        
        except KeyboardInterrupt:
            print(f"\nâš ï¸  Generation {generation} è¢«ä¸­æ–­")
            break
        
        except Exception as e:
            print(f"\nâŒ Generation {generation} å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            break
        
        # æ£€æŸ¥æ”¶æ•›
        convergence_config = coevo_config.get('convergence', {})
        if convergence_config.get('enabled', True):
            if check_convergence(
                all_results,
                threshold=convergence_config.get('performance_change_threshold', 0.03)
            ):
                print(f"\nâœ… åœ¨ Generation {generation} è¾¾åˆ°æ”¶æ•›")
                break
    
    # æ‰“å°æœ€ç»ˆæ±‡æ€»
    print(f"\n{'='*70}")
    print(f"âœ… Stage 1.3 å®Œæˆ")
    print(f"{'='*70}")
    print(f"æ€»å…±è®­ç»ƒäº† {len(all_results)} ä»£")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/evaluation/cross_eval.py
================================================================================
"""
äº¤å‰è¯„ä¼°ä¸»è„šæœ¬
æ‰§è¡Œ 5Ã—5 ç®—æ³•å¯¹æˆ˜çŸ©é˜µè¯„ä¼°
"""

import sys
import argparse
import pickle
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from eval_single_matchup import evaluate_single_matchup
from metrics_calculator import MetricsCalculator
from src.utils.config_loader import config_loader


def parse_args():
    parser = argparse.ArgumentParser(description='äº¤å‰è¯„ä¼°')
    parser.add_argument('--mode', type=str, default='test',
                        choices=['test', 'dryrun', 'prod'],
                        help='è¯„ä¼°æ¨¡å¼')
    parser.add_argument('--n-episodes', type=int, default=None,
                        help='æ¯ä¸ªç»„åˆçš„è¯„ä¼°episodeæ•°')
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3', 'RANDOM'],
                        help='è¦è¯„ä¼°çš„ç®—æ³•åˆ—è¡¨')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='è¾“å‡ºç›®å½•')
    return parser.parse_args()


def find_model_path(base_dir: Path, algo: str, side: str) -> Path:
    """
    æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
    
    ä¼˜å…ˆçº§ï¼š
    1. æœ€æ–°çš„æ¨¡å‹ï¼ˆæŒ‰æ—¶é—´æˆ³ï¼‰
    2. å›ºå®šæ± ä¸­çš„æ¨¡å‹
    """
    # æœç´¢è·¯å¾„
    search_patterns = [
        base_dir / f"stage1.*_{side}_*" / f"{algo}_{side}_*.zip",
        base_dir / f"*{side}*" / f"{algo}_{side}_*.zip",
        base_dir / f"**/{algo}_{side}_*.zip",
    ]
    
    found_models = []
    for pattern in search_patterns:
        found_models.extend(base_dir.glob(str(pattern.relative_to(base_dir))))
    
    if not found_models:
        return None
    
    # è¿”å›æœ€æ–°çš„
    found_models.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return found_models[0]


def main():
    args = parse_args()
    
    # åŠ è½½é…ç½®
    eval_config = config_loader.load_yaml('evaluation/cross_eval.yaml')
    mode_config = eval_config['evaluation']['model_pools'][args.mode]
    
    # å‚æ•°
    algos = args.algos
    n_episodes = args.n_episodes or eval_config['evaluation']['n_episodes']
    output_dir = Path(args.output_dir) if args.output_dir else \
                 Path(f"{args.mode}_outputs/evaluation_results")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = output_dir / f"cross_eval_{timestamp}"
    run_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*70)
    print("ğŸ§ª äº¤å‰è¯„ä¼° (Cross-Algorithm Evaluation)")
    print("="*70)
    print(f"æ¨¡å¼:        {args.mode}")
    print(f"ç®—æ³•:        {', '.join(algos)}")
    print(f"Episodes:    {n_episodes}")
    print(f"è¾“å‡ºç›®å½•:    {run_dir}")
    print("="*70 + "\n")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹
    print("ğŸ“‚ æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶...")
    
    saved_models_base = Path(mode_config['saved_models_base'])
    
    predator_models = {}
    prey_models = {}
    
    for algo in algos:
        if algo == 'RANDOM':
            predator_models[algo] = None
            prey_models[algo] = None
            print(f"  âœ“ {algo}: ä½¿ç”¨éšæœºç­–ç•¥")
            continue
        
        # æŸ¥æ‰¾ predator
        pred_path = find_model_path(saved_models_base, algo, 'predator')
        if pred_path:
            predator_models[algo] = pred_path
            print(f"  âœ“ {algo}_predator: {pred_path.name}")
        else:
            print(f"  âœ— {algo}_predator: æœªæ‰¾åˆ°")
            predator_models[algo] = None
        
        # æŸ¥æ‰¾ prey
        prey_path = find_model_path(saved_models_base, algo, 'prey')
        if prey_path:
            prey_models[algo] = prey_path
            print(f"  âœ“ {algo}_prey: {prey_path.name}")
        else:
            print(f"  âœ— {algo}_prey: æœªæ‰¾åˆ°")
            prey_models[algo] = None
    
    # æ‰§è¡Œäº¤å‰è¯„ä¼°
    print(f"\n{'='*70}")
    print(f"ğŸ¯ å¼€å§‹äº¤å‰è¯„ä¼° ({len(algos)}Ã—{len(algos)} = {len(algos)**2} ç»„åˆ)")
    print(f"{'='*70}\n")
    
    results_matrix = {}
    total_matchups = len(algos) * len(algos)
    current_matchup = 0
    
    for pred_algo in algos:
        results_matrix[pred_algo] = {}
        
        for prey_algo in algos:
            current_matchup += 1
            print(f"\n[{current_matchup}/{total_matchups}] "
                  f"è¯„ä¼°: {pred_algo}_pred vs {prey_algo}_prey")
            print("-" * 70)
            
            # è·å–æ¨¡å‹è·¯å¾„
            pred_path = predator_models.get(pred_algo)
            prey_path = prey_models.get(prey_algo)
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            if pred_path is None and pred_algo != 'RANDOM':
                print(f"  âš ï¸  è·³è¿‡: {pred_algo}_predator æ¨¡å‹æœªæ‰¾åˆ°")
                results_matrix[pred_algo][prey_algo] = None
                continue
            
            if prey_path is None and prey_algo != 'RANDOM':
                print(f"  âš ï¸  è·³è¿‡: {prey_algo}_prey æ¨¡å‹æœªæ‰¾åˆ°")
                results_matrix[pred_algo][prey_algo] = None
                continue
            
            # æ‰§è¡Œè¯„ä¼°
            try:
                metrics = evaluate_single_matchup(
                    predator_model_path=pred_path,
                    prey_model_path=prey_path,
                    predator_algo=pred_algo,
                    prey_algo=prey_algo,
                    env_config_name='waterworld_fast',
                    n_episodes=n_episodes,
                    deterministic=True,
                    verbose=1
                )
                
                results_matrix[pred_algo][prey_algo] = metrics
                
            except Exception as e:
                print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                results_matrix[pred_algo][prey_algo] = None
    
    # ä¿å­˜åŸå§‹ç»“æœ
    print(f"\n{'='*70}")
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    print(f"{'='*70}")
    
    # ä¿å­˜ä¸ºpickleï¼ˆå®Œæ•´æ•°æ®ï¼‰
    raw_results_path = run_dir / "raw_results.pkl"
    with open(raw_results_path, 'wb') as f:
        pickle.dump(results_matrix, f)
    print(f"  âœ“ åŸå§‹ç»“æœ: {raw_results_path}")
    
    # ä¿å­˜ä¸ºJSONï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
    json_results = {}
    for pred_algo, prey_dict in results_matrix.items():
        json_results[pred_algo] = {}
        for prey_algo, metrics in prey_dict.items():
            if metrics is not None:
                # åªä¿å­˜ä¸»è¦æŒ‡æ ‡
                json_results[pred_algo][prey_algo] = {
                    'catch_rate': metrics['catch_rate'],
                    'survival_rate': metrics['survival_rate'],
                    'pred_avg_reward': metrics['pred_avg_reward'],
                    'prey_avg_reward': metrics['prey_avg_reward'],
                    'balance_score': metrics['balance_score'],
                    'is_ood': metrics['is_ood']
                }
            else:
                json_results[pred_algo][prey_algo] = None
    
    json_path = run_dir / "results_summary.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    print(f"  âœ“ JSONæ‘˜è¦: {json_path}")
    
    # è®¡ç®—è‡ªé€‚åº”æ€§å¾—åˆ†
    print(f"\n{'='*70}")
    print("ğŸ“Š è®¡ç®—è‡ªé€‚åº”æ€§å¾—åˆ†...")
    print(f"{'='*70}\n")
    
    # è¿‡æ»¤æ‰Noneç»“æœ
    valid_results = {}
    for pred_algo, prey_dict in results_matrix.items():
        if pred_algo == 'RANDOM':
            continue
        valid_results[pred_algo] = {}
        for prey_algo, metrics in prey_dict.items():
            if metrics is not None:
                valid_results[pred_algo][prey_algo] = metrics
    
    if valid_results:
        adaptability_scores = MetricsCalculator.compute_adaptability_scores(valid_results)
        ranking = MetricsCalculator.compute_ranking(adaptability_scores)
        
        # æ‰“å°æ’å
        print(f"{'='*80}")
        print("Algorithm Adaptability Ranking (Predator)")
        print(f"{'='*80}")
        print(f"{'Rank':<6} {'Algorithm':<10} {'In-Dist':<10} {'OOD Avg':<10} "
              f"{'Adapt':<8} {'Drop':<8} {'Std':<8}")
        print("-"*80)
        
        for scores in ranking:
            print(f"{scores['rank']:<6} {scores['algorithm']:<10} "
                  f"{scores['in_dist_performance']:<10.3f} "
                  f"{scores['ood_avg_performance']:<10.3f} "
                  f"{scores['adaptability_score']:<8.3f} "
                  f"{scores['performance_drop']:<8.3f} "
                  f"{scores['ood_std']:<8.3f}")
        
        print(f"{'='*80}\n")
        
        # ä¿å­˜è‡ªé€‚åº”æ€§å¾—åˆ†
        adapt_path = run_dir / "adaptability_scores.json"
        with open(adapt_path, 'w', encoding='utf-8') as f:
            json.dump({
                'scores': adaptability_scores,
                'ranking': ranking
            }, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ è‡ªé€‚åº”æ€§å¾—åˆ†: {adapt_path}")
    
    # ç”Ÿæˆæ€§èƒ½çŸ©é˜µï¼ˆCSVæ ¼å¼ï¼‰
    print(f"\n{'='*70}")
    print("ğŸ“‹ ç”Ÿæˆæ€§èƒ½çŸ©é˜µ...")
    print(f"{'='*70}\n")
    
    # Catch RateçŸ©é˜µ
    catch_rate_matrix = []
    header = ['Predator\\Prey'] + algos
    catch_rate_matrix.append(header)
    
    for pred_algo in algos:
        row = [pred_algo]
        for prey_algo in algos:
            metrics = results_matrix[pred_algo].get(prey_algo)
            if metrics:
                row.append(f"{metrics['catch_rate']:.3f}")
            else:
                row.append("N/A")
        catch_rate_matrix.append(row)
    
    # ä¿å­˜ä¸ºCSV
    csv_path = run_dir / "catch_rate_matrix.csv"
    import csv
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerows(catch_rate_matrix)
    print(f"  âœ“ Catch RateçŸ©é˜µ: {csv_path}")
    
    # æ‰“å°çŸ©é˜µé¢„è§ˆ
    print("\n  Catch Rate Matrix Preview:")
    print("  " + "-" * 60)
    for row in catch_rate_matrix[:6]:  # åªæ˜¾ç¤ºå‰6è¡Œ
        print("  " + " | ".join(f"{cell:>12}" for cell in row))
    print("  " + "-" * 60)
    
    # ä¿å­˜é…ç½®å¿«ç…§
    config_snapshot = {
        'mode': args.mode,
        'algorithms': algos,
        'n_episodes': n_episodes,
        'timestamp': timestamp,
        'model_paths': {
            'predators': {k: str(v) if v else None for k, v in predator_models.items()},
            'preys': {k: str(v) if v else None for k, v in prey_models.items()}
        }
    }
    
    config_path = run_dir / "eval_config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_snapshot, f, indent=2, ensure_ascii=False)
    print(f"  âœ“ é…ç½®å¿«ç…§: {config_path}")
    
    # å®Œæˆ
    print(f"\n{'='*70}")
    print("âœ… äº¤å‰è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {run_dir}")
    print("\nä¸‹ä¸€æ­¥ï¼š")
    print(f"  1. æŸ¥çœ‹ç»“æœ: cat {json_path}")
    print(f"  2. ç”Ÿæˆå¯è§†åŒ–: python scripts/analysis/plot_results.py --input {run_dir}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/evaluation/eval_random_baseline.py
================================================================================
"""
Random Baseline è¯„ä¼°è„šæœ¬
åœ¨è®­ç»ƒå‰å…ˆè¯„ä¼°éšæœºç­–ç•¥çš„æ€§èƒ½ï¼Œå»ºç«‹åŸºçº¿
"""

import argparse
import sys
import json
from pathlib import Path
from datetime import datetime
import numpy as np

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Random Baseline è¯„ä¼°')
    
    parser.add_argument('--stage', type=str, required=True,
                        choices=['1.1', '1.2'],
                        help='è®­ç»ƒé˜¶æ®µï¼ˆ1.1=Preyé¢„çƒ­, 1.2=Predatorå¼•å¯¼ï¼‰')
    
    parser.add_argument('--n-episodes', type=int, default=20,
                        help='è¯„ä¼°episodeæ•°')
    
    parser.add_argument('--env-config', type=str, default='waterworld_standard',
                        help='ç¯å¢ƒé…ç½®')
    
    parser.add_argument('--device', type=str, default='cpu',
                        help='è®¡ç®—è®¾å¤‡')
    
    parser.add_argument('--output-dir', type=str, default='outputs/baselines',
                        help='ç»“æœä¿å­˜ç›®å½•')
    
    return parser.parse_args()


def evaluate_random_baseline(stage: str, args, stage_config: dict):
    """
    è¯„ä¼° Random Baseline
    
    Args:
        stage: è®­ç»ƒé˜¶æ®µ
        args: å‘½ä»¤è¡Œå‚æ•°
        stage_config: é˜¶æ®µé…ç½®
    """
    
    print(f"\n{'='*70}")
    print(f"ğŸ² Random Baseline è¯„ä¼° - Stage {stage}")
    print(f"{'='*70}\n")
    
    # æ ¹æ®é˜¶æ®µç¡®å®šè®­ç»ƒæ–¹å’Œå¯¹æ‰‹
    if stage == '1.1':
        train_side = 'prey'
        opponent_config = {
            'type': 'algorithm',
            'side': 'predator',
            'algorithm': 'RANDOM',
            'freeze': True
        }
        scenario = "Random Prey vs Random Predator"
    
    elif stage == '1.2':
        train_side = 'predator'
        prey_pool = 'outputs/fixed_pools/prey_pool_v1'
        
        # æ£€æŸ¥ prey_pool æ˜¯å¦å­˜åœ¨
        if not Path(prey_pool).exists():
            print(f"âŒ Preyæ± ä¸å­˜åœ¨: {prey_pool}")
            print(f"   è¯·å…ˆè¿è¡Œ Stage 1.1 è®­ç»ƒ")
            sys.exit(1)
        
        opponent_config = {
            'type': 'mixed_pool',
            'side': 'prey',
            'pool_path': prey_pool,
            'mix_strategy': {
                'fixed_ratio': 0.7,
                'sampling': 'uniform'
            },
            'freeze': True
        }
        scenario = "Random Predator vs prey_pool_v1"
    
    else:
        raise ValueError(f"æœªçŸ¥çš„é˜¶æ®µ: {stage}")
    
    print(f"åœºæ™¯: {scenario}")
    print(f"è¯„ä¼°Episodes: {args.n_episodes}\n")
    
    # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆä½¿ç”¨ RANDOM ç®—æ³•ï¼‰
    trainer = MultiAgentTrainer(
        train_side=train_side,
        train_algo='RANDOM',
        opponent_config=opponent_config,
        experiment_name=f"random_baseline_stage{stage}",
        stage_name=f"baseline_stage{stage}",
        generation=0,
        version='baseline',
        run_mode='dryrun',  # ä½¿ç”¨ dryrun æ¨¡å¼
        total_timesteps=0,  # ä¸è®­ç»ƒï¼Œåªè¯„ä¼°
        device=args.device
    )
    
    # è®¾ç½®ç¯å¢ƒ
    trainer.setup()
    
    print(f"å¼€å§‹è¯„ä¼°...")
    print(f"{'-'*70}\n")
    
    # è¯„ä¼°
    eval_results = trainer.evaluate(n_episodes=args.n_episodes)
    
    # æ¸…ç†
    trainer.cleanup()
    
    return eval_results, scenario


def save_baseline_results(stage: str, scenario: str, results: dict, output_dir: Path):
    """ä¿å­˜åŸºçº¿ç»“æœ"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"random_baseline_stage{stage}_{timestamp}.json"
    filepath = output_dir / filename
    
    # æ„å»ºå®Œæ•´è®°å½•
    record = {
        'timestamp': datetime.now().isoformat(),
        'stage': stage,
        'scenario': scenario,
        'baseline_type': 'RANDOM',
        'results': results
    }
    
    # ä¿å­˜
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(record, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ åŸºçº¿ç»“æœå·²ä¿å­˜: {filepath}")
    
    return filepath


def print_baseline_summary(stage: str, scenario: str, results: dict):
    """æ‰“å°åŸºçº¿æ‘˜è¦"""
    print(f"\n{'='*70}")
    print(f"ğŸ“Š Random Baseline ç»“æœ - Stage {stage}")
    print(f"{'='*70}")
    print(f"\nåœºæ™¯: {scenario}")
    print(f"\næ€§èƒ½æŒ‡æ ‡:")
    print(f"  å¹³å‡å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
    print(f"  æœ€å¤§å¥–åŠ±: {results['max_reward']:.2f}")
    print(f"  æœ€å°å¥–åŠ±: {results['min_reward']:.2f}")
    print(f"  å¹³å‡Episodeé•¿åº¦: {results['mean_length']:.0f} Â± {results['std_length']:.0f}")
    print(f"  è¯„ä¼°Episodes: {results['n_episodes']}")
    print(f"\n{'='*70}")
    print(f"\nğŸ’¡ æç¤º:")
    print(f"   - è®­ç»ƒåçš„ç®—æ³•åº”è¯¥æ˜¾è‘—è¶…è¿‡è¿™ä¸ªåŸºçº¿")
    print(f"   - å¦‚æœè®­ç»ƒåæ€§èƒ½ < åŸºçº¿ï¼Œè¯´æ˜è®­ç»ƒå¯èƒ½æœ‰é—®é¢˜")
    print(f"   - å»ºè®®æå‡å¹…åº¦: > 20% ä¸ºè‰¯å¥½ï¼Œ> 50% ä¸ºä¼˜ç§€")
    print(f"{'='*70}\n")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½é˜¶æ®µé…ç½®
    if args.stage == '1.1':
        stage_config = get_training_config('stage1_1_prey_warmup')
    elif args.stage == '1.2':
        stage_config = get_training_config('stage1_2_pred_guided')
    else:
        raise ValueError(f"æœªçŸ¥çš„é˜¶æ®µ: {args.stage}")
    
    # è¯„ä¼°
    eval_results, scenario = evaluate_random_baseline(args.stage, args, stage_config)
    
    # ä¿å­˜ç»“æœ
    save_baseline_results(args.stage, scenario, eval_results, args.output_dir)
    
    # æ‰“å°æ‘˜è¦
    print_baseline_summary(args.stage, scenario, eval_results)


if __name__ == "__main__":
    main()

================================================================================
FILE: scripts/evaluation/eval_single_matchup.py
================================================================================
"""
å•æ¬¡å¯¹æˆ˜è¯„ä¼°
è¯„ä¼°ä¸€å¯¹ Predator vs Prey çš„æ€§èƒ½
"""

import sys
from pathlib import Path
import numpy as np
from typing import Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.environment import WaterworldEnvManager, create_training_env
from src.core.opponent_pool import create_opponent_policies
from src.core.agent_manager import AgentManager
from src.utils.config_loader import get_env_config


def evaluate_single_matchup(
    predator_model_path: Path,
    prey_model_path: Path,
    predator_algo: str,
    prey_algo: str,
    env_config_name: str = "waterworld_fast",
    n_episodes: int = 20,
    deterministic: bool = True,
    verbose: int = 1
) -> Dict[str, Any]:
    """
    è¯„ä¼°å•æ¬¡Predator vs Preyå¯¹æˆ˜
    
    Args:
        predator_model_path: Predatoræ¨¡å‹è·¯å¾„
        prey_model_path: Preyæ¨¡å‹è·¯å¾„
        predator_algo: Predatorç®—æ³•åç§°
        prey_algo: Preyç®—æ³•åç§°
        env_config_name: ç¯å¢ƒé…ç½®åç§°
        n_episodes: è¯„ä¼°episodeæ•°
        deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        verbose: è¯¦ç»†ç¨‹åº¦
    
    Returns:
        è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    
    if verbose > 0:
        print(f"\n{'='*70}")
        print(f"Evaluating: {predator_algo}_pred vs {prey_algo}_prey")
        print(f"{'='*70}")
        # âœ… ä¿®å¤ï¼šå¤„ç† None çš„æƒ…å†µ
        pred_model_name = predator_model_path.name if predator_model_path else "RANDOM (no model)"
        prey_model_name = prey_model_path.name if prey_model_path else "RANDOM (no model)"
        print(f"  Predator Model: {pred_model_name}")
        print(f"  Prey Model:     {prey_model_name}")
        print(f"  Episodes:       {n_episodes}")
    
    # 1. åŠ è½½ç¯å¢ƒé…ç½®
    env_config = get_env_config(env_config_name)
    env_manager = WaterworldEnvManager(env_config)
    env_manager.create_env()
    
    # 2. è·å–ç©ºé—´ä¿¡æ¯
    pred_obs_space = env_manager.get_observation_space('predator')
    pred_action_space = env_manager.get_action_space('predator')
    
    prey_obs_space = env_manager.get_observation_space('prey')
    prey_action_space = env_manager.get_action_space('prey')
    
    # 3. åŠ è½½æ¨¡å‹
    try:
        if predator_algo == 'RANDOM':
            predator_model = AgentManager.create_random_agent(
                pred_obs_space, pred_action_space
            )
        else:
            predator_model = AgentManager.load_agent(
                predator_model_path, pred_obs_space, pred_action_space
            )
        
        if prey_algo == 'RANDOM':
            prey_model = AgentManager.create_random_agent(
                prey_obs_space, prey_action_space
            )
        else:
            prey_model = AgentManager.load_agent(
                prey_model_path, prey_obs_space, prey_action_space
            )
    
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None
    
    # 4. åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆå•ç¯å¢ƒï¼Œä¾¿äºç²¾ç¡®æ§åˆ¶ï¼‰
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªç‰¹æ®Šçš„è¯„ä¼°ç¯å¢ƒï¼ŒåŒæ–¹éƒ½ç”¨å›ºå®šç­–ç•¥
    
    # 5. è¿è¡Œè¯„ä¼°
    episode_results = []
    
    for ep in range(n_episodes):
        obs, info = env_manager.reset(seed=42 + ep)
        done = False
        step = 0
        
        ep_data = {
            'predator_rewards': [],
            'prey_rewards': [],
            'predator_dones': [],
            'prey_dones': [],
            'episode_length': 0
        }
        
        while not done and step < 500:  # æœ€å¤š500æ­¥
            # è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„åŠ¨ä½œ
            actions = {}
            
            for agent_id in env_manager.env.agents:
                if 'predator' in agent_id:
                    obs_agent = obs.get(agent_id, np.zeros(pred_obs_space.shape))
                    action, _ = predator_model.predict(obs_agent, deterministic=deterministic)
                    actions[agent_id] = action
                
                elif 'prey' in agent_id:
                    obs_agent = obs.get(agent_id, np.zeros(prey_obs_space.shape))
                    action, _ = prey_model.predict(obs_agent, deterministic=deterministic)
                    actions[agent_id] = action
            
            # æ‰§è¡Œç¯å¢ƒæ­¥è¿›
            obs, rewards, terminations, truncations, infos = env_manager.env.step(actions)
            
            # è®°å½•æ•°æ®
            pred_rewards = [rewards.get(a, 0) for a in env_manager.env.possible_agents if 'predator' in a]
            prey_rewards = [rewards.get(a, 0) for a in env_manager.env.possible_agents if 'prey' in a]
            
            ep_data['predator_rewards'].extend(pred_rewards)
            ep_data['prey_rewards'].extend(prey_rewards)
            
            # è®°å½•æ­»äº¡äº‹ä»¶
            pred_dones = [terminations.get(a, False) for a in env_manager.env.possible_agents if 'predator' in a]
            prey_dones = [terminations.get(a, False) for a in env_manager.env.possible_agents if 'prey' in a]
            
            ep_data['predator_dones'].extend(pred_dones)
            ep_data['prey_dones'].extend(prey_dones)
            
            step += 1
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ™ºèƒ½ä½“éƒ½ç»“æŸ
            if len(env_manager.env.agents) == 0:
                done = True
        
        ep_data['episode_length'] = step
        episode_results.append(ep_data)
        
        if verbose > 1:
            print(f"  Episode {ep+1}/{n_episodes}: "
                  f"Length={step}, "
                  f"PredReward={np.mean(ep_data['predator_rewards']):.2f}, "
                  f"PreyReward={np.mean(ep_data['prey_rewards']):.2f}")
    
    # 6. è®¡ç®—èšåˆæŒ‡æ ‡
    from metrics_calculator import MetricsCalculator
    
    episode_metrics = [
        MetricsCalculator.compute_episode_metrics(ep_data)
        for ep_data in episode_results
    ]
    
    aggregated_metrics = MetricsCalculator.aggregate_metrics(episode_metrics)
    
    # 7. æ·»åŠ å…ƒæ•°æ®
    aggregated_metrics['predator_algo'] = predator_algo
    aggregated_metrics['prey_algo'] = prey_algo
    aggregated_metrics['is_ood'] = (predator_algo != prey_algo) and (prey_algo != 'RANDOM')
    
    # 8. æ¸…ç†
    env_manager.close()
    
    if verbose > 0:
        print(f"\n  Results:")
        print(f"    Catch Rate:     {aggregated_metrics['catch_rate']:.3f}")
        print(f"    Survival Rate:  {aggregated_metrics['survival_rate']:.3f}")
        print(f"    Pred Reward:    {aggregated_metrics['pred_avg_reward']:+.2f}")
        print(f"    Prey Reward:    {aggregated_metrics['prey_avg_reward']:+.2f}")
        print(f"    Balance Score:  {aggregated_metrics['balance_score']:.3f}")
        print(f"    Is OOD:         {aggregated_metrics['is_ood']}")
        print(f"{'='*70}\n")
    
    return aggregated_metrics


if __name__ == "__main__":
    # æµ‹è¯•å•æ¬¡è¯„ä¼°
    import argparse
    
    parser = argparse.ArgumentParser(description='å•æ¬¡å¯¹æˆ˜è¯„ä¼°')
    parser.add_argument('--pred-model', type=str, required=True, help='Predatoræ¨¡å‹è·¯å¾„')
    parser.add_argument('--prey-model', type=str, required=True, help='Preyæ¨¡å‹è·¯å¾„')
    parser.add_argument('--pred-algo', type=str, required=True, help='Predatorç®—æ³•')
    parser.add_argument('--prey-algo', type=str, required=True, help='Preyç®—æ³•')
    parser.add_argument('--n-episodes', type=int, default=20, help='è¯„ä¼°episodeæ•°')
    
    args = parser.parse_args()
    
    result = evaluate_single_matchup(
        predator_model_path=Path(args.pred_model),
        prey_model_path=Path(args.prey_model),
        predator_algo=args.pred_algo,
        prey_algo=args.prey_algo,
        n_episodes=args.n_episodes
    )
    
    print("\næœ€ç»ˆç»“æœ:")
    for key, value in result.items():
        print(f"  {key}: {value}")

================================================================================
FILE: scripts/evaluation/metrics_calculator.py
================================================================================
"""
æŒ‡æ ‡è®¡ç®—å™¨
è®¡ç®—äº¤å‰è¯„ä¼°ä¸­çš„å„ç§æ€§èƒ½æŒ‡æ ‡
"""

import numpy as np
from typing import Dict, List, Any


class MetricsCalculator:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def compute_episode_metrics(episode_data: Dict) -> Dict[str, float]:
        """
        è®¡ç®—å•ä¸ªepisodeçš„æŒ‡æ ‡
        
        Args:
            episode_data: {
                'predator_rewards': [...],
                'prey_rewards': [...],
                'predator_dones': [...],
                'prey_dones': [...],
                'episode_length': int
            }
        
        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        metrics = {}
        
        # PredatoræŒ‡æ ‡
        pred_rewards = np.array(episode_data['predator_rewards'])
        pred_dones = np.array(episode_data['predator_dones'])
        
        metrics['pred_total_reward'] = float(np.sum(pred_rewards))
        metrics['pred_avg_reward'] = float(np.mean(pred_rewards))
        
        # æ•è·äº‹ä»¶ï¼ˆå‡è®¾preyæ­»äº¡æ—¶predatorè·å¾—æ­£å¥–åŠ±ï¼‰
        prey_dones = np.array(episode_data['prey_dones'])
        metrics['n_catches'] = int(np.sum(prey_dones))
        
        # PreyæŒ‡æ ‡
        prey_rewards = np.array(episode_data['prey_rewards'])
        metrics['prey_total_reward'] = float(np.sum(prey_rewards))
        metrics['prey_avg_reward'] = float(np.mean(prey_rewards))
        
        # Episodeçº§æŒ‡æ ‡
        metrics['episode_length'] = episode_data['episode_length']
        metrics['reward_gap'] = metrics['pred_total_reward'] - metrics['prey_total_reward']
        
        return metrics
    
    @staticmethod
    def aggregate_metrics(episode_metrics_list: List[Dict]) -> Dict[str, Any]:
        """
        èšåˆå¤šä¸ªepisodeçš„æŒ‡æ ‡
        
        Args:
            episode_metrics_list: å¤šä¸ªepisodeçš„æŒ‡æ ‡åˆ—è¡¨
        
        Returns:
            èšåˆåçš„æŒ‡æ ‡
        """
        n_episodes = len(episode_metrics_list)
        
        # æå–å„æŒ‡æ ‡çš„æ•°ç»„
        pred_rewards = [m['pred_avg_reward'] for m in episode_metrics_list]
        prey_rewards = [m['prey_avg_reward'] for m in episode_metrics_list]
        n_catches = [m['n_catches'] for m in episode_metrics_list]
        episode_lengths = [m['episode_length'] for m in episode_metrics_list]
        
        # èšåˆç»Ÿè®¡
        aggregated = {
            'n_episodes': n_episodes,
            
            # PredatoræŒ‡æ ‡
            'pred_avg_reward': float(np.mean(pred_rewards)),
            'pred_std_reward': float(np.std(pred_rewards)),
            'pred_min_reward': float(np.min(pred_rewards)),
            'pred_max_reward': float(np.max(pred_rewards)),
            
            # PreyæŒ‡æ ‡
            'prey_avg_reward': float(np.mean(prey_rewards)),
            'prey_std_reward': float(np.std(prey_rewards)),
            'prey_min_reward': float(np.min(prey_rewards)),
            'prey_max_reward': float(np.max(prey_rewards)),
            
            # å¯¹æˆ˜æŒ‡æ ‡
            'catch_rate': float(np.mean(n_catches)) / 10.0,  # å‡è®¾10ä¸ªprey
            'avg_catches': float(np.mean(n_catches)),
            'std_catches': float(np.std(n_catches)),
            
            'survival_rate': 1.0 - (float(np.mean(n_catches)) / 10.0),
            
            'avg_episode_length': float(np.mean(episode_lengths)),
            'std_episode_length': float(np.std(episode_lengths)),
            
            # å¹³è¡¡åº¦æŒ‡æ ‡
            'reward_gap': float(np.mean([m['reward_gap'] for m in episode_metrics_list])),
            'balance_score': MetricsCalculator._compute_balance_score(n_catches)
        }
        
        # é¢å¤–æŒ‡æ ‡
        aggregated['energy_efficiency'] = (
            aggregated['pred_avg_reward'] / (aggregated['avg_episode_length'] + 1e-6)
        )
        
        aggregated['escape_success'] = aggregated['survival_rate']
        
        # é¦–æ¬¡æ•è·æ—¶é—´ï¼ˆç®€åŒ–ç‰ˆï¼Œå‡è®¾ç¬¬ä¸€æ¬¡æ•è·å‘ç”Ÿåœ¨episodeä¸­é—´ï¼‰
        aggregated['first_catch_time'] = aggregated['avg_episode_length'] / 2.0
        
        # Preyå¹³å‡å¯¿å‘½
        aggregated['prey_avg_lifespan'] = aggregated['avg_episode_length'] * aggregated['survival_rate']
        
        return aggregated
    
    @staticmethod
    def _compute_balance_score(n_catches_list: List[int]) -> float:
        """
        è®¡ç®—å¹³è¡¡åº¦åˆ†æ•°
        
        0.5 = å®Œç¾å¹³è¡¡ï¼ˆcatch_rate = 50%ï¼‰
        0.0 = æåº¦ä¸å¹³è¡¡
        
        Args:
            n_catches_list: æ¯ä¸ªepisodeçš„æ•è·æ•°
        
        Returns:
            å¹³è¡¡åº¦åˆ†æ•° [0, 1]
        """
        avg_catches = np.mean(n_catches_list)
        catch_rate = avg_catches / 10.0  # å‡è®¾10ä¸ªprey
        
        # è·ç¦»0.5è¶Šè¿‘ï¼Œå¹³è¡¡åº¦è¶Šé«˜
        deviation = abs(catch_rate - 0.5)
        balance_score = 1.0 - (deviation * 2.0)  # å½’ä¸€åŒ–åˆ°[0, 1]
        
        return float(max(0.0, balance_score))
    
    @staticmethod
    def compute_adaptability_scores(results_matrix: Dict) -> Dict[str, Dict]:
        """
        è®¡ç®—æ‰€æœ‰ç®—æ³•çš„è‡ªé€‚åº”æ€§å¾—åˆ†
        
        Args:
            results_matrix: {
                'PPO': {'PPO': {...}, 'A2C': {...}, ...},
                'A2C': {...},
                ...
            }
        
        Returns:
            è‡ªé€‚åº”æ€§å¾—åˆ†å­—å…¸
        """
        adaptability_scores = {}
        
        for pred_algo in results_matrix.keys():
            if pred_algo == 'RANDOM':
                continue
            
            # In-Distributionæ€§èƒ½ï¼ˆå¯¹è§’çº¿ï¼‰
            in_dist_perf = results_matrix[pred_algo][pred_algo]['catch_rate']
            
            # Out-of-Distributionæ€§èƒ½ï¼ˆéå¯¹è§’çº¿ï¼Œæ’é™¤RANDOMï¼‰
            ood_perfs = []
            for prey_algo in results_matrix[pred_algo].keys():
                if prey_algo != pred_algo and prey_algo != 'RANDOM':
                    ood_perfs.append(results_matrix[pred_algo][prey_algo]['catch_rate'])
            
            ood_avg = float(np.mean(ood_perfs))
            ood_std = float(np.std(ood_perfs))
            
            # è‡ªé€‚åº”æ€§å¾—åˆ† = OODä¿æŒç‡
            adaptability = ood_avg / (in_dist_perf + 1e-6)
            
            adaptability_scores[pred_algo] = {
                'algorithm': pred_algo,
                'in_dist_performance': float(in_dist_perf),
                'ood_avg_performance': ood_avg,
                'ood_std': ood_std,
                'adaptability_score': float(adaptability),
                'performance_drop': float(in_dist_perf - ood_avg),
                'ood_performances': ood_perfs  # åŸå§‹æ•°æ®
            }
        
        return adaptability_scores
    
    @staticmethod
    def compute_ranking(adaptability_scores: Dict) -> List[Dict]:
        """
        æ ¹æ®è‡ªé€‚åº”æ€§å¾—åˆ†æ’å
        
        Returns:
            æ’åºåçš„åˆ—è¡¨
        """
        scores_list = list(adaptability_scores.values())
        scores_list.sort(key=lambda x: x['adaptability_score'], reverse=True)
        
        # æ·»åŠ æ’å
        for i, scores in enumerate(scores_list):
            scores['rank'] = i + 1
        
        return scores_list

================================================================================
FILE: scripts/evaluation/run_cross_eval.py
================================================================================


================================================================================
FILE: src/__init__.py
================================================================================


================================================================================
FILE: src/core/__init__.py
================================================================================
"""
æ ¸å¿ƒæ¨¡å—åˆå§‹åŒ–
"""

from .environment import WaterworldEnvManager, SingleAgentWrapper, create_training_env
from .opponent_pool import OpponentPool, MixedOpponentSampler, create_opponent_policies
from .trainer import MultiAgentTrainer
from .agent_manager import AgentManager


__all__ = [
    'WaterworldEnvManager',
    'SingleAgentWrapper',
    'create_training_env',
    'OpponentPool',
    'MixedOpponentSampler',
    'create_opponent_policies',
    'MultiAgentTrainer',
    'AgentManager'
]

================================================================================
FILE: src/core/agent_manager.py
================================================================================
"""
æ™ºèƒ½ä½“ç®¡ç†
åŠ è½½å’Œç®¡ç†è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
"""

from pathlib import Path
from typing import Dict, Any, Optional
import gymnasium as gym

from src.algorithms import create_algorithm
from src.utils.config_loader import get_algo_config


class AgentManager:
    """æ™ºèƒ½ä½“ç®¡ç†å™¨"""
    
    @staticmethod
    def load_agent(
        model_path: Path,
        observation_space: gym.Space,
        action_space: gym.Space,
        device: str = "auto"
    ):
        """
        åŠ è½½è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            observation_space: è§‚å¯Ÿç©ºé—´
            action_space: åŠ¨ä½œç©ºé—´
            device: è®¡ç®—è®¾å¤‡
        
        Returns:
            åŠ è½½çš„ç®—æ³•å®ä¾‹
        """
        # ä»æ–‡ä»¶åè§£æç®—æ³•åç§°
        filename = model_path.stem
        parts = filename.split('_')
        
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€ï¼ˆDEBUG_, DRYRUN_ï¼‰
        if parts[0] in ['DEBUG', 'DRYRUN']:
            parts = parts[1:]
        
        algo_name = parts[0]
        
        # åŠ è½½ç®—æ³•é…ç½®
        algo_config = get_algo_config(algo_name)
        
        # åˆ›å»ºç®—æ³•å®ä¾‹
        algorithm = create_algorithm(
            algo_name=algo_name,
            observation_space=observation_space,
            action_space=action_space,
            config=algo_config,
            device=device
        )
        
        # åŠ è½½æ¨¡å‹æƒé‡
        algorithm.load(str(model_path))
        
        return algorithm
    
    @staticmethod
    def create_random_agent(
        observation_space: gym.Space,
        action_space: gym.Space
    ):
        """
        åˆ›å»ºéšæœºæ™ºèƒ½ä½“
        
        Args:
            observation_space: è§‚å¯Ÿç©ºé—´
            action_space: åŠ¨ä½œç©ºé—´
        
        Returns:
            éšæœºç­–ç•¥å®ä¾‹
        """
        algo_config = get_algo_config('RANDOM')
        
        algorithm = create_algorithm(
            algo_name='RANDOM',
            observation_space=observation_space,
            action_space=action_space,
            config=algo_config,
            device='cpu'
        )
        
        algorithm.create_model(None)
        
        return algorithm

================================================================================
FILE: src/core/environment.py
================================================================================
"""
ç¯å¢ƒåˆ›å»ºä¸ç®¡ç† - ä½¿ç”¨ SuperSuit æ–¹æ¡ˆ
"""

from typing import Dict, Any, Optional, List
from pettingzoo.sisl import waterworld_v4
import gymnasium as gym
import numpy as np
import supersuit as ss
from stable_baselines3.common.vec_env import VecEnv


class WaterworldEnvManager:
    """Waterworldç¯å¢ƒç®¡ç†å™¨"""
    
    def __init__(self, env_config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ç¯å¢ƒç®¡ç†å™¨
        
        Args:
            env_config: ç¯å¢ƒé…ç½®å­—å…¸
        """
        self.config = env_config['environment']
        self.env = None
    
    def create_env(self, render_mode: Optional[str] = None):
        """
        åˆ›å»ºWaterworldç¯å¢ƒ
        
        Args:
            render_mode: æ¸²æŸ“æ¨¡å¼ï¼ˆNone/"rgb_array"/"human"ï¼‰
        
        Returns:
            PettingZooç¯å¢ƒå®ä¾‹
        """
        if render_mode is None:
            render_mode = self.config.get('render_mode', None)
        
        self.env = waterworld_v4.parallel_env(
            n_predators=self.config.get('n_predators', 5),
            n_preys=self.config.get('n_preys', 10),
            n_evaders=self.config.get('n_evaders', 90),
            n_poisons=self.config.get('n_poisons', 10),
            n_obstacles=self.config.get('n_obstacles', 2),
            obstacle_coord=self.config.get('obstacle_coord', [[0.2, 0.2], [0.8, 0.2]]),
            
            predator_speed=self.config.get('predator_speed', 0.06),
            prey_speed=self.config.get('prey_speed', 0.001),
            
            sensor_range=self.config.get('sensor_range', 0.8),
            thrust_penalty=self.config.get('thrust_penalty', 0.0),
            
            max_cycles=self.config.get('max_cycles', 3000),
            
            render_mode=render_mode
        )
        
        return self.env
    
    def get_observation_space(self, agent_type: str) -> gym.Space:
        """
        è·å–è§‚å¯Ÿç©ºé—´
        
        Args:
            agent_type: æ™ºèƒ½ä½“ç±»å‹ï¼ˆpredator/preyï¼‰
        
        Returns:
            è§‚å¯Ÿç©ºé—´
        """
        if self.env is None:
            self.create_env()
        
        for agent in self.env.possible_agents:
            if agent_type in agent:
                return self.env.observation_space(agent)
        
        raise ValueError(f"æœªæ‰¾åˆ°ç±»å‹ä¸º {agent_type} çš„æ™ºèƒ½ä½“")
    
    def get_action_space(self, agent_type: str) -> gym.Space:
        """
        è·å–åŠ¨ä½œç©ºé—´
        
        Args:
            agent_type: æ™ºèƒ½ä½“ç±»å‹ï¼ˆpredator/preyï¼‰
        
        Returns:
            åŠ¨ä½œç©ºé—´
        """
        if self.env is None:
            self.create_env()
        
        for agent in self.env.possible_agents:
            if agent_type in agent:
                return self.env.action_space(agent)
        
        raise ValueError(f"æœªæ‰¾åˆ°ç±»å‹ä¸º {agent_type} çš„æ™ºèƒ½ä½“")
    
    def get_agents_by_type(self, agent_type: str) -> List[str]:
        """
        è·å–æŒ‡å®šç±»å‹çš„æ‰€æœ‰æ™ºèƒ½ä½“ID
        
        Args:
            agent_type: æ™ºèƒ½ä½“ç±»å‹ï¼ˆpredator/preyï¼‰
        
        Returns:
            æ™ºèƒ½ä½“IDåˆ—è¡¨
        """
        if self.env is None:
            self.create_env()
        
        return [agent for agent in self.env.possible_agents if agent_type in agent]
    
    def reset(self, seed: Optional[int] = None):
        """é‡ç½®ç¯å¢ƒ"""
        if self.env is None:
            self.create_env()
        
        return self.env.reset(seed=seed)
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        if self.env is not None:
            self.env.close()
            self.env = None


class MixedAgentVecEnv(VecEnv):
    """
    æ··åˆæ™ºèƒ½ä½“å‘é‡åŒ–ç¯å¢ƒ
    åŸºäºå‚è€ƒä»£ç çš„æˆåŠŸå®ç°
    """
    
    def __init__(self, venv, train_agent_indices: List[int], opponent_policies: Dict[str, Any]):
        """
        åˆå§‹åŒ–
        
        Args:
            venv: SuperSuitè½¬æ¢åçš„å‘é‡åŒ–ç¯å¢ƒ
            train_agent_indices: è®­ç»ƒæ™ºèƒ½ä½“çš„ç´¢å¼•åˆ—è¡¨
            opponent_policies: å¯¹æ‰‹ç­–ç•¥å­—å…¸ {agent_name: policy}
        """
        self.venv = venv
        self.train_indices = train_agent_indices
        self.opponent_policies = opponent_policies
        
        self.n_training = len(train_agent_indices)
        self.n_total = venv.num_envs
        
        # è·å–è®­ç»ƒæ™ºèƒ½ä½“çš„ç©ºé—´
        super().__init__(
            num_envs=self.n_training,
            observation_space=venv.observation_space,
            action_space=venv.action_space
        )
        
        self.latest_obs = None
        
        print(f"  MixedAgentVecEnv: training {self.n_training}/{self.n_total} agents")
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        obs = self.venv.reset()
        self.latest_obs = obs
        
        # é‡ç½®å¯¹æ‰‹ç­–ç•¥
        for policy in self.opponent_policies.values():
            if hasattr(policy, 'reset'):
                policy.reset()
        
        # è¿”å›è®­ç»ƒæ™ºèƒ½ä½“çš„è§‚å¯Ÿ
        return obs[self.train_indices]
    
    def step_async(self, actions):
        """å¼‚æ­¥æ­¥è¿›"""
        # æ„å»ºå®Œæ•´åŠ¨ä½œæ•°ç»„
        full_actions = np.zeros((self.n_total, self.action_space.shape[0]), dtype=np.float32)
        
        # å¡«å……è®­ç»ƒæ™ºèƒ½ä½“çš„åŠ¨ä½œ
        for i, train_idx in enumerate(self.train_indices):
            full_actions[train_idx] = actions[i]
        
        # å¡«å……å¯¹æ‰‹æ™ºèƒ½ä½“çš„åŠ¨ä½œ
        agent_names = list(self.opponent_policies.keys())
        for i, agent_name in enumerate(agent_names):
            if i not in self.train_indices:
                policy = self.opponent_policies[agent_name]
                obs = self.latest_obs[i] if self.latest_obs is not None else None
                if obs is not None:
                    full_actions[i] = policy.predict(obs, deterministic=False)[0]
                else:
                    full_actions[i] = self.action_space.sample()
        
        self.venv.step_async(full_actions)
    
    def step_wait(self):
        """ç­‰å¾…æ­¥è¿›ç»“æœ"""
        obs, rewards, dones, infos = self.venv.step_wait()
        self.latest_obs = obs
        
        # æå–è®­ç»ƒæ™ºèƒ½ä½“çš„æ•°æ®
        train_obs = obs[self.train_indices]
        train_rewards = rewards[self.train_indices]
        train_dones = dones[self.train_indices]
        train_infos = [infos[i] for i in self.train_indices]
        
        return train_obs, train_rewards, train_dones, train_infos
    
    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        return self.venv.close()
    
    def get_attr(self, attr_name, indices=None):
        return self.venv.get_attr(attr_name, indices)
    
    def set_attr(self, attr_name, value, indices=None):
        return self.venv.set_attr(attr_name, value, indices)
    
    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):
        return self.venv.env_method(method_name, *method_args, indices=indices, **method_kwargs)
    
    def env_is_wrapped(self, wrapper_class, indices=None):
        """æ£€æŸ¥ç¯å¢ƒæ˜¯å¦è¢«åŒ…è£…"""
        return self.venv.env_is_wrapped(wrapper_class, indices)


# ç®€åŒ–å ä½ç¬¦
class SingleAgentWrapper:
    pass


def create_training_env(
    env_config: Dict[str, Any],
    train_side: str,
    opponent_policies: Dict[str, Any],
    n_envs: int = 1
):
    """
    åˆ›å»ºè®­ç»ƒç¯å¢ƒ - ä½¿ç”¨ SuperSuit æ–¹æ¡ˆ
    
    Args:
        env_config: ç¯å¢ƒé…ç½®
        train_side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
        opponent_policies: å¯¹æ‰‹ç­–ç•¥å­—å…¸
        n_envs: å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆç›®å‰ä»…æ”¯æŒ1ï¼‰
    
    Returns:
        è®­ç»ƒç¯å¢ƒ
    """
    # 1. åˆ›å»ºåŸºç¡€ç¯å¢ƒ
    env_manager = WaterworldEnvManager(env_config)
    pz_env = env_manager.create_env()
    
    # 2. ä½¿ç”¨ SuperSuit è½¬æ¢ï¼ˆå‚è€ƒæˆåŠŸä»£ç ï¼‰
    # black_death: æ­»äº¡çš„æ™ºèƒ½ä½“è¿”å›é›¶è§‚å¯Ÿå’Œé›¶å¥–åŠ±
    env = ss.black_death_v3(pz_env)
    
    # è½¬æ¢ä¸ºå‘é‡åŒ–ç¯å¢ƒ
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    
    # æ‹¼æ¥ç¯å¢ƒï¼ˆè¿™é‡Œ n_envs æ€»æ˜¯1ï¼‰
    env = ss.concat_vec_envs_v1(
        env, 
        num_vec_envs=1, 
        num_cpus=1, 
        base_class='stable_baselines3'
    )
    
    # 3. ç¡®å®šè®­ç»ƒæ™ºèƒ½ä½“
    all_agents = pz_env.possible_agents
    train_agents = [agent for agent in all_agents if train_side in agent]
    
    if not train_agents:
        raise ValueError(f"æ²¡æœ‰æ‰¾åˆ° {train_side} ç±»å‹çš„æ™ºèƒ½ä½“")
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªä½œä¸ºè®­ç»ƒæ™ºèƒ½ä½“
    train_agent_name = train_agents[0]
    train_agent_idx = all_agents.index(train_agent_name)
    
    print(f"  è®­ç»ƒæ™ºèƒ½ä½“: {train_agent_name} (index={train_agent_idx})")
    
    # 4. åº”ç”¨æ··åˆæ™ºèƒ½ä½“åŒ…è£…å™¨
    env = MixedAgentVecEnv(
        venv=env,
        train_agent_indices=[train_agent_idx],
        opponent_policies=opponent_policies
    )
    
    return env


================================================================================
FILE: src/core/opponent_pool.py
================================================================================
"""
å¯¹æ‰‹æ± ç®¡ç†
è´Ÿè´£å›ºå®šå¯¹æ‰‹çš„åŠ è½½ã€é‡‡æ ·å’Œç»´æŠ¤
"""

import os
import json
import random
from pathlib import Path
from typing import Dict, Any, List, Optional
import numpy as np
import gymnasium as gym  # â† æ·»åŠ è¿™ä¸€è¡Œï¼
from src.algorithms import create_algorithm
from src.utils.config_loader import get_algo_config


class OpponentPool:
    """å¯¹æ‰‹æ± ç®¡ç†å™¨"""
    
    def __init__(self, pool_dir: Optional[Path] = None):
        """
        åˆå§‹åŒ–å¯¹æ‰‹æ± 
        
        Args:
            pool_dir: æ± ç›®å½•è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneåˆ™åˆ›å»ºç©ºæ± ï¼‰
        """
        self.pool_dir = pool_dir
        self.opponents = []  # å¯¹æ‰‹åˆ—è¡¨ï¼š[{name, path, policy, metadata}, ...]
        self.metadata = {}
        
        if pool_dir and pool_dir.exists():
            self.load_pool()
    
    def load_pool(self):
        """ä»ç›®å½•åŠ è½½å¯¹æ‰‹æ± """
        if not self.pool_dir.exists():
            print(f"âš ï¸  å¯¹æ‰‹æ± ç›®å½•ä¸å­˜åœ¨: {self.pool_dir}")
            return
        
        # åŠ è½½å…ƒæ•°æ®
        metadata_path = self.pool_dir / "metadata.json"
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹
        for model_file in self.pool_dir.glob("*.zip"):
            opponent_info = self._load_opponent(model_file)
            if opponent_info:
                self.opponents.append(opponent_info)
        
        print(f"âœ… ä» {self.pool_dir} åŠ è½½äº† {len(self.opponents)} ä¸ªå¯¹æ‰‹")
    
    def _load_opponent(self, model_path: Path) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½å•ä¸ªå¯¹æ‰‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        
        Returns:
            å¯¹æ‰‹ä¿¡æ¯å­—å…¸ï¼ŒåŠ è½½å¤±è´¥è¿”å›None
        """
        try:
            # ä»æ–‡ä»¶åè§£æä¿¡æ¯
            filename = model_path.stem  # å»æ‰.zip
            
            # å‡è®¾æ–‡ä»¶åæ ¼å¼: ALGO_side_version
            parts = filename.split('_')
            if len(parts) < 3:
                print(f"âš ï¸  æ— æ³•è§£ææ–‡ä»¶å: {filename}")
                return None
            
            algo_name = parts[0]
            side = parts[1]
            version = parts[2]
            
            # ä»metadataä¸­è·å–è¯¦ç»†ä¿¡æ¯
            model_metadata = {}
            for model_info in self.metadata.get('models', []):
                if model_info.get('name') == filename:
                    model_metadata = model_info
                    break
            
            # åŠ è½½ç®—æ³•é…ç½®
            try:
                algo_config = get_algo_config(algo_name)
            except:
                print(f"âš ï¸  æ— æ³•åŠ è½½ç®—æ³•é…ç½®: {algo_name}")
                return None
            
            # åˆ›å»ºç®—æ³•å®ä¾‹ï¼ˆæš‚ä¸åŠ è½½æ¨¡å‹ï¼Œå»¶è¿ŸåŠ è½½ï¼‰
            opponent_info = {
                'name': filename,
                'algo': algo_name,
                'side': side,
                'version': version,
                'path': model_path,
                'policy': None,  # å»¶è¿ŸåŠ è½½
                'config': algo_config,
                'metadata': model_metadata
            }
            
            return opponent_info
        
        except Exception as e:
            print(f"âŒ åŠ è½½å¯¹æ‰‹å¤±è´¥ {model_path}: {e}")
            return None
    
    def get_opponent_policy(
        self, 
        opponent_info: Dict[str, Any], 
        device: str = "auto",
        observation_space: gym.Space = None,
        action_space: gym.Space = None
    ):
        """
        è·å–å¯¹æ‰‹ç­–ç•¥ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        
        Args:
            opponent_info: å¯¹æ‰‹ä¿¡æ¯
            device: è®¡ç®—è®¾å¤‡
        
        Returns:
            åŠ è½½çš„ç­–ç•¥
        """
        # å¦‚æœå·²ç»åŠ è½½è¿‡ï¼Œç›´æ¥è¿”å›
        if opponent_info['policy'] is not None:
            return opponent_info['policy']
        
        # âœ… åˆ›å»ºç®—æ³•å®ä¾‹ï¼ˆä½¿ç”¨ä¼ å…¥çš„ç©ºé—´ä¿¡æ¯ï¼‰
        algo = create_algorithm(
            algo_name=opponent_info['algo'],
            observation_space=observation_space,  # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
            action_space=action_space,             # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
            config=opponent_info['config'],
            device=device
        )
        
        # åŠ è½½æ¨¡å‹æƒé‡
        algo.load(str(opponent_info['path']))
        
        # ç¼“å­˜
        opponent_info['policy'] = algo
        
        return algo
    
    def sample_opponents(
        self,
        n: int,
        strategy: str = "uniform",
        exclude: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        """
        ä»æ± ä¸­é‡‡æ ·å¯¹æ‰‹
        
        Args:
            n: é‡‡æ ·æ•°é‡
            strategy: é‡‡æ ·ç­–ç•¥ï¼ˆuniform/weightedï¼‰
            exclude: è¦æ’é™¤çš„å¯¹æ‰‹åç§°åˆ—è¡¨
        
        Returns:
            å¯¹æ‰‹ä¿¡æ¯åˆ—è¡¨
        """
        if not self.opponents:
            print("âš ï¸  å¯¹æ‰‹æ± ä¸ºç©ºï¼Œæ— æ³•é‡‡æ ·")
            return []
        
        # è¿‡æ»¤è¦æ’é™¤çš„å¯¹æ‰‹
        available = self.opponents
        if exclude:
            available = [opp for opp in self.opponents if opp['name'] not in exclude]
        
        if not available:
            print("âš ï¸  æ²¡æœ‰å¯ç”¨çš„å¯¹æ‰‹")
            return []
        
        # é‡‡æ ·
        if strategy == "uniform":
            # å‡åŒ€é‡‡æ ·ï¼ˆå¯é‡å¤ï¼‰
            sampled = random.choices(available, k=min(n, len(available)))
        
        elif strategy == "weighted":
            # æŒ‰æ€§èƒ½åŠ æƒé‡‡æ ·ï¼ˆæ€§èƒ½è¶Šå¥½æƒé‡è¶Šé«˜ï¼‰
            weights = []
            for opp in available:
                # ä»metadataä¸­è·å–æ€§èƒ½æŒ‡æ ‡
                perf = opp['metadata'].get('eval_metrics', {})
                # ä½¿ç”¨å¹³å‡å¥–åŠ±ä½œä¸ºæƒé‡
                weight = perf.get('avg_reward', 1.0) + 5.0  # +5ç¡®ä¿æƒé‡ä¸ºæ­£
                weights.append(max(weight, 0.1))
            
            sampled = random.choices(available, weights=weights, k=min(n, len(available)))
        
        else:
            raise ValueError(f"æœªçŸ¥çš„é‡‡æ ·ç­–ç•¥: {strategy}")
        
        return sampled
    
    def add_opponent(
        self,
        model_path: Path,
        metadata: Dict[str, Any]
    ):
        """
        æ·»åŠ æ–°å¯¹æ‰‹åˆ°æ± ä¸­
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            metadata: å¯¹æ‰‹å…ƒæ•°æ®
        """
        opponent_info = self._load_opponent(model_path)
        if opponent_info:
            opponent_info['metadata'] = metadata
            self.opponents.append(opponent_info)
            
            # æ›´æ–°æ± çš„metadataæ–‡ä»¶
            self._update_metadata()
            
            print(f"âœ… æ·»åŠ å¯¹æ‰‹åˆ°æ± : {opponent_info['name']}")
    
    def _update_metadata(self):
        """æ›´æ–°æ± çš„metadataæ–‡ä»¶"""
        if not self.pool_dir:
            return
        
        self.metadata['models'] = []
        for opp in self.opponents:
            self.metadata['models'].append({
                'name': opp['name'],
                'algo': opp['algo'],
                'side': opp['side'],
                'version': opp['version'],
                'path': str(opp['path']),
                'metadata': opp['metadata']
            })
        
        metadata_path = self.pool_dir / "metadata.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, indent=2, ensure_ascii=False)
    
    def size(self) -> int:
        """è·å–æ± å¤§å°"""
        return len(self.opponents)
    
    def get_all_opponents(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¯¹æ‰‹"""
        return self.opponents.copy()
    
    def clear(self):
        """æ¸…ç©ºå¯¹æ‰‹æ± ï¼ˆä»…æ¸…ç©ºå†…å­˜ï¼Œä¸åˆ é™¤æ–‡ä»¶ï¼‰"""
        self.opponents = []


class MixedOpponentSampler:
    """æ··åˆå¯¹æ‰‹é‡‡æ ·å™¨ï¼ˆå›ºå®šæ±  + éšæœºï¼‰"""
    
    def __init__(
        self,
        fixed_pool: OpponentPool,
        random_policy_creator,
        fixed_ratio: float = 0.7,
        sampling_strategy: str = "uniform"
    ):
        """
        åˆå§‹åŒ–æ··åˆé‡‡æ ·å™¨
        
        Args:
            fixed_pool: å›ºå®šå¯¹æ‰‹æ± 
            random_policy_creator: åˆ›å»ºéšæœºç­–ç•¥çš„å‡½æ•°
            fixed_ratio: å›ºå®šå¯¹æ‰‹å æ¯”
            sampling_strategy: é‡‡æ ·ç­–ç•¥
        """
        self.fixed_pool = fixed_pool
        self.random_policy_creator = random_policy_creator
        self.fixed_ratio = fixed_ratio
        self.sampling_strategy = sampling_strategy
    
    def sample(self, n: int) -> List[Dict[str, Any]]:
        """
        é‡‡æ ·æ··åˆå¯¹æ‰‹
        
        Args:
            n: æ€»å¯¹æ‰‹æ•°é‡
        
        Returns:
            å¯¹æ‰‹åˆ—è¡¨ï¼ˆåŒ…å«å›ºå®šå¯¹æ‰‹å’Œéšæœºå¯¹æ‰‹ï¼‰
        """
        n_fixed = int(n * self.fixed_ratio)
        n_random = n - n_fixed
        
        opponents = []
        
        # ä»å›ºå®šæ± é‡‡æ ·
        if n_fixed > 0 and self.fixed_pool.size() > 0:
            fixed_opponents = self.fixed_pool.sample_opponents(
                n=n_fixed,
                strategy=self.sampling_strategy
            )
            opponents.extend(fixed_opponents)
        
        # æ·»åŠ éšæœºå¯¹æ‰‹
        for i in range(n_random):
            random_opp = {
                'name': f'RANDOM_{i}',
                'algo': 'RANDOM',
                'policy': self.random_policy_creator(),
                'is_random': True
            }
            opponents.append(random_opp)
        
        return opponents


def create_opponent_policies(
    opponent_config: Dict[str, Any],
    env_manager,
    device: str = "auto"
) -> Dict[str, Any]:
    """
    æ ¹æ®é…ç½®åˆ›å»ºå¯¹æ‰‹ç­–ç•¥
    
    Args:
        opponent_config: å¯¹æ‰‹é…ç½®
        env_manager: ç¯å¢ƒç®¡ç†å™¨ï¼ˆç”¨äºè·å–ç©ºé—´ä¿¡æ¯ï¼‰
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        å¯¹æ‰‹ç­–ç•¥å­—å…¸ {agent_id: policy}
    """
    opp_type = opponent_config.get('type', 'algorithm')
    opp_side = opponent_config.get('side', 'predator')
    
    # è·å–å¯¹æ‰‹æ™ºèƒ½ä½“åˆ—è¡¨
    opponent_agents = env_manager.get_agents_by_type(opp_side)
    
    # è·å–ç©ºé—´ä¿¡æ¯
    obs_space = env_manager.get_observation_space(opp_side)
    action_space = env_manager.get_action_space(opp_side)
    
    policies = {}
    
    if opp_type == "algorithm":
        # ä½¿ç”¨æŒ‡å®šç®—æ³•ï¼ˆé€šå¸¸æ˜¯RANDOMï¼‰
        algo_name = opponent_config.get('algorithm', 'RANDOM')
        algo_config = get_algo_config(algo_name)
        
        # ä¸ºæ‰€æœ‰å¯¹æ‰‹åˆ›å»ºç›¸åŒçš„ç­–ç•¥
        policy = create_algorithm(
            algo_name=algo_name,
            observation_space=obs_space,
            action_space=action_space,
            config=algo_config,
            device=device
        )
        
        # å¦‚æœæ˜¯RANDOMï¼Œéœ€è¦åˆ›å»ºæ¨¡å‹
        if algo_name == 'RANDOM':
            policy.create_model(None)
        
        for agent in opponent_agents:
            policies[agent] = policy
    
    elif opp_type == "fixed_model":
        # åŠ è½½å›ºå®šæ¨¡å‹
        model_path = opponent_config.get('path')
        if not model_path or not os.path.exists(model_path):
            raise FileNotFoundError(f"å¯¹æ‰‹æ¨¡å‹ä¸å­˜åœ¨: {model_path}")
        
        # ä»è·¯å¾„è§£æç®—æ³•åç§°
        filename = Path(model_path).stem
        algo_name = filename.split('_')[0]
        algo_config = get_algo_config(algo_name)
        
        policy = create_algorithm(
            algo_name=algo_name,
            observation_space=obs_space,
            action_space=action_space,
            config=algo_config,
            device=device
        )
        policy.load(model_path)
        
        for agent in opponent_agents:
            policies[agent] = policy
    
    elif opp_type == "mixed_pool":
        # ä»æ··åˆæ± é‡‡æ ·
        pool_path = Path(opponent_config.get('pool_path', ''))
        
        if not pool_path.exists():
            print(f"âš ï¸  å¯¹æ‰‹æ± ä¸å­˜åœ¨: {pool_path}ï¼Œä½¿ç”¨RANDOMç­–ç•¥")
            # å›é€€åˆ°RANDOM
            algo_config = get_algo_config('RANDOM')
            policy = create_algorithm(
                algo_name='RANDOM',
                observation_space=obs_space,
                action_space=action_space,
                config=algo_config,
                device=device
            )
            policy.create_model(None)
            
            for agent in opponent_agents:
                policies[agent] = policy
        else:
            # åŠ è½½æ± 
            pool = OpponentPool(pool_path)
            
            # åˆ›å»ºæ··åˆé‡‡æ ·å™¨
            def create_random():
                algo_config = get_algo_config('RANDOM')
                policy = create_algorithm(
                    algo_name='RANDOM',
                    observation_space=obs_space,
                    action_space=action_space,
                    config=algo_config,
                    device=device
                )
                policy.create_model(None)
                return policy
            
            mix_strategy = opponent_config.get('mix_strategy', {})
            sampler = MixedOpponentSampler(
                fixed_pool=pool,
                random_policy_creator=create_random,
                fixed_ratio=mix_strategy.get('fixed_ratio', 0.7),
                sampling_strategy=mix_strategy.get('sampling', 'uniform')
            )
            
            # é‡‡æ ·å¯¹æ‰‹
            n_opponents = len(opponent_agents)
            sampled_opponents = sampler.sample(n_opponents)
            
            # åˆ†é…ç»™æ™ºèƒ½ä½“
            for i, agent in enumerate(opponent_agents):
                opp = sampled_opponents[i % len(sampled_opponents)]
                
                if opp.get('is_random', False):
                    # éšæœºå¯¹æ‰‹
                    policies[agent] = opp['policy']
                else:
                    # âœ… å›ºå®šæ± å¯¹æ‰‹ï¼ˆä¼ å…¥ç©ºé—´ä¿¡æ¯ï¼‰
                    loaded_policy = pool.get_opponent_policy(
                        opp, 
                        device,
                        obs_space,
                        action_space
                    )
                    policies[agent] = loaded_policy
    
    else:
        raise ValueError(f"æœªçŸ¥çš„å¯¹æ‰‹ç±»å‹: {opp_type}")
    
    return policies

================================================================================
FILE: src/core/trainer.py
================================================================================
"""
æ ¸å¿ƒè®­ç»ƒå™¨
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„è®­ç»ƒæ¥å£
"""

import os
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

from src.core.environment import WaterworldEnvManager, create_training_env
from src.core.opponent_pool import create_opponent_policies
from src.algorithms import create_algorithm
from src.callbacks import create_callbacks
from src.utils.config_loader import get_mode_config, get_env_config, get_algo_config
from src.utils.path_manager import PathManager
from src.utils.naming import FileNaming
from src.utils.logger import create_logger
from src.utils.banner import print_mode_banner, print_training_start, print_training_complete
from src.utils.config_snapshot import save_config_snapshot, save_training_summary
from src.utils.config_validator import validator
from src.utils.cleanup import cleanup_debug


class MultiAgentTrainer:
    """å¤šæ™ºèƒ½ä½“è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        # æ ¸å¿ƒé…ç½®
        train_side: str,
        train_algo: str,
        opponent_config: Dict[str, Any],
        
        # å®éªŒå…ƒæ•°æ®
        experiment_name: str,
        stage_name: str,
        generation: int = 0,
        version: str = "v1",
        
        # è¿è¡Œæ¨¡å¼
        run_mode: str = "prod",
        
        # ç¯å¢ƒé…ç½®
        env_config: Optional[Dict[str, Any]] = None,
        
        # ç®—æ³•é…ç½®
        algo_config: Optional[Dict[str, Any]] = None,
        
        # è®­ç»ƒé…ç½®ï¼ˆè¦†ç›–æ¨¡å¼é»˜è®¤å€¼ï¼‰
        total_timesteps: Optional[int] = None,
        n_envs: Optional[int] = None,
        eval_freq: Optional[int] = None,
        checkpoint_freq: Optional[int] = None,
        n_eval_episodes: Optional[int] = None,
        
        # å…¶ä»–
        device: str = "auto",
        seed: Optional[int] = None,
        notes: str = ""
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            train_side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
            train_algo: è®­ç»ƒç®—æ³•ï¼ˆPPO/A2C/SAC/TD3ï¼‰
            opponent_config: å¯¹æ‰‹é…ç½®
            experiment_name: å®éªŒåç§°
            stage_name: è®­ç»ƒé˜¶æ®µåç§°ï¼ˆstage1.1_prey_warmupç­‰ï¼‰
            generation: ä»£æ•°
            version: ç‰ˆæœ¬å·
            run_mode: è¿è¡Œæ¨¡å¼ï¼ˆdebug/dryrun/prodï¼‰
            env_config: ç¯å¢ƒé…ç½®ï¼ˆNoneåˆ™ä½¿ç”¨é»˜è®¤ï¼‰
            algo_config: ç®—æ³•é…ç½®ï¼ˆNoneåˆ™ä½¿ç”¨é»˜è®¤ï¼‰
            total_timesteps: æ€»è®­ç»ƒæ­¥æ•°ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            n_envs: å¹¶è¡Œç¯å¢ƒæ•°ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            eval_freq: è¯„ä¼°é¢‘ç‡ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            checkpoint_freq: æ£€æŸ¥ç‚¹é¢‘ç‡ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            n_eval_episodes: è¯„ä¼°episodeæ•°ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            device: è®¡ç®—è®¾å¤‡
            seed: éšæœºç§å­
            notes: å®éªŒå¤‡æ³¨
        """
        
        # =====================================================================
        # 1. åŸºæœ¬é…ç½®
        # =====================================================================
        self.train_side = train_side
        self.train_algo = train_algo.upper()
        self.opponent_config = opponent_config
        self.experiment_name = experiment_name
        self.stage_name = stage_name
        self.generation = generation
        self.version = version
        self.run_mode = run_mode
        self.device = device
        self.seed = seed
        self.notes = notes
        
        # =====================================================================
        # 2. åŠ è½½é…ç½®
        # =====================================================================
        
        # åŠ è½½è¿è¡Œæ¨¡å¼é…ç½®
        self.mode_config = get_mode_config(run_mode)
        
        # åŠ è½½ç¯å¢ƒé…ç½®
        if env_config is None:
            env_config = get_env_config("waterworld_standard")
        self.env_config = env_config
        
        # åŠ è½½ç®—æ³•é…ç½®
        if algo_config is None:
            algo_config = get_algo_config(self.train_algo)
        self.algo_config = algo_config
        
        # åˆå¹¶è®­ç»ƒé…ç½®ï¼ˆç”¨æˆ·æŒ‡å®š > æ¨¡å¼é»˜è®¤ï¼‰
        self.training_config = {
            'total_timesteps': total_timesteps or self.mode_config.get('total_timesteps', 1000000),
            'n_envs': n_envs or self.mode_config.get('n_envs', 1),
            'eval_freq': eval_freq if eval_freq is not None else self.mode_config.get('eval_freq', 10000),
            'checkpoint_freq': checkpoint_freq if checkpoint_freq is not None else self.mode_config.get('checkpoint_freq', 100000),
            'n_eval_episodes': n_eval_episodes or self.mode_config.get('n_eval_episodes', 10),
            'save_checkpoints': self.mode_config.get('save_checkpoints', True),
            'save_final_model': self.mode_config.get('save_final_model', True),
            'tensorboard_enabled': self.mode_config.get('tensorboard_enabled', True),
            'verbose': self.mode_config.get('verbose', 1),
            'deterministic_eval': self.mode_config.get('deterministic_eval', True),
            'show_progress': True,
            'check_freeze': False  # åœ¨è®­ç»ƒåæ‰‹åŠ¨æ£€æŸ¥
        }
        
        # =====================================================================
        # 3. éªŒè¯é…ç½®
        # =====================================================================
        full_config = {
            'run_mode': run_mode,
            'train_side': train_side,
            'train_algo': train_algo,
            'experiment_name': experiment_name,
            **self.training_config
        }
        
        if not validator.validate_run_mode(run_mode, full_config):
            validator.print_results()
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥")
        # test æ¨¡å¼ä¸éœ€è¦ç¡®è®¤
        if run_mode not in ['debug', 'test']:  # âœ… æ·»åŠ  'test'
            if not validator.require_confirmation():
                raise KeyboardInterrupt("ç”¨æˆ·å–æ¶ˆè®­ç»ƒ")
        if not validator.require_confirmation():
            raise KeyboardInterrupt("ç”¨æˆ·å–æ¶ˆè®­ç»ƒ")
        
        # =====================================================================
        # 4. è·¯å¾„ç®¡ç†
        # =====================================================================
        self.path_manager = PathManager(run_mode, experiment_name)
        
        # å„ç§è¾“å‡ºè·¯å¾„
        self.model_dir = self.path_manager.get_model_dir(stage_name)
        self.checkpoint_dir = self.path_manager.get_checkpoint_dir(stage_name)
        self.tensorboard_dir = self.path_manager.get_tensorboard_dir(stage_name)
        self.experiment_dir = self.path_manager.get_experiment_dir(stage_name)
        
        # =====================================================================
        # 5. æ—¥å¿—ç³»ç»Ÿ
        # =====================================================================
        self.naming = FileNaming()
        log_filename = self.naming.generate_log_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        self.logger = create_logger(
            name=f"{self.train_algo}_{self.train_side}",
            log_dir=self.experiment_dir,
            log_level=self.mode_config.get('log_level', 'INFO')
        )
        
        # =====================================================================
        # 6. ç¯å¢ƒå’Œæ¨¡å‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        # =====================================================================
        self.env_manager = None
        self.train_env = None
        self.eval_env = None
        self.algorithm = None
        self.opponent_policies = None
        
        # =====================================================================
        # 7. è®­ç»ƒç»Ÿè®¡
        # =====================================================================
        self.training_start_time = None
        self.training_end_time = None
        self.total_training_time = None
        self.final_model_path = None
        
        # =====================================================================
        # 8. æ‰“å°æ¨ªå¹…
        # =====================================================================
        print_mode_banner(run_mode, self.mode_config)
        
        # =====================================================================
        # 9. æ¸…ç†è°ƒè¯•æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # =====================================================================
        if run_mode == "debug":
            cleanup_debug(self.mode_config)
        
        # =====================================================================
        # 10. ä¿å­˜é…ç½®å¿«ç…§
        # =====================================================================
        self._save_config_snapshot()
    
    def _save_config_snapshot(self):
        """ä¿å­˜é…ç½®å¿«ç…§"""
        snapshot = {
            'run_mode': self.run_mode,
            'train_side': self.train_side,
            'train_algo': self.train_algo,
            'opponent_config': self.opponent_config,
            'experiment_name': self.experiment_name,
            'stage_name': self.stage_name,
            'generation': self.generation,
            'version': self.version,
            'device': self.device,
            'seed': self.seed,
            'notes': self.notes,
            'env_config': self.env_config,
            'algo_config': self.algo_config,
            'training_config': self.training_config,
            'mode_config': self.mode_config
        }
        
        config_filename = self.naming.generate_config_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        save_config_snapshot(
            config=snapshot,
            save_dir=self.experiment_dir,
            name=config_filename.replace('.yaml', '')
        )
    
    def setup(self):
        """
        è®¾ç½®è®­ç»ƒç¯å¢ƒå’Œæ¨¡å‹
        åœ¨è®­ç»ƒå‰å¿…é¡»è°ƒç”¨æ­¤æ–¹æ³•
        """
        self.logger.log_banner("ğŸ”§ è®¾ç½®è®­ç»ƒç¯å¢ƒ", "=")
        
        # =====================================================================
        # 1. åˆ›å»ºç¯å¢ƒç®¡ç†å™¨
        # =====================================================================
        self.logger.info("åˆ›å»ºç¯å¢ƒç®¡ç†å™¨...")
        self.env_manager = WaterworldEnvManager(self.env_config)
        
        # =====================================================================
        # 2. åˆ›å»ºå¯¹æ‰‹ç­–ç•¥
        # =====================================================================
        self.logger.info(f"åˆ›å»ºå¯¹æ‰‹ç­–ç•¥ (ç±»å‹: {self.opponent_config.get('type')})...")
        self.opponent_policies = create_opponent_policies(
            opponent_config=self.opponent_config,
            env_manager=self.env_manager,
            device=self.device
        )
        
        self.logger.info(f"  âœ“ åˆ›å»ºäº† {len(self.opponent_policies)} ä¸ªå¯¹æ‰‹ç­–ç•¥")
        
        # =====================================================================
        # 3. åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        # =====================================================================
        self.logger.info(f"åˆ›å»ºè®­ç»ƒç¯å¢ƒ (å¹¶è¡Œæ•°: {self.training_config['n_envs']})...")
        self.train_env = create_training_env(
            env_config=self.env_config,
            train_side=self.train_side,
            opponent_policies=self.opponent_policies,
            n_envs=self.training_config['n_envs']
        )
        
        # =====================================================================
        # 4. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        # =====================================================================
        if self.training_config['eval_freq'] > 0:
            self.logger.info("åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
            self.eval_env = create_training_env(
                env_config=self.env_config,
                train_side=self.train_side,
                opponent_policies=self.opponent_policies,
                n_envs=1  # è¯„ä¼°ç”¨å•ç¯å¢ƒ
            )
        
        # =====================================================================
        # 5. åˆ›å»ºç®—æ³•
        # =====================================================================
        self.logger.info(f"åˆ›å»ºç®—æ³•: {self.train_algo}...")
        
        # è·å–ç©ºé—´ä¿¡æ¯
        obs_space = self.env_manager.get_observation_space(self.train_side)
        action_space = self.env_manager.get_action_space(self.train_side)
        
        # åˆ›å»ºç®—æ³•å®ä¾‹
        self.algorithm = create_algorithm(
            algo_name=self.train_algo,
            observation_space=obs_space,
            action_space=action_space,
            config=self.algo_config,
            device=self.device
        )
        
        # åˆ›å»ºæ¨¡å‹
        tensorboard_log = str(self.tensorboard_dir) if self.training_config['tensorboard_enabled'] else None
        
        self.algorithm.create_model(
            env=self.train_env,
            tensorboard_log=tensorboard_log,
            verbose=self.training_config['verbose']
        )
        
        self.logger.info("  âœ“ ç®—æ³•åˆ›å»ºå®Œæˆ")
        
        # =====================================================================
        # 6. è®°å½•é…ç½®
        # =====================================================================
        self.logger.log_config({
            'è®­ç»ƒæ–¹': self.train_side,
            'è®­ç»ƒç®—æ³•': self.train_algo,
            'ç‰ˆæœ¬': self.version,
            'æ€»æ­¥æ•°': self.training_config['total_timesteps'],
            'å¹¶è¡Œç¯å¢ƒ': self.training_config['n_envs'],
            'è¯„ä¼°é¢‘ç‡': self.training_config['eval_freq'],
            'æ£€æŸ¥ç‚¹é¢‘ç‡': self.training_config['checkpoint_freq'],
            'è®¾å¤‡': self.device,
            'éšæœºç§å­': self.seed
        }, title="è®­ç»ƒé…ç½®")
        
        self.logger.log_banner("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ", "=")
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        
        # =====================================================================
        # 1. æ£€æŸ¥æ˜¯å¦å·²è®¾ç½®
        # =====================================================================
        if self.algorithm is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ setup() æ–¹æ³•è®¾ç½®ç¯å¢ƒå’Œæ¨¡å‹")
        
        # =====================================================================
        # 2. æ‰“å°è®­ç»ƒå¼€å§‹ä¿¡æ¯
        # =====================================================================
        opponent_info = self.naming.format_opponent_info(self.opponent_config)
        print_training_start(
            algo=self.train_algo,
            side=self.train_side,
            version=self.version,
            opponent_info=opponent_info
        )
        
        self.logger.log_banner(f"ğŸš€ å¼€å§‹è®­ç»ƒ {self.train_algo}_{self.train_side}_{self.version}", "=")
        
        # =====================================================================
        # 3. åˆ›å»ºå›è°ƒ
        # =====================================================================
        callbacks = create_callbacks(
            train_side=self.train_side,
            checkpoint_path=self.checkpoint_dir,
            eval_env=self.eval_env,
            config=self.training_config,
            on_freeze=None  # å¯ä»¥æ·»åŠ å†»ç»“å›è°ƒ
        )
        
        # =====================================================================
        # 4. æ‰§è¡Œè®­ç»ƒ
        # =====================================================================
        self.training_start_time = time.time()
        
        try:
            self.algorithm.train(
                env=self.train_env,
                total_timesteps=self.training_config['total_timesteps'],
                callback=callbacks
            )
            
            self.training_end_time = time.time()
            self.total_training_time = self.training_end_time - self.training_start_time
            
        except KeyboardInterrupt:
            self.logger.warning("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            self.training_end_time = time.time()
            self.total_training_time = self.training_end_time - self.training_start_time
            raise
        
        except Exception as e:
            self.logger.error(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        
        # =====================================================================
        # 5. æ‰“å°è®­ç»ƒå®Œæˆä¿¡æ¯
        # =====================================================================
        print_training_complete(
            algo=self.train_algo,
            side=self.train_side,
            total_steps=self.training_config['total_timesteps'],
            time_elapsed=self.total_training_time
        )
        
        self.logger.log_banner("âœ… è®­ç»ƒå®Œæˆ", "=")
    
    def save_model(self, save_to_pool: bool = False, pool_name: Optional[str] = None):
        """
        ä¿å­˜æœ€ç»ˆæ¨¡å‹
        
        Args:
            save_to_pool: æ˜¯å¦ä¿å­˜åˆ°å›ºå®šæ± 
            pool_name: æ± åç§°ï¼ˆå¦‚ prey_pool_v1ï¼‰
        """
        
        if not self.training_config['save_final_model']:
            self.logger.info("é…ç½®ç¦ç”¨äº†æ¨¡å‹ä¿å­˜ï¼Œè·³è¿‡")
            return
        
        self.logger.log_banner("ğŸ’¾ ä¿å­˜æ¨¡å‹", "-")
        
        # =====================================================================
        # 1. ä¿å­˜åˆ°æ¨¡å‹ç›®å½•
        # =====================================================================
        opponent_info = self.naming.format_opponent_info(self.opponent_config)
        
        model_filename = self.naming.generate_model_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version,
            opponent_info=opponent_info,
            run_mode=self.run_mode
        )
        
        model_path = self.model_dir / model_filename
        self.algorithm.save(str(model_path))
        self.final_model_path = model_path
        
        self.logger.info(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # =====================================================================
        # 2. ä¿å­˜åˆ°å›ºå®šæ± ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # =====================================================================
        if save_to_pool and pool_name:
            pool_dir = self.path_manager.get_fixed_pool_dir(pool_name)
            
            pool_model_filename = f"{self.train_algo}_{self.train_side}_{self.version}.zip"
            pool_model_path = pool_dir / pool_model_filename
            
            # å¤åˆ¶æ¨¡å‹åˆ°æ± 
            import shutil
            shutil.copy(str(model_path), str(pool_model_path))
            
            self.logger.info(f"âœ“ æ¨¡å‹å·²åŠ å…¥å›ºå®šæ± : {pool_model_path}")
            
            # æ›´æ–°æ± çš„metadata
            self._update_pool_metadata(pool_dir, pool_model_filename)
        
        self.logger.log_banner("", "-")
    
    def _update_pool_metadata(self, pool_dir: Path, model_filename: str):
        """æ›´æ–°å›ºå®šæ± çš„metadata"""
        metadata_path = pool_dir / "metadata.json"
        
        # åŠ è½½ç°æœ‰metadata
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {
                'pool_name': pool_dir.name,
                'created_at': datetime.now().isoformat(),
                'models': []
            }
        
        # æ·»åŠ æ–°æ¨¡å‹ä¿¡æ¯
        model_info = {
            'name': model_filename.replace('.zip', ''),
            'path': model_filename,
            'algorithm': self.train_algo,
            'training_steps': self.training_config['total_timesteps'],
            'trained_against': self.naming.format_opponent_info(self.opponent_config),
            'added_at': datetime.now().isoformat(),
            'eval_metrics': {}  # å¯ä»¥åœ¨è¯„ä¼°åå¡«å……
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = [m for m in metadata['models'] if m['name'] == model_info['name']]
        if existing:
            # æ›´æ–°ç°æœ‰æ¡ç›®
            idx = metadata['models'].index(existing[0])
            metadata['models'][idx] = model_info
        else:
            # æ·»åŠ æ–°æ¡ç›®
            metadata['models'].append(model_info)
        
        # ä¿å­˜metadata
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def evaluate(self, n_episodes: Optional[int] = None) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            n_episodes: è¯„ä¼°episodeæ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼‰
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        from stable_baselines3.common.evaluation import evaluate_policy
        
        if self.eval_env is None:
            self.logger.warning("è¯„ä¼°ç¯å¢ƒæœªåˆ›å»ºï¼Œæ— æ³•è¯„ä¼°")
            return {}
        
        n_episodes = n_episodes or self.training_config['n_eval_episodes']
        
        self.logger.log_banner(f"ğŸ“Š è¯„ä¼°æ¨¡å‹ ({n_episodes} episodes)", "-")
        
        episode_rewards, episode_lengths = evaluate_policy(
            self.algorithm.model,
            self.eval_env,
            n_eval_episodes=n_episodes,
            deterministic=self.training_config['deterministic_eval'],
            return_episode_rewards=True
        )
        
        results = {
            'mean_reward': float(np.mean(episode_rewards)),
            'std_reward': float(np.std(episode_rewards)),
            'mean_length': float(np.mean(episode_lengths)),
            'std_length': float(np.std(episode_lengths)),
            'min_reward': float(np.min(episode_rewards)),
            'max_reward': float(np.max(episode_rewards)),
            'n_episodes': n_episodes
        }
        
        self.logger.log_config(results, title="è¯„ä¼°ç»“æœ")
        self.logger.log_banner("", "-")
        
        return results
    
    def check_freeze_criteria(self, freeze_criteria: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å†»ç»“æ¡ä»¶
        
        Args:
            freeze_criteria: å†»ç»“æ¡ä»¶å­—å…¸
        
        Returns:
            æ˜¯å¦è¾¾åˆ°å†»ç»“æ¡ä»¶
        """
        # å…ˆè¯„ä¼°æ¨¡å‹
        eval_results = self.evaluate()
        
        if not eval_results:
            return False
        
        # æ£€æŸ¥æœ€ä½å¥–åŠ±
        min_reward = freeze_criteria.get('min_avg_reward', -np.inf)
        if eval_results['mean_reward'] < min_reward:
            self.logger.info(f"âŒ æœªè¾¾åˆ°æœ€ä½å¥–åŠ±: {eval_results['mean_reward']:.2f} < {min_reward:.2f}")
            return False
        
        # è§’è‰²ç‰¹å®šæ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼Œå®Œæ•´ç‰ˆéœ€è¦åœ¨å›è°ƒä¸­å®ç°ï¼‰
        if self.train_side == "predator":
            min_catch_rate = freeze_criteria.get('min_catch_rate', 0.0)
            # è¿™é‡Œéœ€è¦ä»è¯„ä¼°ä¸­è·å–catch_rateï¼Œæš‚æ—¶ç”¨å¥–åŠ±ä»£æ›¿
            self.logger.info(f"âœ“ è¾¾åˆ°æœ€ä½å¥–åŠ±: {eval_results['mean_reward']:.2f} >= {min_reward:.2f}")
        
        elif self.train_side == "prey":
            min_survival_rate = freeze_criteria.get('min_survival_rate', 0.0)
            # è¿™é‡Œéœ€è¦ä»è¯„ä¼°ä¸­è·å–survival_rateï¼Œæš‚æ—¶ç”¨å¥–åŠ±ä»£æ›¿
            self.logger.info(f"âœ“ è¾¾åˆ°æœ€ä½å¥–åŠ±: {eval_results['mean_reward']:.2f} >= {min_reward:.2f}")
        
        return True
    
    def save_training_summary(self):
        """ä¿å­˜è®­ç»ƒæ‘˜è¦"""
        summary = {
            'experiment_name': self.experiment_name,
            'stage_name': self.stage_name,
            'train_side': self.train_side,
            'train_algo': self.train_algo,
            'version': self.version,
            'generation': self.generation,
            'run_mode': self.run_mode,
            'opponent': self.naming.format_opponent_info(self.opponent_config),
            'training_config': self.training_config,
            'training_time_seconds': self.total_training_time,
            'final_model_path': str(self.final_model_path) if self.final_model_path else None,
            'notes': self.notes
        }
        
        summary_filename = self.naming.generate_summary_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        save_training_summary(
            summary=summary,
            save_dir=self.experiment_dir,
            name=summary_filename.replace('.json', '')
        )
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("æ¸…ç†èµ„æº...")
        
        if self.train_env:
            self.train_env.close()
        
        if self.eval_env:
            self.eval_env.close()
        
        if self.env_manager:
            self.env_manager.close()
        
        self.logger.info("âœ“ èµ„æºæ¸…ç†å®Œæˆ")
    
    def run(
        self,
        save_to_pool: bool = False,
        pool_name: Optional[str] = None,
        check_freeze: bool = False,
        freeze_criteria: Optional[Dict[str, Any]] = None
    ):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆsetup â†’ train â†’ evaluate â†’ saveï¼‰
        
        Args:
            save_to_pool: æ˜¯å¦ä¿å­˜åˆ°å›ºå®šæ± 
            pool_name: æ± åç§°
            check_freeze: æ˜¯å¦æ£€æŸ¥å†»ç»“æ¡ä»¶
            freeze_criteria: å†»ç»“æ¡ä»¶
        """
        try:
            # 1. è®¾ç½®
            self.setup()
            
            # 2. è®­ç»ƒ
            self.train()
            
            # 3. æœ€ç»ˆè¯„ä¼°
            final_eval = self.evaluate()
            
            # 4. æ£€æŸ¥å†»ç»“æ¡ä»¶
            can_freeze = True
            if check_freeze and freeze_criteria:
                can_freeze = self.check_freeze_criteria(freeze_criteria)
                
                if can_freeze:
                    self.logger.log_banner("â„ï¸  æ¨¡å‹è¾¾åˆ°å†»ç»“æ ‡å‡†", "=")
                else:
                    self.logger.log_banner("âš ï¸  æ¨¡å‹æœªè¾¾åˆ°å†»ç»“æ ‡å‡†", "=")
            
            # 5. ä¿å­˜æ¨¡å‹
            should_save_to_pool = save_to_pool and (not check_freeze or can_freeze)
            self.save_model(save_to_pool=should_save_to_pool, pool_name=pool_name)
            
            # 6. ä¿å­˜è®­ç»ƒæ‘˜è¦
            self.save_training_summary()
            
            return final_eval
        
        finally:
            # 7. æ¸…ç†
            self.cleanup()


# å¯¼å…¥numpyï¼ˆå‰é¢å¿˜è®°äº†ï¼‰
import numpy as np

================================================================================
FILE: src/analysis/__init__.py
================================================================================


================================================================================
FILE: src/callbacks/__init__.py
================================================================================
"""
å›è°ƒæ¨¡å—åˆå§‹åŒ–
"""

from stable_baselines3.common.callbacks import CallbackList

from .tensorboard_logger import MultiAgentTensorBoardCallback
from .checkpoint_callback import CheckpointCallback
from .eval_callback import EvalCallback
from .freeze_callback import FreezeCallback
from .progress_callback import ProgressBarCallback


def create_callbacks(
    train_side: str,
    checkpoint_path,
    eval_env,
    config: dict,
    on_freeze=None
) -> CallbackList:
    """
    åˆ›å»ºå›è°ƒåˆ—è¡¨
    
    Args:
        train_side: è®­ç»ƒæ–¹
        checkpoint_path: æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„
        eval_env: è¯„ä¼°ç¯å¢ƒ
        config: é…ç½®å­—å…¸
        on_freeze: å†»ç»“å›è°ƒå‡½æ•°
    
    Returns:
        å›è°ƒåˆ—è¡¨
    """
    callbacks = []
    
    # 1. TensorBoardæ—¥å¿—
    tb_callback = MultiAgentTensorBoardCallback(
        train_side=train_side,
        verbose=config.get('verbose', 1)
    )
    callbacks.append(tb_callback)
    
    # 2. æ£€æŸ¥ç‚¹ä¿å­˜
    if config.get('save_checkpoints', True):
        checkpoint_freq = config.get('checkpoint_freq', 100000)
        if checkpoint_freq > 0:
            checkpoint_callback = CheckpointCallback(
                save_freq=checkpoint_freq,
                save_path=checkpoint_path,
                name_prefix="checkpoint",
                verbose=config.get('verbose', 1)
            )
            callbacks.append(checkpoint_callback)
    
    # 3. è¯„ä¼°
    if config.get('eval_freq', -1) > 0:
        eval_callback = EvalCallback(
            eval_env=eval_env,
            train_side=train_side,
            eval_freq=config.get('eval_freq', 10000),
            n_eval_episodes=config.get('n_eval_episodes', 10),
            deterministic=config.get('deterministic_eval', True),
            verbose=config.get('verbose', 1),
            best_model_save_path=checkpoint_path
        )
        callbacks.append(eval_callback)
        
        # 4. å†»ç»“æ¡ä»¶æ£€æŸ¥
        if config.get('check_freeze', False):
            freeze_criteria = config.get('freeze_criteria', {})
            freeze_callback = FreezeCallback(
                eval_callback=eval_callback,
                train_side=train_side,
                freeze_criteria=freeze_criteria,
                on_freeze=on_freeze,
                verbose=config.get('verbose', 1)
            )
            callbacks.append(freeze_callback)
    
    # 5. è¿›åº¦æ¡
    if config.get('show_progress', True):
        progress_callback = ProgressBarCallback(
            total_timesteps=config.get('total_timesteps', 1000000),
            verbose=config.get('verbose', 1)
        )
        callbacks.append(progress_callback)
    
    return CallbackList(callbacks)


__all__ = [
    'MultiAgentTensorBoardCallback',
    'CheckpointCallback',
    'EvalCallback',
    'FreezeCallback',
    'ProgressBarCallback',
    'create_callbacks'
]

================================================================================
FILE: src/callbacks/checkpoint_callback.py
================================================================================
"""
æ£€æŸ¥ç‚¹ä¿å­˜å›è°ƒ
å®šæœŸä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
"""

import os
from pathlib import Path
from typing import Optional
from stable_baselines3.common.callbacks import BaseCallback


class CheckpointCallback(BaseCallback):
    """æ£€æŸ¥ç‚¹ä¿å­˜å›è°ƒ"""
    
    def __init__(
        self,
        save_freq: int,
        save_path: Path,
        name_prefix: str = "checkpoint",
        save_replay_buffer: bool = False,
        save_vecnormalize: bool = False,
        verbose: int = 0
    ):
        """
        åˆå§‹åŒ–å›è°ƒ
        
        Args:
            save_freq: ä¿å­˜é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
            save_path: ä¿å­˜è·¯å¾„
            name_prefix: æ–‡ä»¶åå‰ç¼€
            save_replay_buffer: æ˜¯å¦ä¿å­˜replay bufferï¼ˆSAC/TD3ï¼‰
            save_vecnormalize: æ˜¯å¦ä¿å­˜VecNormalizeç»Ÿè®¡
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.save_freq = save_freq
        self.save_path = Path(save_path)
        self.name_prefix = name_prefix
        self.save_replay_buffer = save_replay_buffer
        self.save_vecnormalize = save_vecnormalize
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_path.mkdir(parents=True, exist_ok=True)
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        if self.save_freq > 0 and self.n_calls % self.save_freq == 0:
            self._save_checkpoint()
        
        return True
    
    def _save_checkpoint(self):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # æ„å»ºæ–‡ä»¶å
        checkpoint_name = f"{self.name_prefix}_step_{self.n_calls}.zip"
        checkpoint_path = self.save_path / checkpoint_name
        
        # ä¿å­˜æ¨¡å‹
        self.model.save(checkpoint_path)
        
        if self.verbose > 0:
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
        
        # ä¿å­˜replay bufferï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if self.save_replay_buffer and hasattr(self.model, 'replay_buffer'):
            if self.model.replay_buffer is not None:
                buffer_path = self.save_path / f"{self.name_prefix}_replay_buffer_step_{self.n_calls}.pkl"
                self.model.save_replay_buffer(buffer_path)
                
                if self.verbose > 0:
                    print(f"ğŸ’¾ ä¿å­˜replay buffer: {buffer_path}")
        
        # ä¿å­˜VecNormalizeç»Ÿè®¡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if self.save_vecnormalize:
            from stable_baselines3.common.vec_env import VecNormalize
            if isinstance(self.training_env, VecNormalize):
                vecnorm_path = self.save_path / f"{self.name_prefix}_vecnormalize_step_{self.n_calls}.pkl"
                self.training_env.save(vecnorm_path)
                
                if self.verbose > 0:
                    print(f"ğŸ’¾ ä¿å­˜VecNormalize: {vecnorm_path}")

================================================================================
FILE: src/callbacks/eval_callback.py
================================================================================
"""
è¯„ä¼°å›è°ƒ
å®šæœŸè¯„ä¼°æ¨¡å‹æ€§èƒ½
"""

import numpy as np
from pathlib import Path
from typing import Optional, Callable, Dict, Any
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy


class EvalCallback(BaseCallback):
    """è¯„ä¼°å›è°ƒ"""
    
    def __init__(
        self,
        eval_env,
        train_side: str,
        eval_freq: int = 10000,
        n_eval_episodes: int = 10,
        deterministic: bool = True,
        render: bool = False,
        verbose: int = 1,
        best_model_save_path: Optional[Path] = None,
        log_path: Optional[Path] = None
    ):
        """
        åˆå§‹åŒ–å›è°ƒ
        
        Args:
            eval_env: è¯„ä¼°ç¯å¢ƒ
            train_side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
            eval_freq: è¯„ä¼°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
            n_eval_episodes: è¯„ä¼°episodeæ•°
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            render: æ˜¯å¦æ¸²æŸ“
            verbose: è¯¦ç»†ç¨‹åº¦
            best_model_save_path: æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
            log_path: æ—¥å¿—ä¿å­˜è·¯å¾„
        """
        super().__init__(verbose)
        self.eval_env = eval_env
        self.train_side = train_side
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.deterministic = deterministic
        self.render = render
        self.best_model_save_path = best_model_save_path
        self.log_path = log_path
        
        # æœ€ä½³æ€§èƒ½è·Ÿè¸ª
        self.best_mean_reward = -np.inf
        self.last_mean_reward = -np.inf
        
        # è¯„ä¼°å†å²
        self.evaluations_timesteps = []
        self.evaluations_results = []
        self.evaluations_length = []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        if self.best_model_save_path:
            self.best_model_save_path = Path(self.best_model_save_path)
            self.best_model_save_path.mkdir(parents=True, exist_ok=True)
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            self._evaluate()
        
        return True
    
    def _evaluate(self):
        """æ‰§è¡Œè¯„ä¼°"""
        if self.verbose > 0:
            print(f"\n{'='*70}")
            print(f"ğŸ“Š è¯„ä¼° (æ­¥æ•°: {self.n_calls})")
            print(f"{'='*70}")
        
        # è¯„ä¼°æ¨¡å‹
        episode_rewards, episode_lengths = evaluate_policy(
            self.model,
            self.eval_env,
            n_eval_episodes=self.n_eval_episodes,
            render=self.render,
            deterministic=self.deterministic,
            return_episode_rewards=True
        )
        
        mean_reward = np.mean(episode_rewards)
        std_reward = np.std(episode_rewards)
        mean_length = np.mean(episode_lengths)
        
        self.last_mean_reward = mean_reward
        
        # è®°å½•ç»“æœ
        self.evaluations_timesteps.append(self.n_calls)
        self.evaluations_results.append(episode_rewards)
        self.evaluations_length.append(episode_lengths)
        
        # è®°å½•åˆ°TensorBoard
        self.logger.record('eval/mean_reward', mean_reward)
        self.logger.record('eval/std_reward', std_reward)
        self.logger.record('eval/mean_ep_length', mean_length)
        
        if self.verbose > 0:
            print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.2f} +/- {std_reward:.2f}")
            print(f"  å¹³å‡é•¿åº¦: {mean_length:.0f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if mean_reward > self.best_mean_reward:
            if self.verbose > 0:
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! (æ—§: {self.best_mean_reward:.2f}, æ–°: {mean_reward:.2f})")
            
            self.best_mean_reward = mean_reward
            
            if self.best_model_save_path:
                best_model_path = self.best_model_save_path / "best_model.zip"
                self.model.save(best_model_path)
                
                if self.verbose > 0:
                    print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_model_path}")
        
        if self.verbose > 0:
            print(f"{'='*70}\n")
    
    def get_best_mean_reward(self) -> float:
        """è·å–æœ€ä½³å¹³å‡å¥–åŠ±"""
        return self.best_mean_reward
    
    def get_last_mean_reward(self) -> float:
        """è·å–æœ€è¿‘ä¸€æ¬¡å¹³å‡å¥–åŠ±"""
        return self.last_mean_reward

================================================================================
FILE: src/callbacks/freeze_callback.py
================================================================================
"""
å†»ç»“æ¡ä»¶æ£€æŸ¥å›è°ƒ
æ£€æŸ¥æ¨¡å‹æ˜¯å¦è¾¾åˆ°å†»ç»“æ ‡å‡†ï¼Œå¯ä»¥åŠ å…¥å›ºå®šæ± 
"""

from pathlib import Path
from typing import Dict, Any, Optional, Callable
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np


class FreezeCallback(BaseCallback):
    """å†»ç»“æ¡ä»¶æ£€æŸ¥å›è°ƒ"""
    
    def __init__(
        self,
        eval_callback: BaseCallback,
        train_side: str,
        freeze_criteria: Dict[str, Any],
        on_freeze: Optional[Callable] = None,
        verbose: int = 1
    ):
        """
        åˆå§‹åŒ–å›è°ƒ
        
        Args:
            eval_callback: è¯„ä¼°å›è°ƒï¼ˆç”¨äºè·å–è¯„ä¼°ç»“æœï¼‰
            train_side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
            freeze_criteria: å†»ç»“æ¡ä»¶
            on_freeze: è¾¾åˆ°å†»ç»“æ¡ä»¶æ—¶çš„å›è°ƒå‡½æ•°
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.eval_callback = eval_callback
        self.train_side = train_side
        self.freeze_criteria = freeze_criteria
        self.on_freeze = on_freeze
        
        # å†»ç»“çŠ¶æ€
        self.is_frozen = False
        self.freeze_timestep = None
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        # åªåœ¨è¯„ä¼°åæ£€æŸ¥
        if not hasattr(self.eval_callback, 'last_mean_reward'):
            return True
        
        # å¦‚æœå·²ç»å†»ç»“ï¼Œä¸å†æ£€æŸ¥
        if self.is_frozen:
            return True
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å†»ç»“æ¡ä»¶
        if self._check_freeze_criteria():
            self._freeze_model()
        
        return True
    
    def _check_freeze_criteria(self) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å†»ç»“æ¡ä»¶
        
        Returns:
            æ˜¯å¦è¾¾åˆ°å†»ç»“æ¡ä»¶
        """
        # è·å–è¯„ä¼°ç»“æœ
        last_mean_reward = self.eval_callback.get_last_mean_reward()
        
        # è·å–å¯¹åº”è§’è‰²çš„å†»ç»“æ ‡å‡†
        criteria = self.freeze_criteria
        
        # æ£€æŸ¥æœ€ä½å¥–åŠ±
        min_reward = criteria.get('min_avg_reward', -np.inf)
        if last_mean_reward < min_reward:
            if self.verbose > 1:
                print(f"  âŒ å¥–åŠ±ä¸è¶³: {last_mean_reward:.2f} < {min_reward:.2f}")
            return False
        
        # æ£€æŸ¥è§’è‰²ç‰¹å®šæŒ‡æ ‡
        if self.train_side == "predator":
            # Predatoréœ€è¦æ£€æŸ¥æ•è·ç‡
            min_catch_rate = criteria.get('min_catch_rate', 0.0)
            # è¿™é‡Œéœ€è¦ä»è¯„ä¼°ç»“æœä¸­è·å–catch_rate
            # æš‚æ—¶ç®€åŒ–å¤„ç†
            if self.verbose > 1:
                print(f"  âœ“ å¥–åŠ±è¾¾æ ‡: {last_mean_reward:.2f} >= {min_reward:.2f}")
        
        elif self.train_side == "prey":
            # Preyéœ€è¦æ£€æŸ¥ç”Ÿå­˜ç‡
            min_survival_rate = criteria.get('min_survival_rate', 0.0)
            # è¿™é‡Œéœ€è¦ä»è¯„ä¼°ç»“æœä¸­è·å–survival_rate
            # æš‚æ—¶ç®€åŒ–å¤„ç†
            if self.verbose > 1:
                print(f"  âœ“ å¥–åŠ±è¾¾æ ‡: {last_mean_reward:.2f} >= {min_reward:.2f}")
        
        # æ£€æŸ¥è¯„ä¼°episodeæ•°
        min_episodes = criteria.get('min_episodes', 0)
        n_eval_episodes = self.eval_callback.n_eval_episodes
        if n_eval_episodes < min_episodes:
            if self.verbose > 1:
                print(f"  âŒ è¯„ä¼°episodeä¸è¶³: {n_eval_episodes} < {min_episodes}")
            return False
        
        return True
    
    def _freeze_model(self):
        """å†»ç»“æ¨¡å‹"""
        self.is_frozen = True
        self.freeze_timestep = self.n_calls
        
        if self.verbose > 0:
            print(f"\n{'='*70}")
            print(f"â„ï¸  æ¨¡å‹è¾¾åˆ°å†»ç»“æ¡ä»¶")
            print(f"{'='*70}")
            print(f"  è®­ç»ƒæ­¥æ•°: {self.freeze_timestep}")
            print(f"  å¹³å‡å¥–åŠ±: {self.eval_callback.get_last_mean_reward():.2f}")
            print(f"{'='*70}\n")
        
        # è°ƒç”¨è‡ªå®šä¹‰å›è°ƒ
        if self.on_freeze:
            self.on_freeze(self.model, self.freeze_timestep)
    
    def is_model_frozen(self) -> bool:
        """æ¨¡å‹æ˜¯å¦å·²å†»ç»“"""
        return self.is_frozen

================================================================================
FILE: src/callbacks/progress_callback.py
================================================================================
"""
è¿›åº¦æ˜¾ç¤ºå›è°ƒ
æ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡å’Œç»Ÿè®¡ä¿¡æ¯
"""

from typing import Optional
from stable_baselines3.common.callbacks import BaseCallback
from tqdm import tqdm


class ProgressBarCallback(BaseCallback):
    """è¿›åº¦æ¡å›è°ƒ"""
    
    def __init__(
        self,
        total_timesteps: int,
        verbose: int = 1
    ):
        """
        åˆå§‹åŒ–å›è°ƒ
        
        Args:
            total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.total_timesteps = total_timesteps
        self.pbar: Optional[tqdm] = None
    
    def _on_training_start(self) -> None:
        """è®­ç»ƒå¼€å§‹æ—¶åˆ›å»ºè¿›åº¦æ¡"""
        if self.verbose > 0:
            self.pbar = tqdm(
                total=self.total_timesteps,
                desc="è®­ç»ƒè¿›åº¦",
                unit="æ­¥"
            )
    
    def _on_step(self) -> bool:
        """æ¯æ­¥æ›´æ–°è¿›åº¦æ¡"""
        if self.pbar:
            self.pbar.update(1)
            
            # æ›´æ–°è¿›åº¦æ¡åç¼€ä¿¡æ¯
            if hasattr(self, 'locals') and 'infos' in self.locals:
                for info in self.locals['infos']:
                    if 'episode' in info:
                        ep_reward = info['episode']['r']
                        ep_length = info['episode']['l']
                        self.pbar.set_postfix({
                            'reward': f'{ep_reward:.2f}',
                            'length': f'{ep_length:.0f}'
                        })
                        break
        
        return True
    
    def _on_training_end(self) -> None:
        """è®­ç»ƒç»“æŸæ—¶å…³é—­è¿›åº¦æ¡"""
        if self.pbar:
            self.pbar.close()
            self.pbar = None

================================================================================
FILE: src/callbacks/tensorboard_logger.py
================================================================================
"""
è‡ªå®šä¹‰TensorBoardæ—¥å¿—å›è°ƒ
è®°å½•å¤šæ™ºèƒ½ä½“ç‰¹å®šçš„æŒ‡æ ‡
"""

from typing import Dict, Any, Optional
import numpy as np
from stable_baselines3.common.callbacks import BaseCallback


class MultiAgentTensorBoardCallback(BaseCallback):
    """å¤šæ™ºèƒ½ä½“TensorBoardæ—¥å¿—å›è°ƒ"""
    
    def __init__(
        self,
        train_side: str,
        verbose: int = 0
    ):
        """
        åˆå§‹åŒ–å›è°ƒ
        
        Args:
            train_side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.train_side = train_side
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_counts = 0
        
        # è§’è‰²ç‰¹å®šæŒ‡æ ‡
        self.catch_rates = []  # Predator
        self.survival_rates = []  # Prey
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        # æ£€æŸ¥æ˜¯å¦æœ‰episodeç»“æŸ
        for info in self.locals.get('infos', []):
            if 'episode' in info:
                # è®°å½•åŸºæœ¬æŒ‡æ ‡
                episode_reward = info['episode']['r']
                episode_length = info['episode']['l']
                
                self.episode_rewards.append(episode_reward)
                self.episode_lengths.append(episode_length)
                self.episode_counts += 1
                
                # è®°å½•åˆ°TensorBoard
                self.logger.record('rollout/ep_rew_mean', episode_reward)
                self.logger.record('rollout/ep_len_mean', episode_length)
                
                # è§’è‰²ç‰¹å®šæŒ‡æ ‡
                if self.train_side == "predator":
                    # PredatoræŒ‡æ ‡ï¼šæ•è·ç‡
                    if 'catch_rate' in info:
                        catch_rate = info['catch_rate']
                        self.catch_rates.append(catch_rate)
                        self.logger.record('metrics/catch_rate', catch_rate)
                    
                    if 'first_catch_time' in info:
                        self.logger.record('metrics/first_catch_time', info['first_catch_time'])
                    
                    if 'energy_efficiency' in info:
                        self.logger.record('metrics/energy_efficiency', info['energy_efficiency'])
                
                elif self.train_side == "prey":
                    # PreyæŒ‡æ ‡ï¼šç”Ÿå­˜ç‡
                    if 'survival_rate' in info:
                        survival_rate = info['survival_rate']
                        self.survival_rates.append(survival_rate)
                        self.logger.record('metrics/survival_rate', survival_rate)
                    
                    if 'avg_lifespan' in info:
                        self.logger.record('metrics/avg_lifespan', info['avg_lifespan'])
                    
                    if 'escape_success' in info:
                        self.logger.record('metrics/escape_success', info['escape_success'])
                
                # å¯¹æˆ˜çº§æŒ‡æ ‡
                if 'reward_gap' in info:
                    self.logger.record('metrics/reward_gap', info['reward_gap'])
                
                if 'balance_score' in info:
                    self.logger.record('metrics/balance_score', info['balance_score'])
        
        return True
    
    def _on_training_end(self) -> None:
        """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨"""
        if self.episode_counts > 0:
            # è®°å½•æ±‡æ€»ç»Ÿè®¡
            self.logger.record('summary/total_episodes', self.episode_counts)
            self.logger.record('summary/mean_episode_reward', np.mean(self.episode_rewards))
            self.logger.record('summary/mean_episode_length', np.mean(self.episode_lengths))
            
            if self.train_side == "predator" and self.catch_rates:
                self.logger.record('summary/mean_catch_rate', np.mean(self.catch_rates))
            
            if self.train_side == "prey" and self.survival_rates:
                self.logger.record('summary/mean_survival_rate', np.mean(self.survival_rates))

================================================================================
FILE: src/algorithms/__init__.py
================================================================================
"""
ç®—æ³•æ¨¡å—åˆå§‹åŒ–
æä¾›ç®—æ³•å·¥å‚å‡½æ•°
"""

from typing import Dict, Any
import gymnasium as gym

from .base_algorithm import BaseAlgorithm
from .ppo_wrapper import PPOWrapper
from .a2c_wrapper import A2CWrapper
from .sac_wrapper import SACWrapper
from .td3_wrapper import TD3Wrapper
from .random_policy import RandomPolicy


# ç®—æ³•æ˜ å°„
ALGORITHM_MAP = {
    'PPO': PPOWrapper,
    'A2C': A2CWrapper,
    'SAC': SACWrapper,
    'TD3': TD3Wrapper,
    'RANDOM': RandomPolicy
}


def create_algorithm(
    algo_name: str,
    observation_space: gym.Space,
    action_space: gym.Space,
    config: Dict[str, Any],
    device: str = "auto"
) -> BaseAlgorithm:
    """
    åˆ›å»ºç®—æ³•å®ä¾‹
    
    Args:
        algo_name: ç®—æ³•åç§°ï¼ˆPPO/A2C/SAC/TD3/RANDOMï¼‰
        observation_space: è§‚å¯Ÿç©ºé—´
        action_space: åŠ¨ä½œç©ºé—´
        config: ç®—æ³•é…ç½®
        device: è®¡ç®—è®¾å¤‡
    
    Returns:
        ç®—æ³•å®ä¾‹
    """
    algo_name = algo_name.upper()
    
    if algo_name not in ALGORITHM_MAP:
        raise ValueError(
            f"æœªçŸ¥çš„ç®—æ³•: {algo_name}. "
            f"æ”¯æŒçš„ç®—æ³•: {list(ALGORITHM_MAP.keys())}"
        )
    
    algo_class = ALGORITHM_MAP[algo_name]
    return algo_class(observation_space, action_space, config, device)


__all__ = [
    'BaseAlgorithm',
    'PPOWrapper',
    'A2CWrapper',
    'SACWrapper',
    'TD3Wrapper',
    'RandomPolicy',
    'create_algorithm',
    'ALGORITHM_MAP'
]

================================================================================
FILE: src/algorithms/a2c_wrapper.py
================================================================================
"""
A2Cç®—æ³•å°è£…
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym
from stable_baselines3 import A2C
import torch

from .base_algorithm import BaseAlgorithm


class A2CWrapper(BaseAlgorithm):
    """A2Cç®—æ³•åŒ…è£…å™¨"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        super().__init__(observation_space, action_space, config, device)
        
        hyperparams = config.get('hyperparameters', {})
        policy_kwargs = hyperparams.get('policy_kwargs', {}).copy()
        
        if 'activation_fn' in policy_kwargs:
            activation_str = policy_kwargs['activation_fn']
            if activation_str == "torch.nn.ReLU":
                policy_kwargs['activation_fn'] = torch.nn.ReLU
            elif activation_str == "torch.nn.Tanh":
                policy_kwargs['activation_fn'] = torch.nn.Tanh
        
        self.hyperparams = {
            'policy': hyperparams.get('policy', 'MlpPolicy'),
            'learning_rate': hyperparams.get('learning_rate', 7e-4),
            'n_steps': hyperparams.get('n_steps', 5),
            'gamma': hyperparams.get('gamma', 0.99),
            'gae_lambda': hyperparams.get('gae_lambda', 1.0),
            'ent_coef': hyperparams.get('ent_coef', 0.01),
            'vf_coef': hyperparams.get('vf_coef', 0.25),
            'max_grad_norm': hyperparams.get('max_grad_norm', 0.5),
            'rms_prop_eps': hyperparams.get('rms_prop_eps', 1e-5),
            'normalize_advantage': hyperparams.get('normalize_advantage', False),
            'policy_kwargs': policy_kwargs,
            'device': device,
            'seed': config.get('seed', None)
        }
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """åˆ›å»ºA2Cæ¨¡å‹"""
        self.model = A2C(
            env=env,
            tensorboard_log=tensorboard_log,
            verbose=verbose,
            **self.hyperparams
        )
        return self.model
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """è®­ç»ƒA2C"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """é¢„æµ‹åŠ¨ä½œ"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        self.model.save(path)
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        self.model = A2C.load(path, device=self.device)
        return self.model


================================================================================
FILE: src/algorithms/base_algorithm.py
================================================================================
"""
ç®—æ³•åŸºç±»
æä¾›ç»Ÿä¸€çš„æ¥å£
"""

from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym


class BaseAlgorithm(ABC):
    """ç®—æ³•åŸºç±»"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        """
        åˆå§‹åŒ–ç®—æ³•
        
        Args:
            observation_space: è§‚å¯Ÿç©ºé—´
            action_space: åŠ¨ä½œç©ºé—´
            config: ç®—æ³•é…ç½®
            device: è®¡ç®—è®¾å¤‡
        """
        self.observation_space = observation_space
        self.action_space = action_space
        self.config = config
        self.device = device
        self.model = None
    
    @abstractmethod
    def train(
        self,
        env,
        total_timesteps: int,
        callback=None,
        **kwargs
    ):
        """
        è®­ç»ƒç®—æ³•
        
        Args:
            env: è®­ç»ƒç¯å¢ƒ
            total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
            callback: å›è°ƒå‡½æ•°
            **kwargs: å…¶ä»–å‚æ•°
        """
        pass
    
    @abstractmethod
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """
        é¢„æµ‹åŠ¨ä½œ
        
        Args:
            observation: è§‚å¯Ÿ
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        
        Returns:
            (action, state) å…ƒç»„
        """
        pass
    
    @abstractmethod
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        pass
    
    @abstractmethod
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        pass
    
    @abstractmethod
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """åˆ›å»ºæ¨¡å‹"""
        pass
    
    def get_name(self) -> str:
        """è·å–ç®—æ³•åç§°"""
        return self.config.get('algorithm', {}).get('name', 'UNKNOWN')


================================================================================
FILE: src/algorithms/ppo_wrapper.py
================================================================================
"""
PPOç®—æ³•å°è£…
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym
from stable_baselines3 import PPO
import torch

from .base_algorithm import BaseAlgorithm


class PPOWrapper(BaseAlgorithm):
    """PPOç®—æ³•åŒ…è£…å™¨"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        super().__init__(observation_space, action_space, config, device)
        
        # æå–è¶…å‚æ•°
        hyperparams = config.get('hyperparameters', {})
        
        # å¤„ç†policy_kwargs
        policy_kwargs = hyperparams.get('policy_kwargs', {}).copy()
        if 'activation_fn' in policy_kwargs:
            activation_str = policy_kwargs['activation_fn']
            if activation_str == "torch.nn.ReLU":
                policy_kwargs['activation_fn'] = torch.nn.ReLU
            elif activation_str == "torch.nn.Tanh":
                policy_kwargs['activation_fn'] = torch.nn.Tanh
        
        self.hyperparams = {
            'policy': hyperparams.get('policy', 'MlpPolicy'),
            'learning_rate': hyperparams.get('learning_rate', 3e-4),
            'n_steps': hyperparams.get('n_steps', 2048),
            'batch_size': hyperparams.get('batch_size', 64),
            'n_epochs': hyperparams.get('n_epochs', 10),
            'gamma': hyperparams.get('gamma', 0.99),
            'gae_lambda': hyperparams.get('gae_lambda', 0.95),
            'clip_range': hyperparams.get('clip_range', 0.2),
            'clip_range_vf': hyperparams.get('clip_range_vf', None),
            'ent_coef': hyperparams.get('ent_coef', 0.01),
            'vf_coef': hyperparams.get('vf_coef', 0.5),
            'max_grad_norm': hyperparams.get('max_grad_norm', 0.5),
            'normalize_advantage': hyperparams.get('normalize_advantage', True),
            'target_kl': hyperparams.get('target_kl', None),
            'policy_kwargs': policy_kwargs,
            'device': device,
            'seed': config.get('seed', None)
        }
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """åˆ›å»ºPPOæ¨¡å‹ - å…³é”®ä¿®å¤ï¼šç›´æ¥ä¼ å…¥envï¼Œè®©SB3è‡ªåŠ¨è·å–ç©ºé—´"""
        self.model = PPO(
            env=env,  # SB3ä¼šä»envè‡ªåŠ¨è·å–observation_spaceå’Œaction_space
            tensorboard_log=tensorboard_log,
            verbose=verbose,
            **self.hyperparams
        )
        return self.model
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """è®­ç»ƒPPO"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨ create_model()")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """é¢„æµ‹åŠ¨ä½œ"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        self.model.save(path)
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        self.model = PPO.load(path, device=self.device)
        return self.model


================================================================================
FILE: src/algorithms/random_policy.py
================================================================================
"""
Randomç­–ç•¥ï¼ˆBaselineï¼‰
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym

from .base_algorithm import BaseAlgorithm


class RandomPolicy(BaseAlgorithm):
    """éšæœºç­–ç•¥ï¼ˆBaselineï¼‰"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "cpu"
    ):
        super().__init__(observation_space, action_space, config, device)
        self.model = self  # Randomç­–ç•¥è‡ªå·±å°±æ˜¯æ¨¡å‹
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """Randomç­–ç•¥ä¸éœ€è¦è®­ç»ƒ"""
        print("âš ï¸  Randomç­–ç•¥ä¸éœ€è¦è®­ç»ƒï¼Œè·³è¿‡è®­ç»ƒæ­¥éª¤")
        pass
    
    def predict(
        self,
        observation: np.ndarray,
        state: Optional[np.ndarray] = None,
        episode_start: Optional[np.ndarray] = None,  # â† æ–°å¢å‚æ•°
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        éšæœºé‡‡æ ·åŠ¨ä½œ
        
        Args:
            observation: è§‚å¯Ÿï¼ˆæœªä½¿ç”¨ï¼‰
            state: RNN çŠ¶æ€ï¼ˆRandomç­–ç•¥ä¸ä½¿ç”¨ï¼‰
            episode_start: Episodeå¼€å§‹æ ‡å¿—ï¼ˆRandomç­–ç•¥ä¸ä½¿ç”¨ï¼‰
            deterministic: æ˜¯å¦ç¡®å®šæ€§ï¼ˆæœªä½¿ç”¨ï¼‰
        
        Returns:
            (action, state) å…ƒç»„
        """
        action = self.action_space.sample()
        return action, None
    
    def save(self, path: str):
        """Randomç­–ç•¥ä¸éœ€è¦ä¿å­˜"""
        print(f"â„¹ï¸  Randomç­–ç•¥ä¸éœ€è¦ä¿å­˜æ¨¡å‹æ–‡ä»¶: {path}")
        pass
    
    def load(self, path: str):
        """Randomç­–ç•¥ä¸éœ€è¦åŠ è½½"""
        print(f"â„¹ï¸  Randomç­–ç•¥ä¸éœ€è¦åŠ è½½æ¨¡å‹æ–‡ä»¶: {path}")
        return self
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """Randomç­–ç•¥ä¸éœ€è¦åˆ›å»ºæ¨¡å‹"""
        self.model = self
        return self


================================================================================
FILE: src/algorithms/sac_wrapper.py
================================================================================
"""
SACç®—æ³•å°è£…
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym
from stable_baselines3 import SAC
import torch

from .base_algorithm import BaseAlgorithm


class SACWrapper(BaseAlgorithm):
    """SACç®—æ³•åŒ…è£…å™¨"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        super().__init__(observation_space, action_space, config, device)
        
        hyperparams = config.get('hyperparameters', {})
        policy_kwargs = hyperparams.get('policy_kwargs', {}).copy()
        
        if 'activation_fn' in policy_kwargs:
            activation_str = policy_kwargs['activation_fn']
            if activation_str == "torch.nn.ReLU":
                policy_kwargs['activation_fn'] = torch.nn.ReLU
            elif activation_str == "torch.nn.Tanh":
                policy_kwargs['activation_fn'] = torch.nn.Tanh
        
        self.hyperparams = {
            'policy': hyperparams.get('policy', 'MlpPolicy'),
            'learning_rate': hyperparams.get('learning_rate', 3e-4),
            'buffer_size': hyperparams.get('buffer_size', 1000000),
            'learning_starts': hyperparams.get('learning_starts', 10000),
            'batch_size': hyperparams.get('batch_size', 256),
            'tau': hyperparams.get('tau', 0.005),
            'gamma': hyperparams.get('gamma', 0.99),
            'train_freq': hyperparams.get('train_freq', 1),
            'gradient_steps': hyperparams.get('gradient_steps', 1),
            'ent_coef': hyperparams.get('ent_coef', 'auto'),
            'target_entropy': hyperparams.get('target_entropy', 'auto'),
            'use_sde': hyperparams.get('use_sde', False),
            'sde_sample_freq': hyperparams.get('sde_sample_freq', -1),
            'policy_kwargs': policy_kwargs,
            'device': device,
            'seed': config.get('seed', None)
        }
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """åˆ›å»ºSACæ¨¡å‹"""
        self.model = SAC(
            env=env,
            tensorboard_log=tensorboard_log,
            verbose=verbose,
            **self.hyperparams
        )
        return self.model
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """è®­ç»ƒSAC"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """é¢„æµ‹åŠ¨ä½œ"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        self.model.save(path)
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        self.model = SAC.load(path, device=self.device)
        return self.model


================================================================================
FILE: src/algorithms/td3_wrapper.py
================================================================================
"""
TD3ç®—æ³•å°è£…
"""

from typing import Dict, Any, Optional, Tuple
import numpy as np
import gymnasium as gym
from stable_baselines3 import TD3
from stable_baselines3.common.noise import NormalActionNoise
import torch

from .base_algorithm import BaseAlgorithm


class TD3Wrapper(BaseAlgorithm):
    """TD3ç®—æ³•åŒ…è£…å™¨"""
    
    def __init__(
        self,
        observation_space: gym.Space,
        action_space: gym.Space,
        config: Dict[str, Any],
        device: str = "auto"
    ):
        super().__init__(observation_space, action_space, config, device)
        
        hyperparams = config.get('hyperparameters', {})
        policy_kwargs = hyperparams.get('policy_kwargs', {}).copy()
        
        if 'activation_fn' in policy_kwargs:
            activation_str = policy_kwargs['activation_fn']
            if activation_str == "torch.nn.ReLU":
                policy_kwargs['activation_fn'] = torch.nn.ReLU
            elif activation_str == "torch.nn.Tanh":
                policy_kwargs['activation_fn'] = torch.nn.Tanh
        
        # å¤„ç†åŠ¨ä½œå™ªå£°
        action_noise = None
        if hyperparams.get('action_noise') == 'normal':
            noise_kwargs = hyperparams.get('action_noise_kwargs', {})
            n_actions = action_space.shape[0]
            action_noise = NormalActionNoise(
                mean=np.zeros(n_actions) + noise_kwargs.get('mean', 0.0),
                sigma=np.ones(n_actions) * noise_kwargs.get('sigma', 0.1)
            )
        
        self.hyperparams = {
            'policy': hyperparams.get('policy', 'MlpPolicy'),
            'learning_rate': hyperparams.get('learning_rate', 3e-4),
            'buffer_size': hyperparams.get('buffer_size', 1000000),
            'learning_starts': hyperparams.get('learning_starts', 10000),
            'batch_size': hyperparams.get('batch_size', 256),
            'tau': hyperparams.get('tau', 0.005),
            'gamma': hyperparams.get('gamma', 0.99),
            'train_freq': hyperparams.get('train_freq', 1),
            'gradient_steps': hyperparams.get('gradient_steps', 1),
            'policy_delay': hyperparams.get('policy_delay', 2),
            'target_policy_noise': hyperparams.get('target_policy_noise', 0.2),
            'target_noise_clip': hyperparams.get('target_noise_clip', 0.5),
            'action_noise': action_noise,
            'policy_kwargs': policy_kwargs,
            'device': device,
            'seed': config.get('seed', None)
        }
    
    def create_model(self, env, tensorboard_log: Optional[str] = None, verbose: int = 1):
        """åˆ›å»ºTD3æ¨¡å‹"""
        self.model = TD3(
            env=env,
            tensorboard_log=tensorboard_log,
            verbose=verbose,
            **self.hyperparams
        )
        return self.model
    
    def train(self, env, total_timesteps: int, callback=None, **kwargs):
        """è®­ç»ƒTD3"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        self.model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            **kwargs
        )
    
    def predict(
        self,
        observation: np.ndarray,
        deterministic: bool = False
    ) -> Tuple[np.ndarray, Optional[Any]]:
        """é¢„æµ‹åŠ¨ä½œ"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        return self.model.predict(observation, deterministic=deterministic)
    
    def save(self, path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")
        
        self.model.save(path)
    
    def load(self, path: str):
        """åŠ è½½æ¨¡å‹"""
        self.model = TD3.load(path, device=self.device)
        return self.model


================================================================================
FILE: src/evaluation/__init__.py
================================================================================


================================================================================
FILE: src/utils/__init__.py
================================================================================


================================================================================
FILE: src/utils/banner.py
================================================================================
"""
è¿è¡Œæ¨¡å¼æ¨ªå¹…æ˜¾ç¤º
æé†’ç”¨æˆ·å½“å‰è¿è¡Œæ¨¡å¼
"""

import sys
from typing import Dict, Any


def print_mode_banner(run_mode: str, config: Dict[str, Any]):
    """
    æ‰“å°è¿è¡Œæ¨¡å¼æ¨ªå¹…
    
    Args:
        run_mode: è¿è¡Œæ¨¡å¼
        config: æ¨¡å¼é…ç½®
    """
    
    if run_mode == "debug":
        print("\n" + "="*70)
        print("ğŸ› DEBUG MODE - è°ƒè¯•æ¨¡å¼")
        print("="*70)
        print(f"  è®­ç»ƒæ­¥æ•°: {config.get('total_timesteps', 'N/A')}")
        print(f"  å¹¶è¡Œç¯å¢ƒ: {config.get('n_envs', 'N/A')}")
        print(f"  ä¿å­˜æ¨¡å‹: {'å¦' if not config.get('save_final_model', False) else 'æ˜¯'}")
        print(f"  è¾“å‡ºç›®å½•: {config.get('output_base_dir', 'N/A')}")
        print(f"  TensorBoard: {'å¯ç”¨' if config.get('tensorboard_enabled', False) else 'ç¦ç”¨'}")
        print("\n  âš ï¸  æ­¤æ¨¡å¼æ•°æ®å°†è¢«å®šæœŸæ¸…ç†ï¼Œä¸ç”¨äºæ­£å¼å®éªŒï¼")
        print("="*70 + "\n")
    
    elif run_mode == "dryrun":
        print("\n" + "="*70)
        print("ğŸ§ª DRYRUN MODE - é¢„æ¼”æ¨¡å¼")
        print("="*70)
        print(f"  è®­ç»ƒæ­¥æ•°: {config.get('total_timesteps', 'N/A')}")
        print(f"  å¹¶è¡Œç¯å¢ƒ: {config.get('n_envs', 'N/A')}")
        print(f"  ä¿å­˜æ¨¡å‹: æ˜¯ï¼ˆæ ‡è®°ä¸ºDRYRUNï¼‰")
        print(f"  è¾“å‡ºç›®å½•: {config.get('output_base_dir', 'N/A')}")
        print(f"  TensorBoard: {'å¯ç”¨' if config.get('tensorboard_enabled', False) else 'ç¦ç”¨'}")
        print("\n  â„¹ï¸  ç”¨äºéªŒè¯å®Œæ•´æµç¨‹ï¼Œæ•°æ®ä¿ç•™æœ€è¿‘ {config.get('max_runs', 3)} æ¬¡")
        print("="*70 + "\n")
    elif run_mode == "test":  # âœ… æ–°å¢
        print("\n" + "="*70)
        print("ğŸ§ª TEST MODE - å¿«é€Ÿæµç¨‹æµ‹è¯•")
        print("="*70)
        print(f"  è®­ç»ƒæ­¥æ•°: {config.get('total_timesteps', 'N/A')}")
        print(f"  å¹¶è¡Œç¯å¢ƒ: {config.get('n_envs', 'N/A')}")
        print(f"  ä¿å­˜æ¨¡å‹: æ˜¯ï¼ˆæ ‡è®°ä¸ºTESTï¼‰")
        print(f"  è¾“å‡ºç›®å½•: {config.get('output_base_dir', 'N/A')}")
        print(f"  TensorBoard: {'å¯ç”¨' if config.get('tensorboard_enabled', False) else 'ç¦ç”¨'}")
        print("\n  â„¹ï¸  ç”¨äºæµ‹è¯•å®Œæ•´è®­ç»ƒæµç¨‹ï¼Œæ•°æ®ä¿ç•™åœ¨ test_outputs/")
        print("="*70 + "\n")    
    else:  # production
        print("\n" + "="*70)
        print("âœ… PRODUCTION MODE - ç”Ÿäº§æ¨¡å¼")
        print("="*70)
        print(f"  è®­ç»ƒæ­¥æ•°: {config.get('total_timesteps', 'N/A')}")
        print(f"  å¹¶è¡Œç¯å¢ƒ: {config.get('n_envs', 'N/A')}")
        print(f"  ä¿å­˜æ¨¡å‹: æ˜¯")
        print(f"  è¾“å‡ºç›®å½•: {config.get('output_base_dir', 'N/A')}")
        print(f"  TensorBoard: å¯ç”¨")
        print("\n  ğŸš¨ æ‰€æœ‰æ•°æ®å°†è¢«æ°¸ä¹…ä¿å­˜ï¼Œè¯·ç¡®è®¤é…ç½®æ— è¯¯ï¼")
        print("="*70 + "\n")
        
        # ç”Ÿäº§æ¨¡å¼éœ€è¦ç¡®è®¤
        if config.get("require_confirmation", True):
            response = input("ç¡®è®¤å¼€å§‹æ­£å¼å®éªŒï¼Ÿ(yes/no): ").strip().lower()
            if response not in ['yes', 'y']:
                print("âŒ å·²å–æ¶ˆ")
                sys.exit(0)


def print_training_start(
    algo: str,
    side: str,
    version: str,
    opponent_info: str
):
    """
    æ‰“å°è®­ç»ƒå¼€å§‹ä¿¡æ¯
    
    Args:
        algo: ç®—æ³•åç§°
        side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
        version: ç‰ˆæœ¬å·
        opponent_info: å¯¹æ‰‹ä¿¡æ¯
    """
    print("\n" + "="*70)
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ")
    print("="*70)
    print(f"  ç®—æ³•: {algo}")
    print(f"  è§’è‰²: {side}")
    print(f"  ç‰ˆæœ¬: {version}")
    print(f"  å¯¹æ‰‹: {opponent_info}")
    print("="*70 + "\n")


def print_training_complete(
    algo: str,
    side: str,
    total_steps: int,
    time_elapsed: float
):
    """
    æ‰“å°è®­ç»ƒå®Œæˆä¿¡æ¯
    
    Args:
        algo: ç®—æ³•åç§°
        side: è®­ç»ƒæ–¹
        total_steps: æ€»è®­ç»ƒæ­¥æ•°
        time_elapsed: è®­ç»ƒè€—æ—¶ï¼ˆç§’ï¼‰
    """
    hours = int(time_elapsed // 3600)
    minutes = int((time_elapsed % 3600) // 60)
    seconds = int(time_elapsed % 60)
    
    print("\n" + "="*70)
    print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"  ç®—æ³•: {algo}")
    print(f"  è§’è‰²: {side}")
    print(f"  æ€»æ­¥æ•°: {total_steps:,}")
    print(f"  è€—æ—¶: {hours:02d}:{minutes:02d}:{seconds:02d}")
    print("="*70 + "\n")


def print_evaluation_start(n_episodes: int):
    """æ‰“å°è¯„ä¼°å¼€å§‹ä¿¡æ¯"""
    print("\n" + "-"*70)
    print(f"ğŸ“Š å¼€å§‹è¯„ä¼° ({n_episodes} episodes)")
    print("-"*70)


def print_evaluation_results(metrics: Dict[str, Any]):
    """
    æ‰“å°è¯„ä¼°ç»“æœ
    
    Args:
        metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
    """
    print("\n" + "-"*70)
    print("ğŸ“Š è¯„ä¼°ç»“æœ")
    print("-"*70)
    
    for key, value in metrics.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.4f}")
        else:
            print(f"  {key}: {value}")
    
    print("-"*70 + "\n")


def print_freeze_decision(
    algo: str,
    side: str,
    is_frozen: bool,
    reason: str = ""
):
    """
    æ‰“å°å†»ç»“å†³ç­–
    
    Args:
        algo: ç®—æ³•åç§°
        side: è®­ç»ƒæ–¹
        is_frozen: æ˜¯å¦å†»ç»“
        reason: åŸå› è¯´æ˜
    """
    if is_frozen:
        print("\n" + "="*70)
        print(f"â„ï¸  æ¨¡å‹å†»ç»“ï¼š{algo}_{side}")
        print("="*70)
        print(f"  âœ… è¾¾åˆ°å†»ç»“æ ‡å‡†ï¼Œå·²åŠ å…¥å›ºå®šæ± ")
        if reason:
            print(f"  åŸå› : {reason}")
        print("="*70 + "\n")
    else:
        print("\n" + "="*70)
        print(f"âš ï¸  æ¨¡å‹æœªå†»ç»“ï¼š{algo}_{side}")
        print("="*70)
        print(f"  âŒ æœªè¾¾åˆ°å†»ç»“æ ‡å‡†")
        if reason:
            print(f"  åŸå› : {reason}")
        print("="*70 + "\n")

================================================================================
FILE: src/utils/cleanup.py
================================================================================
"""
æ¸…ç†å·¥å…·
ç®¡ç†è°ƒè¯•å’Œé¢„æ¼”æ•°æ®çš„è‡ªåŠ¨æ¸…ç†
"""

import shutil
from pathlib import Path
from datetime import datetime
from typing import List
import os


class OutputCleaner:
    """è¾“å‡ºæ¸…ç†å™¨"""
    
    @staticmethod
    def cleanup_debug_on_start(config: dict):
        """å¯åŠ¨æ—¶æ¸…ç†è°ƒè¯•ç›®å½•"""
        if not config.get("clear_on_start", False):
            return
        
        current_dir = Path("debug_outputs/current")
        
        if current_dir.exists():
            print(f"ğŸ—‘ï¸  æ¸…ç©ºè°ƒè¯•ç›®å½•: {current_dir}")
            shutil.rmtree(current_dir)
        
        current_dir.mkdir(parents=True, exist_ok=True)
    
    @staticmethod
    def archive_debug_run():
        """å½’æ¡£å½“å‰è°ƒè¯•ä¼šè¯"""
        current_dir = Path("debug_outputs/current")
        
        if not current_dir.exists() or not any(current_dir.iterdir()):
            print("â„¹ï¸  è°ƒè¯•ç›®å½•ä¸ºç©ºï¼Œè·³è¿‡å½’æ¡£")
            return
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_dir = Path(f"debug_outputs/archive/debug_{timestamp}")
        
        print(f"ğŸ“¦ å½’æ¡£è°ƒè¯•æ•°æ®åˆ°: {archive_dir}")
        archive_dir.parent.mkdir(parents=True, exist_ok=True)
        shutil.move(str(current_dir), str(archive_dir))
        
        # æ¸…ç†æ—§å½’æ¡£
        OutputCleaner.cleanup_old_debug_archives()
    
    @staticmethod
    def cleanup_old_debug_archives(max_archives: int = 5):
        """åˆ é™¤è¶…å‡ºé™åˆ¶çš„æ—§è°ƒè¯•å½’æ¡£"""
        archive_dir = Path("debug_outputs/archive")
        
        if not archive_dir.exists():
            return
        
        # è·å–æ‰€æœ‰å½’æ¡£ç›®å½•
        archives = sorted(
            [d for d in archive_dir.iterdir() if d.is_dir() and d.name.startswith("debug_")],
            key=lambda x: x.name,
            reverse=True
        )
        
        # åˆ é™¤è¶…å‡ºçš„
        for old_archive in archives[max_archives:]:
            print(f"ğŸ—‘ï¸  åˆ é™¤æ—§è°ƒè¯•å½’æ¡£: {old_archive.name}")
            shutil.rmtree(old_archive)
    
    @staticmethod
    def cleanup_old_dryruns(max_runs: int = 3):
        """æ¸…ç†æ—§çš„é¢„æ¼”æ•°æ®"""
        dryrun_dir = Path("dryrun_outputs")
        
        if not dryrun_dir.exists():
            return
        
        # è·å–æ‰€æœ‰è¿è¡Œç›®å½•
        runs = sorted(
            [d for d in dryrun_dir.iterdir() if d.is_dir() and d.name.startswith("run_")],
            key=lambda x: x.name,
            reverse=True
        )
        
        # åˆ é™¤è¶…å‡ºçš„
        for old_run in runs[max_runs:]:
            print(f"ğŸ—‘ï¸  åˆ é™¤æ—§é¢„æ¼”æ•°æ®: {old_run.name}")
            shutil.rmtree(old_run)
    
    @staticmethod
    def get_directory_size(path: Path) -> float:
        """
        è·å–ç›®å½•å¤§å°ï¼ˆMBï¼‰
        
        Args:
            path: ç›®å½•è·¯å¾„
        
        Returns:
            å¤§å°ï¼ˆMBï¼‰
        """
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                if os.path.exists(filepath):
                    total_size += os.path.getsize(filepath)
        
        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    
    @staticmethod
    def print_storage_summary():
        """æ‰“å°å­˜å‚¨ç©ºé—´ä½¿ç”¨æ‘˜è¦"""
        print("\n" + "="*70)
        print("ğŸ’¾ å­˜å‚¨ç©ºé—´ä½¿ç”¨æ‘˜è¦")
        print("="*70)
        
        directories = {
            "outputs": Path("outputs"),
            "dryrun_outputs": Path("dryrun_outputs"),
            "debug_outputs": Path("debug_outputs")
        }
        
        for name, path in directories.items():
            if path.exists():
                size = OutputCleaner.get_directory_size(path)
                print(f"{name:20s}: {size:>10.2f} MB")
            else:
                print(f"{name:20s}: {'ä¸å­˜åœ¨':>10s}")
        
        print("="*70 + "\n")


# ä¾¿æ·å‡½æ•°
def cleanup_debug(config: dict):
    """æ¸…ç†è°ƒè¯•æ•°æ®"""
    OutputCleaner.cleanup_debug_on_start(config)


def archive_debug():
    """å½’æ¡£è°ƒè¯•æ•°æ®"""
    OutputCleaner.archive_debug_run()


def cleanup_dryrun(max_runs: int = 3):
    """æ¸…ç†é¢„æ¼”æ•°æ®"""
    OutputCleaner.cleanup_old_dryruns(max_runs)

================================================================================
FILE: src/utils/config_loader.py
================================================================================
"""
é…ç½®åŠ è½½å·¥å…·
"""

import os
import yaml
from typing import Dict, Any, Optional
from pathlib import Path


class ConfigLoader:
    """é…ç½®åŠ è½½å™¨"""
    
    def __init__(self, config_root: str = "configs"):
        self.config_root = Path(config_root)
        
        # ç¼“å­˜å·²åŠ è½½çš„é…ç½®
        self._cache = {}
    
    def load_yaml(self, config_path: str) -> Dict[str, Any]:
        """
        åŠ è½½YAMLé…ç½®æ–‡ä»¶
        
        Args:
            config_path: ç›¸å¯¹äºconfig_rootçš„è·¯å¾„ï¼Œæˆ–ç»å¯¹è·¯å¾„
        
        Returns:
            é…ç½®å­—å…¸
        """
        # æ£€æŸ¥ç¼“å­˜
        if config_path in self._cache:
            return self._cache[config_path]
        
        # æ„å»ºå®Œæ•´è·¯å¾„
        if os.path.isabs(config_path):
            full_path = Path(config_path)
        else:
            full_path = self.config_root / config_path
        
        # åŠ è½½YAML
        if not full_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {full_path}")
        
        with open(full_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        # ç¼“å­˜
        self._cache[config_path] = config
        
        return config
    
    def load_run_mode_config(self, mode: str) -> Dict[str, Any]:
        """åŠ è½½è¿è¡Œæ¨¡å¼é…ç½®"""
        config = self.load_yaml("run_modes.yaml")
        
        if mode not in config:
            raise ValueError(f"æœªçŸ¥çš„è¿è¡Œæ¨¡å¼: {mode}. æ”¯æŒçš„æ¨¡å¼: {list(config.keys())}")
        
        return config[mode]
    
    def load_environment_config(self, env_name: str = "waterworld_standard") -> Dict[str, Any]:
        """åŠ è½½ç¯å¢ƒé…ç½®"""
        return self.load_yaml(f"environments/{env_name}.yaml")
    
    def load_algorithm_config(self, algo_name: str) -> Dict[str, Any]:
        """åŠ è½½ç®—æ³•é…ç½®"""
        return self.load_yaml(f"algorithms/{algo_name.lower()}.yaml")
    
    def load_training_config(self, stage_name: str) -> Dict[str, Any]:
        """åŠ è½½è®­ç»ƒé˜¶æ®µé…ç½®"""
        return self.load_yaml(f"training/{stage_name}.yaml")
    
    def get_freeze_criteria(self, side: str) -> Dict[str, Any]:
        """è·å–å†»ç»“æ¡ä»¶"""
        config = self.load_yaml("run_modes.yaml")
        return config['freeze_criteria'][side]
    
    def get_fixed_pool_config(self) -> Dict[str, Any]:
        """è·å–å›ºå®šæ± é…ç½®"""
        config = self.load_yaml("run_modes.yaml")
        return config['fixed_pool']
    
    def merge_configs(self, *configs: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶å¤šä¸ªé…ç½®ï¼ˆåé¢çš„è¦†ç›–å‰é¢çš„ï¼‰
        
        Args:
            *configs: å¤šä¸ªé…ç½®å­—å…¸
        
        Returns:
            åˆå¹¶åçš„é…ç½®
        """
        merged = {}
        
        for config in configs:
            merged = self._deep_merge(merged, config)
        
        return merged
    
    def _deep_merge(self, base: Dict, update: Dict) -> Dict:
        """æ·±åº¦åˆå¹¶å­—å…¸"""
        result = base.copy()
        
        for key, value in update.items():
            if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = self._deep_merge(result[key], value)
            else:
                result[key] = value
        
        return result


# å…¨å±€é…ç½®åŠ è½½å™¨å®ä¾‹
config_loader = ConfigLoader()


# ä¾¿æ·å‡½æ•°
def load_config(config_path: str) -> Dict[str, Any]:
    """åŠ è½½é…ç½®çš„ä¾¿æ·å‡½æ•°"""
    return config_loader.load_yaml(config_path)


def get_mode_config(mode: str) -> Dict[str, Any]:
    """è·å–è¿è¡Œæ¨¡å¼é…ç½®"""
    return config_loader.load_run_mode_config(mode)


def get_env_config(env_name: str = "waterworld_standard") -> Dict[str, Any]:
    """è·å–ç¯å¢ƒé…ç½®"""
    return config_loader.load_environment_config(env_name)


def get_algo_config(algo_name: str) -> Dict[str, Any]:
    """è·å–ç®—æ³•é…ç½®"""
    return config_loader.load_algorithm_config(algo_name)


def get_training_config(stage_name: str) -> Dict[str, Any]:
    """è·å–è®­ç»ƒé…ç½®"""
    return config_loader.load_training_config(stage_name)




# from src.utils.config_loader import get_mode_config, get_env_config, get_algo_config

# # åŠ è½½è¿è¡Œæ¨¡å¼é…ç½®
# debug_config = get_mode_config("debug")
# print(f"Debugæ¨¡å¼è®­ç»ƒæ­¥æ•°: {debug_config['total_timesteps']}")

# # åŠ è½½ç¯å¢ƒé…ç½®
# env_config = get_env_config("waterworld_standard")
# print(f"Predatoræ•°é‡: {env_config['environment']['n_predators']}")

# # åŠ è½½ç®—æ³•é…ç½®
# ppo_config = get_algo_config("PPO")
# print(f"PPOå­¦ä¹ ç‡: {ppo_config['hyperparameters']['learning_rate']}")

# # åˆå¹¶é…ç½®
# from src.utils.config_loader import config_loader

# final_config = config_loader.merge_configs(
#     debug_config,
#     {"total_timesteps": 5000}  # è¦†ç›–é»˜è®¤å€¼
# )
# print(f"æœ€ç»ˆè®­ç»ƒæ­¥æ•°: {final_config['total_timesteps']}")

================================================================================
FILE: src/utils/config_snapshot.py
================================================================================
"""
é…ç½®å¿«ç…§å·¥å…·
ä¿å­˜æ¯æ¬¡è®­ç»ƒçš„å®Œæ•´é…ç½®ï¼Œç¡®ä¿å¯å¤ç°
"""

import json
import yaml
from pathlib import Path
from datetime import datetime
from typing import Dict, Any


def make_json_serializable(obj):
    """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
    if isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [make_json_serializable(v) for v in obj]
    elif isinstance(obj, tuple):
        return tuple(make_json_serializable(v) for v in obj)
    elif isinstance(obj, type):
        # å°†ç±»å‹å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return f"{obj.__module__}.{obj.__name__}"
    elif hasattr(obj, '__class__') and obj.__class__.__module__ not in ['builtins', '__builtin__']:
        # å¤æ‚å¯¹è±¡è½¬ä¸ºå­—ç¬¦ä¸²è¡¨ç¤º
        return str(obj)
    else:
        return obj


class ConfigSnapshot:
    """é…ç½®å¿«ç…§ç®¡ç†å™¨"""
    
    @staticmethod
    def save_snapshot(config: Dict[str, Any], save_dir: Path, filename: str):
        save_dir.mkdir(parents=True, exist_ok=True)
        
        snapshot = {
            'timestamp': datetime.now().isoformat(),
            'config': make_json_serializable(config)
        }
        
        # ä¿å­˜ä¸ºYAML
        yaml_path = save_dir / f"{filename}.yaml"
        with open(yaml_path, 'w', encoding='utf-8') as f:
            yaml.dump(snapshot, f, default_flow_style=False, allow_unicode=True)
        
        # å°è¯•ä¿å­˜ä¸ºJSON
        json_path = save_dir / f"{filename}.json"
        try:
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(snapshot, f, indent=2, ensure_ascii=False)
        except TypeError as e:
            print(f"âš ï¸  JSONåºåˆ—åŒ–å¤±è´¥ï¼Œä»…ä¿å­˜YAML: {e}")
        
        print(f"ğŸ’¾ é…ç½®å¿«ç…§å·²ä¿å­˜: {yaml_path}")
    
    @staticmethod
    def load_snapshot(snapshot_path: Path) -> Dict[str, Any]:
        if snapshot_path.suffix == '.yaml':
            with open(snapshot_path, 'r', encoding='utf-8') as f:
                snapshot = yaml.safe_load(f)
        elif snapshot_path.suffix == '.json':
            with open(snapshot_path, 'r', encoding='utf-8') as f:
                snapshot = json.load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {snapshot_path.suffix}")
        
        return snapshot.get('config', {})
    
    @staticmethod
    def save_training_summary(summary: Dict[str, Any], save_dir: Path, filename: str):
        save_dir.mkdir(parents=True, exist_ok=True)
        
        summary['saved_at'] = datetime.now().isoformat()
        serializable_summary = make_json_serializable(summary)
        
        json_path = save_dir / f"{filename}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ è®­ç»ƒæ‘˜è¦å·²ä¿å­˜: {json_path}")


def save_config_snapshot(config: Dict, save_dir: Path, name: str):
    ConfigSnapshot.save_snapshot(config, save_dir, name)


def save_training_summary(summary: Dict, save_dir: Path, name: str):
    ConfigSnapshot.save_training_summary(summary, save_dir, name)


================================================================================
FILE: src/utils/config_validator.py
================================================================================
"""
é…ç½®éªŒè¯å·¥å…·
é˜²æ­¢é…ç½®é”™è¯¯å¯¼è‡´è®­ç»ƒå¤±è´¥
"""

import sys
from typing import Dict, Any, List
from src.utils.config_loader import config_loader


class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""
    
    def __init__(self):
        self.errors = []
        self.warnings = []
    
    def validate_run_mode(self, mode: str, config: Dict[str, Any]) -> bool:
        """
        éªŒè¯è¿è¡Œæ¨¡å¼é…ç½®
        
        Args:
            mode: è¿è¡Œæ¨¡å¼ (debug/dryrun/prod)
            config: é…ç½®å­—å…¸
        
        Returns:
            æ˜¯å¦é€šè¿‡éªŒè¯
        """
        self.errors = []
        self.warnings = []
        
        # åŠ è½½éªŒè¯è§„åˆ™
        validation_rules = config_loader.load_yaml("run_modes.yaml").get("validation", {})
        
        if mode == "prod":
            self._validate_production_mode(config, validation_rules.get("prod", {}))
        elif mode == "debug":
            self._validate_debug_mode(config, validation_rules.get("debug", {}))
        
        return len(self.errors) == 0
    
    def _validate_production_mode(self, config: Dict, rules: Dict):
        """éªŒè¯ç”Ÿäº§æ¨¡å¼"""
        
        # æ£€æŸ¥è®­ç»ƒæ­¥æ•°
        min_timesteps = rules.get("min_timesteps", 500000)
        if config.get("total_timesteps", 0) < min_timesteps:
            self.errors.append(
                f"âš ï¸  ç”Ÿäº§æ¨¡å¼è®­ç»ƒæ­¥æ•°è¿‡ä½: {config['total_timesteps']} "
                f"(å»ºè®®è‡³å°‘ {min_timesteps} æ­¥)"
            )
        
        # æ£€æŸ¥å®éªŒåç§°
        if "experiment_name" in config:
            exp_name = config["experiment_name"].lower()
            forbidden = rules.get("forbidden_keywords", [])
            
            for keyword in forbidden:
                if keyword in exp_name:
                    self.warnings.append(
                        f"âš ï¸  å®éªŒåç§° '{config['experiment_name']}' åŒ…å« '{keyword}' "
                        "å­—æ ·ï¼Œç¡®è®¤è¿™æ˜¯æ­£å¼å®éªŒå—ï¼Ÿ"
                    )
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†ä¿å­˜
        if not config.get("save_final_model", True):
            self.warnings.append(
                "âš ï¸  ç”Ÿäº§æ¨¡å¼æœªå¯ç”¨æ¨¡å‹ä¿å­˜ï¼Œè¿™å¯èƒ½ä¸æ˜¯ä½ æƒ³è¦çš„ï¼"
            )
    
    def _validate_debug_mode(self, config: Dict, rules: Dict):
        """éªŒè¯è°ƒè¯•æ¨¡å¼"""
        
        # æ£€æŸ¥è®­ç»ƒæ­¥æ•°æ˜¯å¦è¿‡å¤š
        max_timesteps = rules.get("max_timesteps", 10000)
        if config.get("total_timesteps", 0) > max_timesteps:
            self.warnings.append(
                f"ğŸ’¡ è°ƒè¯•æ¨¡å¼è®­ç»ƒæ­¥æ•°è¾ƒå¤š ({config['total_timesteps']})ï¼Œ"
                "å¯èƒ½è€—æ—¶è¾ƒé•¿ï¼Œè€ƒè™‘é™ä½æ­¥æ•°ï¼Ÿ"
            )
    
    def validate_environment_config(self, config: Dict[str, Any]) -> bool:
        """éªŒè¯ç¯å¢ƒé…ç½®"""
        self.errors = []
        self.warnings = []
        
        env_config = config.get("environment", {})
        
        # å¿…éœ€å­—æ®µ
        required_fields = [
            "n_predators", "n_preys", "max_cycles",
            "predator_speed", "prey_speed"
        ]
        
        for field in required_fields:
            if field not in env_config:
                self.errors.append(f"âŒ ç¯å¢ƒé…ç½®ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # åˆç†æ€§æ£€æŸ¥
        if env_config.get("n_predators", 0) > env_config.get("n_preys", 0):
            self.warnings.append(
                "âš ï¸  Predatoræ•°é‡å¤šäºPreyï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸å¹³è¡¡"
            )
        
        if env_config.get("predator_speed", 0) < env_config.get("prey_speed", 0):
            self.warnings.append(
                "âš ï¸  Predatoré€Ÿåº¦æ…¢äºPreyï¼Œå¯èƒ½éš¾ä»¥æ•è·"
            )
        
        return len(self.errors) == 0
    
    def validate_algorithm_config(self, algo_name: str, config: Dict[str, Any]) -> bool:
        """éªŒè¯ç®—æ³•é…ç½®"""
        self.errors = []
        self.warnings = []
        
        hyperparams = config.get("hyperparameters", {})
        
        # æ£€æŸ¥å­¦ä¹ ç‡
        lr = hyperparams.get("learning_rate")
        if lr is not None:
            if lr > 0.01:
                self.warnings.append(
                    f"âš ï¸  {algo_name} å­¦ä¹ ç‡è¾ƒé«˜ ({lr})ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š"
                )
            elif lr < 1e-6:
                self.warnings.append(
                    f"âš ï¸  {algo_name} å­¦ä¹ ç‡è¿‡ä½ ({lr})ï¼Œå¯èƒ½å­¦ä¹ ç¼“æ…¢"
                )
        
        return len(self.errors) == 0
    
    def validate_training_config(self, stage_config: Dict[str, Any]) -> bool:
        """éªŒè¯è®­ç»ƒé˜¶æ®µé…ç½®"""
        self.errors = []
        self.warnings = []
        
        # æ£€æŸ¥å¯¹æ‰‹é…ç½®
        opponent = stage_config.get("opponent", {})
        if not opponent:
            self.errors.append("âŒ ç¼ºå°‘å¯¹æ‰‹é…ç½®")
        else:
            opp_type = opponent.get("type")
            if opp_type == "mixed_pool":
                pool_path = opponent.get("pool_path")
                if not pool_path:
                    self.errors.append("âŒ mixed_poolæ¨¡å¼éœ€è¦æŒ‡å®špool_path")
        
        # æ£€æŸ¥è®­ç»ƒç®—æ³•åˆ—è¡¨
        algos = stage_config.get("algorithms_to_train", [])
        if not algos:
            self.errors.append("âŒ æœªæŒ‡å®šè¦è®­ç»ƒçš„ç®—æ³•")
        
        return len(self.errors) == 0
    
    def print_results(self):
        """æ‰“å°éªŒè¯ç»“æœ"""
        if self.errors:
            print("\nâŒ é…ç½®é”™è¯¯:")
            for err in self.errors:
                print(f"  {err}")
        
        if self.warnings:
            print("\nâš ï¸  é…ç½®è­¦å‘Š:")
            for warn in self.warnings:
                print(f"  {warn}")
    
    def require_confirmation(self) -> bool:
        """
        å¦‚æœæœ‰è­¦å‘Šï¼Œè¦æ±‚ç”¨æˆ·ç¡®è®¤
        
        Returns:
            ç”¨æˆ·æ˜¯å¦ç¡®è®¤ç»§ç»­
        """
        if not self.warnings:
            return True
        
        self.print_results()
        response = input("\nç»§ç»­å—ï¼Ÿ(yes/no): ").strip().lower()
        return response in ['yes', 'y']


# å…¨å±€éªŒè¯å™¨å®ä¾‹
validator = ConfigValidator()


def validate_config(mode: str, config: Dict[str, Any]) -> bool:
    """
    éªŒè¯é…ç½®çš„ä¾¿æ·å‡½æ•°
    
    Args:
        mode: è¿è¡Œæ¨¡å¼
        config: é…ç½®å­—å…¸
    
    Returns:
        æ˜¯å¦é€šè¿‡éªŒè¯
    """
    if not validator.validate_run_mode(mode, config):
        validator.print_results()
        return False
    
    return True

================================================================================
FILE: src/utils/logger.py
================================================================================
"""
æ—¥å¿—å·¥å…·
æä¾›ç»Ÿä¸€çš„æ—¥å¿—æ¥å£
"""

import logging
import sys
from pathlib import Path
from typing import Optional
from datetime import datetime


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—å™¨"""
    
    def __init__(
        self,
        name: str,
        log_file: Optional[Path] = None,
        log_level: str = "INFO",
        console_output: bool = True
    ):
        """
        åˆå§‹åŒ–æ—¥å¿—å™¨
        
        Args:
            name: æ—¥å¿—å™¨åç§°
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆNoneåˆ™ä¸å†™æ–‡ä»¶ï¼‰
            log_level: æ—¥å¿—çº§åˆ«ï¼ˆDEBUG/INFO/WARNING/ERRORï¼‰
            console_output: æ˜¯å¦è¾“å‡ºåˆ°æ§åˆ¶å°
        """
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, log_level.upper()))
        
        # æ¸…é™¤å·²æœ‰çš„handlers
        self.logger.handlers = []
        
        # æ—¥å¿—æ ¼å¼
        formatter = logging.Formatter(
            '[%(asctime)s] [%(levelname)s] %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # æ§åˆ¶å°è¾“å‡º
        if console_output:
            console_handler = logging.StreamHandler(sys.stdout)
            console_handler.setLevel(getattr(logging, log_level.upper()))
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
        
        # æ–‡ä»¶è¾“å‡º
        if log_file:
            log_file.parent.mkdir(parents=True, exist_ok=True)
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(getattr(logging, log_level.upper()))
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def debug(self, msg: str):
        """Debugçº§åˆ«æ—¥å¿—"""
        self.logger.debug(msg)
    
    def info(self, msg: str):
        """Infoçº§åˆ«æ—¥å¿—"""
        self.logger.info(msg)
    
    def warning(self, msg: str):
        """Warningçº§åˆ«æ—¥å¿—"""
        self.logger.warning(msg)
    
    def error(self, msg: str):
        """Errorçº§åˆ«æ—¥å¿—"""
        self.logger.error(msg)
    
    def critical(self, msg: str):
        """Criticalçº§åˆ«æ—¥å¿—"""
        self.logger.critical(msg)
    
    def log_config(self, config: dict, title: str = "Configuration"):
        """è®°å½•é…ç½®ä¿¡æ¯"""
        self.info(f"\n{'='*70}")
        self.info(f"{title}")
        self.info(f"{'='*70}")
        
        for key, value in config.items():
            if isinstance(value, dict):
                self.info(f"{key}:")
                for sub_key, sub_value in value.items():
                    self.info(f"  {sub_key}: {sub_value}")
            else:
                self.info(f"{key}: {value}")
        
        self.info(f"{'='*70}\n")
    
    def log_banner(self, text: str, char: str = "="):
        """æ‰“å°æ¨ªå¹…"""
        banner = char * 70
        self.info(f"\n{banner}")
        self.info(text)
        self.info(f"{banner}\n")


def create_logger(
    name: str,
    log_dir: Optional[Path] = None,
    log_level: str = "INFO"
) -> TrainingLogger:
    """
    åˆ›å»ºæ—¥å¿—å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        name: æ—¥å¿—å™¨åç§°
        log_dir: æ—¥å¿—ç›®å½•ï¼ˆNoneåˆ™ä¸å†™æ–‡ä»¶ï¼‰
        log_level: æ—¥å¿—çº§åˆ«
    
    Returns:
        æ—¥å¿—å™¨å®ä¾‹
    """
    if log_dir:
        log_file = log_dir / f"{name}.log"
    else:
        log_file = None
    
    return TrainingLogger(name, log_file, log_level)

================================================================================
FILE: src/utils/naming.py
================================================================================
"""
æ–‡ä»¶å‘½åè§„èŒƒå·¥å…·
ç¡®ä¿æ‰€æœ‰æ–‡ä»¶åéƒ½åŒ…å«å¿…è¦ä¿¡æ¯ä¸”æ ¼å¼ä¸€è‡´
"""

from datetime import datetime
from typing import Optional


class FileNaming:
    """æ–‡ä»¶å‘½åå·¥å…·"""
    
    @staticmethod
    def generate_model_filename(
        train_algo: str,
        train_side: str,
        version: str,
        opponent_info: str,
        run_mode: str,
        extension: str = "zip"
    ) -> str:
        """
        ç”Ÿæˆæ¨¡å‹æ–‡ä»¶å
        
        Args:
            train_algo: è®­ç»ƒç®—æ³•ï¼ˆå¦‚ PPOï¼‰
            train_side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
            version: ç‰ˆæœ¬å·ï¼ˆå¦‚ v1ï¼‰
            opponent_info: å¯¹æ‰‹ä¿¡æ¯ï¼ˆå¦‚ RANDOM_pred æˆ– MIX_pool_v1ï¼‰
            run_mode: è¿è¡Œæ¨¡å¼ï¼ˆdebug/dryrun/prodï¼‰
            extension: æ–‡ä»¶æ‰©å±•å
        
        Returns:
            æ–‡ä»¶å
        
        Example:
            PPO_prey_v1_vs_RANDOM_pred_20251013_143022.zip
            DRYRUN_SAC_pred_v2_vs_MIX_pool_v1_20251013_151033.zip
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ·»åŠ æ¨¡å¼å‰ç¼€
        if run_mode == "debug":
            prefix = "DEBUG_"
        elif run_mode == "dryrun":
            prefix = "DRYRUN_"
        else:
            prefix = ""
        
        # æ„å»ºæ–‡ä»¶å
        filename = (
            f"{prefix}{train_algo}_{train_side}_{version}"
            f"_vs_{opponent_info}_{timestamp}.{extension}"
        )
        
        return filename
    
    @staticmethod
    def generate_checkpoint_filename(
        train_algo: str,
        train_side: str,
        step: int,
        extension: str = "zip"
    ) -> str:
        """
        ç”Ÿæˆæ£€æŸ¥ç‚¹æ–‡ä»¶å
        
        Args:
            train_algo: è®­ç»ƒç®—æ³•
            train_side: è®­ç»ƒæ–¹
            step: è®­ç»ƒæ­¥æ•°
            extension: æ–‡ä»¶æ‰©å±•å
        
        Returns:
            æ–‡ä»¶å
        
        Example:
            PPO_prey_step_500000.zip
        """
        return f"{train_algo}_{train_side}_step_{step}.{extension}"
    
    @staticmethod
    def generate_config_filename(
        train_algo: str,
        train_side: str,
        version: str,
        extension: str = "yaml"
    ) -> str:
        """
        ç”Ÿæˆé…ç½®å¿«ç…§æ–‡ä»¶å
        
        Example:
            PPO_prey_v1_config.yaml
        """
        return f"{train_algo}_{train_side}_{version}_config.{extension}"
    
    @staticmethod
    def generate_log_filename(
        train_algo: str,
        train_side: str,
        version: str,
        extension: str = "log"
    ) -> str:
        """
        ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        
        Example:
            PPO_prey_v1_training.log
        """
        return f"{train_algo}_{train_side}_{version}_training.{extension}"
    
    @staticmethod
    def generate_summary_filename(
        train_algo: str,
        train_side: str,
        version: str,
        extension: str = "json"
    ) -> str:
        """
        ç”Ÿæˆè®­ç»ƒæ‘˜è¦æ–‡ä»¶å
        
        Example:
            PPO_prey_v1_summary.json
        """
        return f"{train_algo}_{train_side}_{version}_summary.{extension}"
    
    @staticmethod
    def parse_model_filename(filename: str) -> dict:
        """
        è§£ææ¨¡å‹æ–‡ä»¶åï¼Œæå–ä¿¡æ¯
        
        Args:
            filename: æ¨¡å‹æ–‡ä»¶å
        
        Returns:
            åŒ…å«è§£æä¿¡æ¯çš„å­—å…¸
        
        Example:
            Input: "PPO_prey_v1_vs_RANDOM_pred_20251013_143022.zip"
            Output: {
                'algo': 'PPO',
                'side': 'prey',
                'version': 'v1',
                'opponent': 'RANDOM_pred',
                'timestamp': '20251013_143022',
                'is_debug': False,
                'is_dryrun': False
            }
        """
        # å»é™¤æ‰©å±•å
        name = filename.rsplit('.', 1)[0]
        
        # æ£€æŸ¥æ¨¡å¼å‰ç¼€
        is_debug = name.startswith("DEBUG_")
        is_dryrun = name.startswith("DRYRUN_")
        
        if is_debug:
            name = name[6:]  # å»é™¤ "DEBUG_"
        elif is_dryrun:
            name = name[7:]  # å»é™¤ "DRYRUN_"
        
        # åˆ†å‰²å­—æ®µ
        parts = name.split('_')
        
        # åŸºæœ¬è§£æï¼ˆå‡è®¾æ ¼å¼æ­£ç¡®ï¼‰
        if len(parts) >= 6:
            return {
                'algo': parts[0],
                'side': parts[1],
                'version': parts[2],
                'opponent': '_'.join(parts[4:-2]),  # vsä¹‹ååˆ°æ—¶é—´æˆ³ä¹‹å‰
                'timestamp': '_'.join(parts[-2:]),
                'is_debug': is_debug,
                'is_dryrun': is_dryrun
            }
        else:
            return {}
    
    @staticmethod
    def format_opponent_info(opponent_config: dict) -> str:
        """
        æ ¹æ®å¯¹æ‰‹é…ç½®ç”Ÿæˆå¯¹æ‰‹ä¿¡æ¯å­—ç¬¦ä¸²
        
        Args:
            opponent_config: å¯¹æ‰‹é…ç½®å­—å…¸
        
        Returns:
            å¯¹æ‰‹ä¿¡æ¯å­—ç¬¦ä¸²
        
        Example:
            {"type": "algorithm", "algorithm": "RANDOM", "side": "predator"}
            -> "RANDOM_pred"
            
            {"type": "mixed_pool", "pool_path": "outputs/fixed_pools/prey_pool_v1"}
            -> "MIX_prey_pool_v1"
        """
        opp_type = opponent_config.get("type", "unknown")
        
        if opp_type == "algorithm":
            algo = opponent_config.get("algorithm", "UNKNOWN")
            side = opponent_config.get("side", "unknown")
            side_abbr = "pred" if side == "predator" else "prey"
            return f"{algo}_{side_abbr}"
        
        elif opp_type == "fixed_model":
            # ä»è·¯å¾„æå–æ¨¡å‹å
            path = opponent_config.get("path", "")
            model_name = path.split('/')[-1].replace('.zip', '')
            return model_name
        
        elif opp_type == "mixed_pool":
            # ä»æ± è·¯å¾„æå–æ± å
            pool_path = opponent_config.get("pool_path", "")
            pool_name = pool_path.split('/')[-1]
            return f"MIX_{pool_name}"
        
        else:
            return "UNKNOWN"


# å…¨å±€å‘½åå·¥å…·å®ä¾‹
naming = FileNaming()


# ä¾¿æ·å‡½æ•°
def generate_model_name(
    algo: str, side: str, version: str,
    opponent: dict, mode: str
) -> str:
    """ç”Ÿæˆæ¨¡å‹æ–‡ä»¶åçš„ä¾¿æ·å‡½æ•°"""
    opponent_info = naming.format_opponent_info(opponent)
    return naming.generate_model_filename(algo, side, version, opponent_info, mode)

================================================================================
FILE: src/utils/path_manager.py
================================================================================
"""
è·¯å¾„ç®¡ç†å·¥å…·
æ ¹æ®è¿è¡Œæ¨¡å¼å’Œå®éªŒé…ç½®ç”Ÿæˆæ­£ç¡®çš„è¾“å‡ºè·¯å¾„
"""

import os
from pathlib import Path
from datetime import datetime
from typing import Optional


class PathManager:
    """è·¯å¾„ç®¡ç†å™¨"""
    
    def __init__(self, run_mode: str, experiment_name: str):
        """
        åˆå§‹åŒ–è·¯å¾„ç®¡ç†å™¨
        
        Args:
            run_mode: è¿è¡Œæ¨¡å¼ (debug/dryrun/prod)
            experiment_name: å®éªŒåç§°
        """
        self.run_mode = run_mode
        self.experiment_name = experiment_name
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®åŸºç¡€ç›®å½•
        if run_mode == "debug":
            self.base_dir = Path("debug_outputs/current")
        elif run_mode == "dryrun":
            self.base_dir = Path(f"dryrun_outputs/run_{self.timestamp}")
        else:  # prod
            self.base_dir = Path("outputs")
        
        # åˆ›å»ºåŸºç¡€ç›®å½•
        self.base_dir.mkdir(parents=True, exist_ok=True)
    
    def get_model_dir(self, stage_name: Optional[str] = None) -> Path:
        """
        è·å–æ¨¡å‹ä¿å­˜ç›®å½•
        
        Args:
            stage_name: è®­ç»ƒé˜¶æ®µåç§°ï¼ˆå¦‚ stage1.1_prey_warmupï¼‰
        
        Returns:
            æ¨¡å‹ç›®å½•è·¯å¾„
        """
        if stage_name:
            path = self.base_dir / "saved_models" / stage_name
        else:
            path = self.base_dir / "saved_models" / self.experiment_name
        
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    def get_checkpoint_dir(self, stage_name: Optional[str] = None) -> Path:
        """è·å–æ£€æŸ¥ç‚¹ç›®å½•"""
        if stage_name:
            path = self.base_dir / "checkpoints" / stage_name / self.experiment_name
        else:
            path = self.base_dir / "checkpoints" / self.experiment_name
        
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    def get_tensorboard_dir(self, stage_name: Optional[str] = None) -> Path:
        """è·å–TensorBoardæ—¥å¿—ç›®å½•"""
        if stage_name:
            # åŒä¸€ stage çš„æ‰€æœ‰å®éªŒå…±äº«ä¸€ä¸ªç›®å½•
            path = self.base_dir / "tensorboard_logs" / stage_name
        else:
            path = self.base_dir / "tensorboard_logs" / self.experiment_name
        
        path.mkdir(parents=True, exist_ok=True)
        return path
    def get_experiment_dir(self, stage_name: Optional[str] = None) -> Path:
        """è·å–å®éªŒè®°å½•ç›®å½•"""
        if stage_name:
            path = self.base_dir / "experiments" / stage_name
        else:
            path = self.base_dir / "experiments" / self.experiment_name
        
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    def get_fixed_pool_dir(self, pool_name: str) -> Path:
        """
        è·å–å›ºå®šæ± ç›®å½•
        
        Args:
            pool_name: æ± åç§°ï¼ˆå¦‚ prey_pool_v1ï¼‰
        """
        # å›ºå®šæ± åªåœ¨æ­£å¼è¾“å‡ºç›®å½•
        path = Path("outputs/fixed_pools") / pool_name
        path.mkdir(parents=True, exist_ok=True)
        return path
    
    def get_evaluation_dir(self) -> Path:
        """è·å–è¯„ä¼°ç»“æœç›®å½•"""
        path = Path("outputs/evaluation_results")
        path.mkdir(parents=True, exist_ok=True)
        return path


def create_path_manager(run_mode: str, experiment_name: str) -> PathManager:
    """åˆ›å»ºè·¯å¾„ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•°"""
    return PathManager(run_mode, experiment_name)

================================================================================
FILE: dryrun_outputs/run_20251013_172831/experiments/baseline_stage1.1/RANDOM_prey_baseline_config.yaml
================================================================================
config:
  algo_config:
    algorithm:
      class: custom
      name: RANDOM
    device: cpu
    hyperparameters: {}
    seed: null
  device: cpu
  env_config:
    action_space:
      dtype: float32
      high: 1.0
      low: -1.0
      shape:
      - 2
      type: Box
    environment:
      evader_speed: 0.01
      local_ratio: 0.5
      max_cycles: 3000
      n_evaders: 90
      n_obstacles: 2
      n_poisons: 10
      n_predators: 5
      n_preys: 10
      name: waterworld_v4
      obstacle_coord:
      - - 0.2
        - 0.2
      - - 0.8
        - 0.2
      poison_speed: 0.01
      predator_speed: 0.06
      prey_speed: 0.001
      render_mode: null
      sensor_range: 0.8
      static_food: true
      static_poison: true
      thrust_penalty: 0.0
    observation_space:
      dtype: float32
      shape:
      - 212
      type: Box
  experiment_name: random_baseline_stage1.1
  generation: 0
  mode_config:
    auto_cleanup: true
    checkpoint_freq: 20000
    deterministic_eval: true
    eval_freq: 10000
    filename_prefix: DRYRUN_
    log_level: INFO
    max_runs: 3
    n_envs: 4
    n_eval_episodes: 10
    output_base_dir: dryrun_outputs
    save_checkpoints: true
    save_final_model: true
    seed: null
    tensorboard_enabled: true
    total_timesteps: 50000
    verbose: 1
  notes: ''
  opponent_config:
    algorithm: RANDOM
    freeze: true
    side: predator
    type: algorithm
  run_mode: dryrun
  seed: null
  stage_name: baseline_stage1.1
  train_algo: RANDOM
  train_side: prey
  training_config:
    check_freeze: false
    checkpoint_freq: 20000
    deterministic_eval: true
    eval_freq: 10000
    n_envs: 4
    n_eval_episodes: 10
    save_checkpoints: true
    save_final_model: true
    show_progress: true
    tensorboard_enabled: true
    total_timesteps: 50000
    verbose: 1
  version: baseline
timestamp: '2025-10-13T17:28:31.146911'


================================================================================
FILE: configs/run_modes.yaml
================================================================================
# ============================================================================
# è¿è¡Œæ¨¡å¼é…ç½®
# ============================================================================
# ä¸‰ç§æ¨¡å¼ï¼šdebugï¼ˆå¿«é€Ÿè°ƒè¯•ï¼‰ã€dryrunï¼ˆæµç¨‹é¢„æ¼”ï¼‰ã€prodï¼ˆæ­£å¼å®éªŒï¼‰

debug:
  # è®­ç»ƒé…ç½®
  total_timesteps: 1000              # åªè®­ç»ƒ1000æ­¥
  n_envs: 1                          # å•ç¯å¢ƒ
  n_eval_episodes: 3                 # è¯„ä¼°æ—¶åªè·‘3ä¸ªepisode
  
  # ä¿å­˜é…ç½®
  save_checkpoints: false            # ä¸ä¿å­˜æ£€æŸ¥ç‚¹
  save_final_model: false            # ä¸ä¿å­˜æœ€ç»ˆæ¨¡å‹
  checkpoint_freq: -1                # ç¦ç”¨æ£€æŸ¥ç‚¹
  
  # è¯„ä¼°é…ç½®
  eval_freq: -1                      # ç¦ç”¨å®šæœŸè¯„ä¼°
  
  # æ—¥å¿—é…ç½®
  tensorboard_enabled: false         # å…³é—­TensorBoardï¼ˆå¯é€‰trueå†™å…¥debugç›®å½•ï¼‰
  log_level: "DEBUG"                 # è¯¦ç»†æ—¥å¿—
  verbose: 2                         # SB3è¯¦ç»†è¾“å‡º
  
  # è¾“å‡ºç®¡ç†
  output_base_dir: "debug_outputs/current"
  clear_on_start: true               # å¯åŠ¨æ—¶æ¸…ç©ºç›®å½•
  archive_on_exit: true              # é€€å‡ºæ—¶å½’æ¡£åˆ°archive/
  max_archives: 5                    # æœ€å¤šä¿ç•™5ä¸ªå†å²å½’æ¡£
  
  # æ–‡ä»¶å‘½å
  filename_prefix: "DEBUG_"          # æ–‡ä»¶åå‰ç¼€
  
  # å…¶ä»–
  deterministic_eval: true           # è¯„ä¼°æ—¶ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
  seed: 42                           # å›ºå®šéšæœºç§å­ï¼ˆä¾¿äºè°ƒè¯•ï¼‰


dryrun:
  # è®­ç»ƒé…ç½®
  total_timesteps: 50000             # 5ä¸‡æ­¥å¿«é€ŸéªŒè¯
  n_envs: 4                          # 4ä¸ªå¹¶è¡Œç¯å¢ƒ
  n_eval_episodes: 10                # è¯„ä¼°10ä¸ªepisode
  
  # ä¿å­˜é…ç½®
  save_checkpoints: true             # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä½†æ ‡è®°ä¸ºdryrunï¼‰
  save_final_model: true             # ä¿å­˜æœ€ç»ˆæ¨¡å‹
  checkpoint_freq: 20000             # æ¯2ä¸‡æ­¥ä¿å­˜
  
  # è¯„ä¼°é…ç½®
  eval_freq: 10000                   # æ¯1ä¸‡æ­¥è¯„ä¼°
  
  # æ—¥å¿—é…ç½®
  tensorboard_enabled: true          # å¯ç”¨TensorBoard
  log_level: "INFO"                  # æ ‡å‡†æ—¥å¿—
  verbose: 1                         # SB3é€‚ä¸­è¾“å‡º
  
  # è¾“å‡ºç®¡ç†
  output_base_dir: "dryrun_outputs"  # ä¼šè‡ªåŠ¨æ·»åŠ æ—¶é—´æˆ³å­ç›®å½•
  auto_cleanup: true                 # è‡ªåŠ¨æ¸…ç†æ—§æ•°æ®
  max_runs: 3                        # æœ€å¤šä¿ç•™3æ¬¡è¿è¡Œ
  
  # æ–‡ä»¶å‘½å
  filename_prefix: "DRYRUN_"
  
  # å…¶ä»–
  deterministic_eval: true
  seed: null                         # ä¸å›ºå®šéšæœºç§å­ï¼ˆæ›´çœŸå®ï¼‰


prod:
  # è®­ç»ƒé…ç½®
  total_timesteps: 1000000           # å®Œæ•´è®­ç»ƒï¼ˆ1Mæ­¥ï¼Œå„é˜¶æ®µå¯è¦†ç›–ï¼‰
  n_envs: 8                          # 8ä¸ªå¹¶è¡Œç¯å¢ƒ
  n_eval_episodes: 100               # è¯„ä¼°100ä¸ªepisode
  
  # ä¿å­˜é…ç½®
  save_checkpoints: true             # ä¿å­˜æ£€æŸ¥ç‚¹
  save_final_model: true             # ä¿å­˜æœ€ç»ˆæ¨¡å‹
  checkpoint_freq: 100000            # æ¯10ä¸‡æ­¥ä¿å­˜
  
  # è¯„ä¼°é…ç½®
  eval_freq: 50000                   # æ¯5ä¸‡æ­¥è¯„ä¼°
  
  # æ—¥å¿—é…ç½®
  tensorboard_enabled: true          # å¯ç”¨TensorBoard
  log_level: "INFO"                  # æ ‡å‡†æ—¥å¿—
  verbose: 1                         # SB3é€‚ä¸­è¾“å‡º
  
  # è¾“å‡ºç®¡ç†
  output_base_dir: "outputs"         # æ­£å¼è¾“å‡ºç›®å½•
  require_confirmation: true         # å¯åŠ¨å‰éœ€è¦ç¡®è®¤
  auto_backup: false                 # ä¸è‡ªåŠ¨æ¸…ç†ï¼ˆæ‰‹åŠ¨ç®¡ç†ï¼‰
  
  # æ–‡ä»¶å‘½å
  filename_prefix: ""                # æ— å‰ç¼€
  
  # å…¶ä»–
  deterministic_eval: true           # è¯„ä¼°æ—¶ç¡®å®šæ€§
  seed: null                         # ä¸å›ºå®šç§å­ï¼ˆæ›´é²æ£’ï¼‰
# ============================================================================
# TEST MODE - å¿«é€Ÿæµç¨‹æµ‹è¯•ï¼ˆ10000æ­¥å®Œæ•´æµç¨‹ï¼‰
# ============================================================================
test:
  # è®­ç»ƒé…ç½® - æçŸ­æ­¥æ•°ï¼Œæµ‹è¯•å®Œæ•´æµç¨‹
  total_timesteps: 500             # åªè®­ç»ƒ500æ­¥
  n_envs: 4                          # 4ä¸ªå¹¶è¡Œç¯å¢ƒï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰
  n_eval_episodes: 2                 # è¯„ä¼°2ä¸ªepisode
  
  # ä¿å­˜é…ç½®
  save_checkpoints: true             # æµ‹è¯•ä¿å­˜é€»è¾‘
  save_final_model: true             # æµ‹è¯•æ¨¡å‹ä¿å­˜
  checkpoint_freq: 500               # æ¯100æ­¥ä¿å­˜ï¼ˆæµ‹è¯•æ£€æŸ¥ç‚¹ï¼‰
  
  # è¯„ä¼°é…ç½®
  eval_freq: 500                     # æ¯100æ­¥è¯„ä¼°ï¼ˆæµ‹è¯•è¯„ä¼°ï¼‰

  # æ—¥å¿—é…ç½®
  tensorboard_enabled: true          # æµ‹è¯•TensorBoard
  log_level: "INFO"
  verbose: 1
  
  # è¾“å‡ºç®¡ç†
  output_base_dir: "test_outputs"    # ä¸“é—¨çš„æµ‹è¯•è¾“å‡ºç›®å½•
  clear_on_start: true               # æ¯æ¬¡æµ‹è¯•å‰æ¸…ç©º
  auto_cleanup: false                # ä¸è‡ªåŠ¨æ¸…ç†ï¼ˆä¾¿äºæ£€æŸ¥ï¼‰
  
  # æ–‡ä»¶å‘½å
  filename_prefix: "TEST_"
  
  # å…¶ä»–
  deterministic_eval: true
  seed: 42                           # å›ºå®šç§å­ï¼ˆä¾¿äºå¤ç°ï¼‰
  require_confirmation: false        # ä¸éœ€è¦ç¡®è®¤ï¼ˆå¿«é€Ÿå¯åŠ¨ï¼‰


# æ›´æ–°éªŒè¯è§„åˆ™ï¼Œæ·»åŠ testæ¨¡å¼
validation:
  prod:
    min_timesteps: 500000
    forbidden_keywords:
      - "test"
      - "debug"
      - "tmp"
    require_notes: false
  
  debug:
    max_timesteps: 10000
    warn_if_exceed: true
  
  test:  # æ–°å¢
    max_timesteps: 50000             # æµ‹è¯•æ¨¡å¼æœ€å¤š5ä¸‡æ­¥
    warn_if_exceed: true

# ============================================================================
# æ¨¡å¼éªŒè¯è§„åˆ™
# ============================================================================
validation:
  prod:
    min_timesteps: 500000            # ç”Ÿäº§æ¨¡å¼æœ€å°‘50ä¸‡æ­¥
    forbidden_keywords:              # å®éªŒåç§°ä¸èƒ½åŒ…å«è¿™äº›è¯
      - "test"
      - "debug"
      - "tmp"
    require_notes: false             # æ˜¯å¦è¦æ±‚å¡«å†™å®éªŒå¤‡æ³¨
  
  debug:
    max_timesteps: 10000             # è°ƒè¯•æ¨¡å¼æœ€å¤š1ä¸‡æ­¥ï¼ˆè­¦å‘Šï¼‰
    warn_if_exceed: true


# ============================================================================
# å†»ç»“æ¡ä»¶ï¼ˆä»€ä¹ˆæ—¶å€™å°†æ¨¡å‹åŠ å…¥å›ºå®šæ± ï¼‰
# ============================================================================
freeze_criteria:
  prey:
    min_survival_rate: 0.65          # æœ€ä½ç”Ÿå­˜ç‡
    min_avg_reward: 2.0              # æœ€ä½å¹³å‡å¥–åŠ±
    min_episodes: 100                # è‡³å°‘è¯„ä¼°100ä¸ªepisode
    
  predator:
    min_catch_rate: 0.55             # æœ€ä½æ•è·ç‡
    min_avg_reward: 2.0              # æœ€ä½å¹³å‡å¥–åŠ±
    min_episodes: 100


# ============================================================================
# å›ºå®šæ± ç®¡ç†
# ============================================================================
fixed_pool:
  max_versions: 5                    # æ¯ä¸ªç®—æ³•æœ€å¤šä¿ç•™5ä¸ªç‰ˆæœ¬
  similarity_threshold: 0.8          # KLæ•£åº¦>0.8è®¤ä¸ºç›¸ä¼¼ï¼Œæ›¿æ¢æ—§ç‰ˆæœ¬
  auto_maintain: true                # è‡ªåŠ¨ç»´æŠ¤æ± å¤§å°

================================================================================
FILE: configs/environments/waterworld_fast.yaml
================================================================================
action_space:
  dtype: float32
  high: 1.0
  low: -1.0
  shape:
  - 2
  type: Box
environment:
  evader_speed: 0.01
  local_ratio: 0.5
  max_cycles: 500
  n_evaders: 90
  n_obstacles: 2
  n_poisons: 10
  n_predators: 5
  n_preys: 10
  name: waterworld_v4
  obstacle_coord:
  - - 0.2
    - 0.2
  - - 0.8
    - 0.2
  poison_speed: 0.01
  predator_speed: 0.06
  prey_speed: 0.001
  render_mode: null
  sensor_range: 0.8
  static_food: true
  static_poison: true
  thrust_penalty: 0.0
observation_space:
  dtype: float32
  shape:
  - 212
  type: Box


================================================================================
FILE: configs/environments/waterworld_standard.yaml
================================================================================
# ============================================================================
# Waterworld æ ‡å‡†ç¯å¢ƒé…ç½®ï¼ˆç”¨äºæ­£å¼å®éªŒï¼‰
# ============================================================================

environment:
  name: "waterworld_v4"
  
  # æ™ºèƒ½ä½“æ•°é‡
  n_predators: 5
  n_preys: 10
  n_evaders: 90                      # é£Ÿç‰©æ•°é‡
  n_poisons: 10
  n_obstacles: 2
  
  # éšœç¢ç‰©ä½ç½®ï¼ˆå›ºå®šï¼‰
  obstacle_coord:
    - [0.2, 0.2]
    - [0.8, 0.2]
  
  # é€Ÿåº¦é…ç½®
  predator_speed: 0.06               # Predatoré€Ÿåº¦
  prey_speed: 0.001                  # Preyé€Ÿåº¦ï¼ˆæ›´æ…¢ï¼Œå®¹æ˜“è¢«æŠ“ï¼‰
  evader_speed: 0.01                 # é£Ÿç‰©é€Ÿåº¦
  poison_speed: 0.01                 # æ¯’è¯é€Ÿåº¦
  
  # ä¼ æ„Ÿå™¨é…ç½®
  sensor_range: 0.8                  # ä¼ æ„Ÿå™¨èŒƒå›´ï¼ˆå¢å¤§ä»¥ä¾¿æ™ºèƒ½ä½“æ„ŸçŸ¥æ›´å¤šï¼‰
  
  # å¥–åŠ±é…ç½®
  thrust_penalty: 0.0                # ç§»åŠ¨æƒ©ç½šï¼ˆ0=æ— æƒ©ç½šï¼‰
  local_ratio: 0.5                   # å±€éƒ¨å¥–åŠ±æ¯”ä¾‹
  
  # ç¯å¢ƒè®¾ç½®
  max_cycles: 3000                   # æœ€å¤§æ­¥æ•°
  static_food: true                  # é£Ÿç‰©é™æ€ï¼ˆä¸ç§»åŠ¨ï¼‰
  static_poison: true                # æ¯’è¯é™æ€
  
  # æ¸²æŸ“è®¾ç½®
  render_mode: null                  # è®­ç»ƒæ—¶ä¸æ¸²æŸ“ï¼ˆnullï¼‰ï¼Œè¯„ä¼°æ—¶å¯è®¾ä¸º"rgb_array"

# è§‚å¯Ÿå’ŒåŠ¨ä½œç©ºé—´ï¼ˆè‡ªåŠ¨ä»ç¯å¢ƒè·å–ï¼Œè¿™é‡Œä»…æ–‡æ¡£ç”¨é€”ï¼‰
observation_space:
  type: "Box"
  shape: [212]                       # Waterworldé»˜è®¤è§‚å¯Ÿç»´åº¦
  dtype: "float32"

action_space:
  type: "Box"
  shape: [2]                         # [Î”x, Î”y]
  low: -1.0
  high: 1.0
  dtype: "float32"

================================================================================
FILE: configs/training/stage1_1_prey_warmup.yaml
================================================================================
# ============================================================================
# Stage 1.1: Prey é¢„çƒ­è®­ç»ƒé…ç½®
# ============================================================================

stage:
  name: "stage1.1_prey_warmup"
  description: "è®­ç»ƒPreyå¯¹æŠ—RANDOM Predatorï¼Œå»ºç«‹åˆå§‹Preyæ± "
  generation: 0

# è®­ç»ƒæ–¹é…ç½®
train_side: "prey"                   # è®­ç»ƒå“ªä¸€æ–¹

# è¦è®­ç»ƒçš„ç®—æ³•åˆ—è¡¨
algorithms_to_train:
  - "PPO"
  - "A2C"
  - "SAC"
  - "TD3"
  # RANDOMä¸éœ€è¦è®­ç»ƒ

# å¯¹æ‰‹é…ç½®
opponent:
  side: "predator"
  type: "algorithm"                  # algorithm / fixed_model / mixed_pool
  algorithm: "RANDOM"                # ä½¿ç”¨RANDOMç®—æ³•
  freeze: true                       # å†»ç»“å¯¹æ‰‹å‚æ•°

# ç¯å¢ƒé…ç½®ï¼ˆå¼•ç”¨ï¼‰
environment_config: "configs/environments/waterworld_standard.yaml"

# è®­ç»ƒé…ç½®ï¼ˆä¼šè¦†ç›–run_modeä¸­çš„é»˜è®¤å€¼ï¼‰
training:
  # prodæ¨¡å¼ä¸‹çš„é…ç½®
  prod:
    total_timesteps: 1000000         # 100ä¸‡æ­¥
    eval_freq: 50000
    checkpoint_freq: 100000
  
  # dryrunæ¨¡å¼ä¸‹çš„é…ç½®
  dryrun:
    total_timesteps: 50000
    eval_freq: 10000
    checkpoint_freq: 20000
  
  # debugæ¨¡å¼ä½¿ç”¨é»˜è®¤é…ç½®ï¼ˆä¸è¦†ç›–ï¼‰

# å†»ç»“æ¡ä»¶ï¼ˆè¾¾åˆ°æ­¤æ ‡å‡†åˆ™åŠ å…¥fixed_poolï¼‰
freeze_on_success:
  enabled: true
  criteria:
    survival_rate: 0.65
    avg_reward: 2.0
    min_eval_episodes: 100
  save_to_pool: "prey_pool_v1"

# è¾“å‡ºé…ç½®
output:
  save_models: true
  save_tensorboard: true
  save_config_snapshot: true         # ä¿å­˜æ­¤é…ç½®çš„å¿«ç…§

================================================================================
FILE: configs/training/stage1_2_pred_guided.yaml
================================================================================
# ============================================================================
# Stage 1.2: Predator å¼•å¯¼è®­ç»ƒé…ç½®
# ============================================================================

stage:
  name: "stage1.2_pred_guided"
  description: "è®­ç»ƒPredatorå¯¹æŠ—å›ºå®šçš„Preyæ± v1"
  generation: 1

train_side: "predator"

algorithms_to_train:
  - "PPO"
  - "A2C"
  - "SAC"
  - "TD3"

# å¯¹æ‰‹é…ç½®ï¼šä»å›ºå®šæ± é‡‡æ ·
opponent:
  side: "prey"
  type: "mixed_pool"                 # æ··åˆæ± æ¨¡å¼
  pool_path: "outputs/fixed_pools/prey_pool_v1"
  mix_strategy:
    fixed_ratio: 0.7                 # 70%ä»å›ºå®šæ± 
    random_ratio: 0.3                # 30%ç”¨RANDOM
  sampling: "uniform"                # uniform / weightedï¼ˆæŒ‰æ€§èƒ½åŠ æƒï¼‰
  freeze: true

environment_config: "configs/environments/waterworld_standard.yaml"

training:
  prod:
    total_timesteps: 1000000
    eval_freq: 50000
    checkpoint_freq: 100000
  
  dryrun:
    total_timesteps: 50000
    eval_freq: 10000
    checkpoint_freq: 20000

freeze_on_success:
  enabled: true
  criteria:
    catch_rate: 0.55
    avg_reward: 2.0
    min_eval_episodes: 100
  save_to_pool: "predator_pool_v1"

output:
  save_models: true
  save_tensorboard: true
  save_config_snapshot: true

================================================================================
FILE: configs/training/stage1_3_coevolution.yaml
================================================================================
# ============================================================================
# Stage 1.3: å…±è¿›åŒ–è®­ç»ƒé…ç½®
# ============================================================================

stage:
  name: "stage1.3_coevolution"
  description: "Predatorå’ŒPreyäº¤æ›¿è®­ç»ƒï¼Œå…±åŒè¿›åŒ–"

# å…±è¿›åŒ–ç‰¹å®šé…ç½®
coevolution:
  max_generations: 4                # æœ€å¤š20ä»£
  start_generation: 2                # ä»ç¬¬2ä»£å¼€å§‹ï¼ˆ0,1å·²åœ¨stage1.1å’Œ1.2å®Œæˆï¼‰
  
  # å¥‡å¶ä»£äº¤æ›¿è§„åˆ™
  alternation:
    even_gen: "predator"             # å¶æ•°ä»£è®­ç»ƒpredator
    odd_gen: "prey"                  # å¥‡æ•°ä»£è®­ç»ƒprey
  
  # æ”¶æ•›æ¡ä»¶
  convergence:
    enabled: true
    check_last_n_gens: 5             # æ£€æŸ¥æœ€è¿‘5ä»£
    performance_change_threshold: 0.03  # æ€§èƒ½å˜åŒ–<3%
    balance_threshold: 0.05          # CatchRateæ¥è¿‘0.5Â±0.05
  
  # åˆå§‹å¯¹æ‰‹æ± 
  initial_pools:
    prey_pool: "outputs/fixed_pools/prey_pool_v1"
    predator_pool: "outputs/fixed_pools/predator_pool_v1"

algorithms_to_train:
  - "PPO"
  - "A2C"
  - "SAC"
  - "TD3"

# æ¯ä»£çš„å¯¹æ‰‹é…ç½®ï¼ˆåŠ¨æ€ï¼‰
opponent:
  type: "mixed_pool"
  mix_strategy:
    fixed_ratio: 0.7
    random_ratio: 0.3
  sampling: "uniform"
  freeze: true

environment_config: "configs/environments/waterworld_standard.yaml"

# æ¯ä»£çš„è®­ç»ƒé…ç½®
training:
  prod:
    total_timesteps: 400000          # æ¯ä»£40ä¸‡æ­¥ï¼ˆæ€»å…±20ä»£=800ä¸‡æ­¥ï¼‰
    eval_freq: 50000
    checkpoint_freq: 100000
  
  dryrun:
    total_timesteps: 20000           # æ¯ä»£2ä¸‡æ­¥
    eval_freq: 5000
    checkpoint_freq: 10000

# å†»ç»“æ¡ä»¶ï¼ˆæ¯ä»£ï¼‰
freeze_on_success:
  enabled: true
  criteria:
    # Prey
    prey:
      survival_rate: 0.45            # å…±è¿›åŒ–é˜¶æ®µæ ‡å‡†å¯ä»¥æ›´å®½æ¾
      avg_reward: 1.0
      min_eval_episodes: 50
    # Predator
    predator:
      catch_rate: 0.45
      avg_reward: 1.0
      min_eval_episodes: 50
  
  # åŠ¨æ€æ± ç®¡ç†
  pool_management:
    max_versions_per_algo: 5         # æ¯ä¸ªç®—æ³•æœ€å¤š5ä¸ªç‰ˆæœ¬
    auto_prune: true                 # è‡ªåŠ¨æ·˜æ±°ç›¸ä¼¼ç‰ˆæœ¬

output:
  save_models: true
  save_tensorboard: true
  save_config_snapshot: true
  per_generation_subfolder: true     # æ¯ä»£ä¸€ä¸ªå­æ–‡ä»¶å¤¹

================================================================================
FILE: configs/training/stage1_3_coevolution_test.yaml
================================================================================
# ============================================================================
# Stage 1.3: å…±è¿›åŒ–è®­ç»ƒé…ç½® - TESTç‰ˆæœ¬ï¼ˆä»…ç”¨äºå¿«é€Ÿæµ‹è¯•æµç¨‹ï¼‰
# ============================================================================

stage:
  name: "stage1.3_coevolution"
  description: "Predatorå’ŒPreyäº¤æ›¿è®­ç»ƒï¼Œå…±åŒè¿›åŒ– - æµ‹è¯•ç‰ˆæœ¬"

# âœ… æµ‹è¯•ç‰ˆæœ¬ï¼šåªè·‘1ä»£
coevolution:
  max_generations: 2                # start=2, max=2 â†’ åªè·‘1ä»£
  start_generation: 2
  
  alternation:
    even_gen: "predator"
    odd_gen: "prey"
  
  convergence:
    enabled: false                  # æµ‹è¯•æ—¶ä¸æ£€æŸ¥æ”¶æ•›
  
  initial_pools:
    prey_pool: "outputs/fixed_pools/prey_pool_v1"
    predator_pool: "outputs/fixed_pools/predator_pool_v1"

# âœ… æµ‹è¯•æ—¶å¯ä»¥åªè·‘1-2ä¸ªç®—æ³•åŠ å¿«é€Ÿåº¦
algorithms_to_train:
  - "PPO"
  - "A2C"
  # æµ‹è¯•æ—¶æ³¨é‡Šæ‰ SAC å’Œ TD3 ä»¥åŠ å¿«é€Ÿåº¦
  # - "SAC"
  # - "TD3"

opponent:
  type: "mixed_pool"
  mix_strategy:
    fixed_ratio: 0.7
    random_ratio: 0.3
  sampling: "uniform"
  freeze: true

environment_config: "configs/environments/waterworld_standard.yaml"

# âœ… æµ‹è¯•é…ç½®ï¼ˆæçŸ­æ­¥æ•°ï¼‰
training:
  test:
    total_timesteps: 500            # æ¯ä»£åªè·‘500æ­¥
    eval_freq: 500
    checkpoint_freq: 500
  
  # ä¸éœ€è¦å…¶ä»–æ¨¡å¼çš„é…ç½®

freeze_on_success:
  enabled: false                    # æµ‹è¯•æ—¶ä¸æ£€æŸ¥å†»ç»“æ¡ä»¶
  
output:
  save_models: true
  save_tensorboard: true
  save_config_snapshot: true
  per_generation_subfolder: true

================================================================================
FILE: configs/algorithms/a2c.yaml
================================================================================
# ============================================================================
# A2C ç®—æ³•é…ç½®
# ============================================================================

algorithm:
  name: "A2C"
  class: "stable_baselines3.A2C"

hyperparameters:
  learning_rate: 7.0e-4              # A2Cé€šå¸¸éœ€è¦æ›´é«˜å­¦ä¹ ç‡
  
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      vf: [256, 256]
    activation_fn: "torch.nn.ReLU"
  
  n_steps: 5                         # A2Cæ¯5æ­¥æ›´æ–°ä¸€æ¬¡
  gamma: 0.99
  gae_lambda: 1.0                    # A2Cé€šå¸¸ç”¨1.0
  
  ent_coef: 0.01
  vf_coef: 0.25                      # A2Cçš„vf_coefé€šå¸¸è¾ƒå°
  max_grad_norm: 0.5
  
  rms_prop_eps: 1.0e-5               # RMSpropç‰¹å®šå‚æ•°
  normalize_advantage: false         # A2Cé€šå¸¸ä¸æ ‡å‡†åŒ–

device: "auto"
seed: null

================================================================================
FILE: configs/algorithms/ppo.yaml
================================================================================
# ============================================================================
# PPO ç®—æ³•é…ç½®
# ============================================================================

algorithm:
  name: "PPO"
  class: "stable_baselines3.PPO"

# è¶…å‚æ•°
hyperparameters:
  # å­¦ä¹ ç‡
  learning_rate: 3.0e-4
  
  # ç½‘ç»œæ¶æ„
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256]                 # Actorç½‘ç»œ
      vf: [256, 256]                 # Criticç½‘ç»œ
    activation_fn: "torch.nn.ReLU"
  
  # è®­ç»ƒå‚æ•°
  n_steps: 2048                      # æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ­¥æ•°
  batch_size: 64                     # å°æ‰¹é‡å¤§å°
  n_epochs: 10                       # æ¯æ¬¡æ›´æ–°çš„epochæ•°
  gamma: 0.99                        # æŠ˜æ‰£å› å­
  gae_lambda: 0.95                   # GAE lambda
  
  # PPOç‰¹å®šå‚æ•°
  clip_range: 0.2                    # PPOè£å‰ªèŒƒå›´
  clip_range_vf: null                # Value functionè£å‰ªï¼ˆnull=ä¸è£å‰ªï¼‰
  
  # æ­£åˆ™åŒ–
  ent_coef: 0.01                     # ç†µç³»æ•°ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
  vf_coef: 0.5                       # Value functionç³»æ•°
  max_grad_norm: 0.5                 # æ¢¯åº¦è£å‰ª
  
  # å…¶ä»–
  normalize_advantage: true          # æ ‡å‡†åŒ–ä¼˜åŠ¿å‡½æ•°
  target_kl: null                    # ç›®æ ‡KLæ•£åº¦ï¼ˆnull=ä¸ä½¿ç”¨ï¼‰
  
# è®¾å¤‡é…ç½®
device: "auto"                       # autoä¼šè‡ªåŠ¨é€‰æ‹©cuda/cpu

# ç§å­ï¼ˆnullè¡¨ç¤ºéšæœºï¼‰
seed: null

================================================================================
FILE: configs/algorithms/random.yaml
================================================================================
# ============================================================================
# Random Policy é…ç½®ï¼ˆBaselineï¼‰
# ============================================================================

algorithm:
  name: "RANDOM"
  class: "custom"                    # è‡ªå®šä¹‰å®ç°

# Randomç­–ç•¥æ²¡æœ‰è¶…å‚æ•°
hyperparameters: {}

# ç”¨äºç»Ÿä¸€æ¥å£
device: "cpu"
seed: null

================================================================================
FILE: configs/algorithms/sac.yaml
================================================================================
# ============================================================================
# SAC ç®—æ³•é…ç½®
# ============================================================================

algorithm:
  name: "SAC"
  class: "stable_baselines3.SAC"

hyperparameters:
  learning_rate: 3.0e-4
  
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      qf: [256, 256]                 # Qç½‘ç»œ
    activation_fn: "torch.nn.ReLU"
  
  buffer_size: 1000000               # Replay bufferå¤§å°
  learning_starts: 10000             # å¼€å§‹å­¦ä¹ å‰çš„éšæœºæ­¥æ•°
  batch_size: 256                    # ä»bufferé‡‡æ ·çš„æ‰¹é‡å¤§å°
  tau: 0.005                         # è½¯æ›´æ–°ç³»æ•°
  gamma: 0.99
  
  # SACç‰¹å®šå‚æ•°
  train_freq: 1                      # æ¯æ­¥éƒ½è®­ç»ƒ
  gradient_steps: 1                  # æ¯æ¬¡è®­ç»ƒçš„æ¢¯åº¦æ­¥æ•°
  
  # è‡ªåŠ¨è°ƒæ•´ç†µç³»æ•°
  ent_coef: "auto"                   # è‡ªåŠ¨è°ƒæ•´alpha
  target_entropy: "auto"             # ç›®æ ‡ç†µï¼ˆauto=-action_dimï¼‰
  
  # å…¶ä»–
  use_sde: false                     # æ˜¯å¦ä½¿ç”¨çŠ¶æ€ä¾èµ–æ¢ç´¢
  sde_sample_freq: -1

device: "auto"
seed: null

================================================================================
FILE: configs/algorithms/td3.yaml
================================================================================
# ============================================================================
# TD3 ç®—æ³•é…ç½®
# ============================================================================

algorithm:
  name: "TD3"
  class: "stable_baselines3.TD3"

hyperparameters:
  learning_rate: 3.0e-4
  
  policy: "MlpPolicy"
  policy_kwargs:
    net_arch:
      pi: [256, 256]
      qf: [256, 256]
    activation_fn: "torch.nn.ReLU"
  
  buffer_size: 1000000
  learning_starts: 10000
  batch_size: 256                    # TD3æ¨è256
  tau: 0.005
  gamma: 0.99
  
  # TD3ç‰¹å®šå‚æ•°
  train_freq: 1
  gradient_steps: 1
  policy_delay: 2                    # å»¶è¿Ÿç­–ç•¥æ›´æ–°ï¼ˆTD3æ ¸å¿ƒï¼‰
  target_policy_noise: 0.2           # ç›®æ ‡ç­–ç•¥å™ªå£°
  target_noise_clip: 0.5             # å™ªå£°è£å‰ª
  
  # æ¢ç´¢å™ªå£°
  action_noise: "normal"             # normalæˆ–ornstein-uhlenbeck
  action_noise_kwargs:
    mean: 0.0
    sigma: 0.1

device: "auto"
seed: null

================================================================================
FILE: configs/evaluation/cross_eval.yaml
================================================================================
# ============================================================================
# äº¤å‰è¯„ä¼°é…ç½®ï¼ˆCross-Evaluation Configurationï¼‰
# ============================================================================

evaluation:
  # è¯„ä¼°æ¨¡å¼
  mode: "test"  # test / dryrun / prod
  
  # æ¨¡å‹æ± è·¯å¾„ï¼ˆæ ¹æ®æ¨¡å¼è‡ªåŠ¨é€‰æ‹©ï¼‰
  model_pools:
    test:
      predator_pool: "outputs/fixed_pools/predator_pool_v1"
      prey_pool: "outputs/fixed_pools/prey_pool_v1"
      saved_models_base: "outputs/saved_models"
    
    dryrun:
      predator_pool: "dryrun_outputs/fixed_pools/predator_pool_v1"
      prey_pool: "dryrun_outputs/fixed_pools/prey_pool_v1"
      saved_models_base: "dryrun_outputs/saved_models"
    
    prod:
      predator_pool: "outputs/fixed_pools/predator_pool_v1"
      prey_pool: "outputs/fixed_pools/prey_pool_v1"
      saved_models_base: "outputs/saved_models"
  
  # è¯„ä¼°å‚æ•°
  n_eval_episodes: 20            # æ¯ä¸ªç»„åˆè¯„ä¼°çš„episodeæ•°
  deterministic: true            # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
  
  # ç¯å¢ƒé…ç½®
  env_config: "waterworld_fast"  # ä½¿ç”¨å¿«é€Ÿç¯å¢ƒåŠ é€Ÿè¯„ä¼°
  
  # è¦è¯„ä¼°çš„ç®—æ³•
  algorithms:
    - "PPO"
    - "A2C"
    - "SAC"
    - "TD3"
    - "RANDOM"
  
  # è¾“å‡ºé…ç½®
  output_dir: "evaluation_results"
  save_raw_results: true
  save_processed_results: true
  
  # æŒ‡æ ‡é…ç½®
  metrics:
    predator:
      - "catch_rate"
      - "avg_reward"
      - "energy_efficiency"
      - "first_catch_time"
      - "avg_episode_length"
    
    prey:
      - "survival_rate"
      - "avg_reward"
      - "escape_success"
      - "avg_lifespan"
      - "avg_episode_length"
    
    matchup:
      - "reward_gap"
      - "balance_score"
      - "episode_length"

# ============================================================================
# å¯è§†åŒ–é…ç½®
# ============================================================================
visualization:
  # çƒ­åŠ›å›¾
  heatmap:
    figsize: [10, 8]
    cmap: "RdYlGn"
    annot: true
    fmt: ".3f"
    
  # æ³›åŒ–æ›²çº¿
  generalization_curve:
    figsize: [10, 6]
    show_std: true
    
  # é›·è¾¾å›¾
  radar_chart:
    figsize: [8, 8]
    metrics:
      - "adaptability_score"
      - "avg_performance"
      - "ood_performance"
      - "stability"

# ============================================================================
# åˆ†æé…ç½®
# ============================================================================
analysis:
  # è‡ªé€‚åº”æ€§è®¡ç®—
  adaptability:
    method: "performance_ratio"  # performance_ratio / performance_drop
    include_random: false         # æ˜¯å¦åŒ…å«RANDOMåœ¨OODè®¡ç®—ä¸­
  
  # ç»Ÿè®¡æ£€éªŒ
  statistical_tests:
    method: "wilcoxon"           # wilcoxon / ttest / mannwhitneyu
    significance_level: 0.05
  
  # ç­–ç•¥è·ç¦»ä¼°ç®—ï¼ˆç”¨äºæ³›åŒ–æ›²çº¿ï¼‰
  policy_distance:
    # é¢„è®¾çš„ç®—æ³•é—´è·ç¦»ï¼ˆåŸºäºæ¶æ„ç›¸ä¼¼åº¦ï¼‰
    distances:
      PPO:
        PPO: 0.0
        A2C: 0.3
        SAC: 0.8
        TD3: 0.6
        RANDOM: 1.0
      A2C:
        PPO: 0.3
        A2C: 0.0
        SAC: 0.7
        TD3: 0.5
        RANDOM: 1.0
      SAC:
        PPO: 0.8
        A2C: 0.7
        SAC: 0.0
        TD3: 0.4
        RANDOM: 1.0
      TD3:
        PPO: 0.6
        A2C: 0.5
        SAC: 0.4
        TD3: 0.0
        RANDOM: 1.0
      RANDOM:
        PPO: 1.0
        A2C: 1.0
        SAC: 1.0
        TD3: 1.0
        RANDOM: 0.5

