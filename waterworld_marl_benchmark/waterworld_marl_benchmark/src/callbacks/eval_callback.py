"""
è¯„ä¼°å›è°ƒ
å®šæœŸè¯„ä¼°æ¨¡å‹æ€§èƒ½
"""

import numpy as np
from pathlib import Path
from typing import Optional, Callable, Dict, Any
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.evaluation import evaluate_policy


class EvalCallback(BaseCallback):
    """è¯„ä¼°å›è°ƒ"""
    
    def __init__(
        self,
        eval_env,
        train_side: str,
        eval_freq: int = 10000,
        n_eval_episodes: int = 10,
        deterministic: bool = True,
        render: bool = False,
        verbose: int = 1,
        best_model_save_path: Optional[Path] = None,
        log_path: Optional[Path] = None
    ):
        """
        åˆå§‹åŒ–å›è°ƒ
        
        Args:
            eval_env: è¯„ä¼°ç¯å¢ƒ
            train_side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
            eval_freq: è¯„ä¼°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰
            n_eval_episodes: è¯„ä¼°episodeæ•°
            deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
            render: æ˜¯å¦æ¸²æŸ“
            verbose: è¯¦ç»†ç¨‹åº¦
            best_model_save_path: æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„
            log_path: æ—¥å¿—ä¿å­˜è·¯å¾„
        """
        super().__init__(verbose)
        self.eval_env = eval_env
        self.train_side = train_side
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.deterministic = deterministic
        self.render = render
        self.best_model_save_path = best_model_save_path
        self.log_path = log_path
        
        # æœ€ä½³æ€§èƒ½è·Ÿè¸ª
        self.best_mean_reward = -np.inf
        self.last_mean_reward = -np.inf
        
        # è¯„ä¼°å†å²
        self.evaluations_timesteps = []
        self.evaluations_results = []
        self.evaluations_length = []
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        if self.best_model_save_path:
            self.best_model_save_path = Path(self.best_model_save_path)
            self.best_model_save_path.mkdir(parents=True, exist_ok=True)
    
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨"""
        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:
            self._evaluate()
        
        return True
    
    def _evaluate(self):
        """æ‰§è¡Œè¯„ä¼°"""
        if self.verbose > 0:
            print(f"\n{'='*70}")
            print(f"ğŸ“Š è¯„ä¼° (æ­¥æ•°: {self.n_calls})")
            print(f"{'='*70}")
        
        # è¯„ä¼°æ¨¡å‹
        episode_rewards, episode_lengths = evaluate_policy(
            self.model,
            self.eval_env,
            n_eval_episodes=self.n_eval_episodes,
            render=self.render,
            deterministic=self.deterministic,
            return_episode_rewards=True
        )
        
        mean_reward = np.mean(episode_rewards)
        std_reward = np.std(episode_rewards)
        mean_length = np.mean(episode_lengths)
        
        self.last_mean_reward = mean_reward
        
        # è®°å½•ç»“æœ
        self.evaluations_timesteps.append(self.n_calls)
        self.evaluations_results.append(episode_rewards)
        self.evaluations_length.append(episode_lengths)
        
        # è®°å½•åˆ°TensorBoard
        self.logger.record('eval/mean_reward', mean_reward)
        self.logger.record('eval/std_reward', std_reward)
        self.logger.record('eval/mean_ep_length', mean_length)
        
        if self.verbose > 0:
            print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.2f} +/- {std_reward:.2f}")
            print(f"  å¹³å‡é•¿åº¦: {mean_length:.0f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if mean_reward > self.best_mean_reward:
            if self.verbose > 0:
                print(f"  ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! (æ—§: {self.best_mean_reward:.2f}, æ–°: {mean_reward:.2f})")
            
            self.best_mean_reward = mean_reward
            
            if self.best_model_save_path:
                best_model_path = self.best_model_save_path / "best_model.zip"
                self.model.save(best_model_path)
                
                if self.verbose > 0:
                    print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_model_path}")
        
        if self.verbose > 0:
            print(f"{'='*70}\n")
    
    def get_best_mean_reward(self) -> float:
        """è·å–æœ€ä½³å¹³å‡å¥–åŠ±"""
        return self.best_mean_reward
    
    def get_last_mean_reward(self) -> float:
        """è·å–æœ€è¿‘ä¸€æ¬¡å¹³å‡å¥–åŠ±"""
        return self.last_mean_reward