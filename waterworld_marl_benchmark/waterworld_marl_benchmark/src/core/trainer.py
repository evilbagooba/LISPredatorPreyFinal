"""
æ ¸å¿ƒè®­ç»ƒå™¨
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›ç»Ÿä¸€çš„è®­ç»ƒæ¥å£
"""

import os
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List

from src.core.environment import WaterworldEnvManager, create_training_env
from src.core.opponent_pool import create_opponent_policies
from src.algorithms import create_algorithm
from src.callbacks import create_callbacks
from src.utils.config_loader import get_mode_config, get_env_config, get_algo_config
from src.utils.path_manager import PathManager
from src.utils.naming import FileNaming
from src.utils.logger import create_logger
from src.utils.banner import print_mode_banner, print_training_start, print_training_complete
from src.utils.config_snapshot import save_config_snapshot, save_training_summary
from src.utils.config_validator import validator
from src.utils.cleanup import cleanup_debug


class MultiAgentTrainer:
    """å¤šæ™ºèƒ½ä½“è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        # æ ¸å¿ƒé…ç½®
        train_side: str,
        train_algo: str,
        opponent_config: Dict[str, Any],
        
        # å®éªŒå…ƒæ•°æ®
        experiment_name: str,
        stage_name: str,
        generation: int = 0,
        version: str = "v1",
        
        # è¿è¡Œæ¨¡å¼
        run_mode: str = "prod",
        
        # ç¯å¢ƒé…ç½®
        env_config: Optional[Dict[str, Any]] = None,
        
        # ç®—æ³•é…ç½®
        algo_config: Optional[Dict[str, Any]] = None,
        
        # è®­ç»ƒé…ç½®ï¼ˆè¦†ç›–æ¨¡å¼é»˜è®¤å€¼ï¼‰
        total_timesteps: Optional[int] = None,
        n_envs: Optional[int] = None,
        eval_freq: Optional[int] = None,
        checkpoint_freq: Optional[int] = None,
        n_eval_episodes: Optional[int] = None,
        
        # å…¶ä»–
        device: str = "auto",
        seed: Optional[int] = None,
        notes: str = ""
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            train_side: è®­ç»ƒæ–¹ï¼ˆpredator/preyï¼‰
            train_algo: è®­ç»ƒç®—æ³•ï¼ˆPPO/A2C/SAC/TD3ï¼‰
            opponent_config: å¯¹æ‰‹é…ç½®
            experiment_name: å®éªŒåç§°
            stage_name: è®­ç»ƒé˜¶æ®µåç§°ï¼ˆstage1.1_prey_warmupç­‰ï¼‰
            generation: ä»£æ•°
            version: ç‰ˆæœ¬å·
            run_mode: è¿è¡Œæ¨¡å¼ï¼ˆdebug/dryrun/prodï¼‰
            env_config: ç¯å¢ƒé…ç½®ï¼ˆNoneåˆ™ä½¿ç”¨é»˜è®¤ï¼‰
            algo_config: ç®—æ³•é…ç½®ï¼ˆNoneåˆ™ä½¿ç”¨é»˜è®¤ï¼‰
            total_timesteps: æ€»è®­ç»ƒæ­¥æ•°ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            n_envs: å¹¶è¡Œç¯å¢ƒæ•°ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            eval_freq: è¯„ä¼°é¢‘ç‡ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            checkpoint_freq: æ£€æŸ¥ç‚¹é¢‘ç‡ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            n_eval_episodes: è¯„ä¼°episodeæ•°ï¼ˆNoneåˆ™ä½¿ç”¨æ¨¡å¼é»˜è®¤ï¼‰
            device: è®¡ç®—è®¾å¤‡
            seed: éšæœºç§å­
            notes: å®éªŒå¤‡æ³¨
        """
        
        # =====================================================================
        # 1. åŸºæœ¬é…ç½®
        # =====================================================================
        self.train_side = train_side
        self.train_algo = train_algo.upper()
        self.opponent_config = opponent_config
        self.experiment_name = experiment_name
        self.stage_name = stage_name
        self.generation = generation
        self.version = version
        self.run_mode = run_mode
        self.device = device
        self.seed = seed
        self.notes = notes
        
        # =====================================================================
        # 2. åŠ è½½é…ç½®
        # =====================================================================
        
        # åŠ è½½è¿è¡Œæ¨¡å¼é…ç½®
        self.mode_config = get_mode_config(run_mode)
        
        # åŠ è½½ç¯å¢ƒé…ç½®
        if env_config is None:
            env_config = get_env_config("waterworld_standard")
        self.env_config = env_config
        
        # åŠ è½½ç®—æ³•é…ç½®
        if algo_config is None:
            algo_config = get_algo_config(self.train_algo)
        self.algo_config = algo_config
        
        # åˆå¹¶è®­ç»ƒé…ç½®ï¼ˆç”¨æˆ·æŒ‡å®š > æ¨¡å¼é»˜è®¤ï¼‰
        self.training_config = {
            'total_timesteps': total_timesteps or self.mode_config.get('total_timesteps', 1000000),
            'n_envs': n_envs or self.mode_config.get('n_envs', 1),
            'eval_freq': eval_freq if eval_freq is not None else self.mode_config.get('eval_freq', 10000),
            'checkpoint_freq': checkpoint_freq if checkpoint_freq is not None else self.mode_config.get('checkpoint_freq', 100000),
            'n_eval_episodes': n_eval_episodes or self.mode_config.get('n_eval_episodes', 10),
            'save_checkpoints': self.mode_config.get('save_checkpoints', True),
            'save_final_model': self.mode_config.get('save_final_model', True),
            'tensorboard_enabled': self.mode_config.get('tensorboard_enabled', True),
            'verbose': self.mode_config.get('verbose', 1),
            'deterministic_eval': self.mode_config.get('deterministic_eval', True),
            'show_progress': True,
            'check_freeze': False  # åœ¨è®­ç»ƒåæ‰‹åŠ¨æ£€æŸ¥
        }
        
        # =====================================================================
        # 3. éªŒè¯é…ç½®
        # =====================================================================
        full_config = {
            'run_mode': run_mode,
            'train_side': train_side,
            'train_algo': train_algo,
            'experiment_name': experiment_name,
            **self.training_config
        }
        
        if not validator.validate_run_mode(run_mode, full_config):
            validator.print_results()
            raise ValueError("é…ç½®éªŒè¯å¤±è´¥")
        # test æ¨¡å¼ä¸éœ€è¦ç¡®è®¤
        if run_mode not in ['debug', 'test']:  # âœ… æ·»åŠ  'test'
            if not validator.require_confirmation():
                raise KeyboardInterrupt("ç”¨æˆ·å–æ¶ˆè®­ç»ƒ")
        if not validator.require_confirmation():
            raise KeyboardInterrupt("ç”¨æˆ·å–æ¶ˆè®­ç»ƒ")
        
        # =====================================================================
        # 4. è·¯å¾„ç®¡ç†
        # =====================================================================
        self.path_manager = PathManager(run_mode, experiment_name)
        
        # å„ç§è¾“å‡ºè·¯å¾„
        self.model_dir = self.path_manager.get_model_dir(stage_name)
        self.checkpoint_dir = self.path_manager.get_checkpoint_dir(stage_name)
        self.tensorboard_dir = self.path_manager.get_tensorboard_dir(stage_name)
        self.experiment_dir = self.path_manager.get_experiment_dir(stage_name)
        
        # =====================================================================
        # 5. æ—¥å¿—ç³»ç»Ÿ
        # =====================================================================
        self.naming = FileNaming()
        log_filename = self.naming.generate_log_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        self.logger = create_logger(
            name=f"{self.train_algo}_{self.train_side}",
            log_dir=self.experiment_dir,
            log_level=self.mode_config.get('log_level', 'INFO')
        )
        
        # =====================================================================
        # 6. ç¯å¢ƒå’Œæ¨¡å‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        # =====================================================================
        self.env_manager = None
        self.train_env = None
        self.eval_env = None
        self.algorithm = None
        self.opponent_policies = None
        
        # =====================================================================
        # 7. è®­ç»ƒç»Ÿè®¡
        # =====================================================================
        self.training_start_time = None
        self.training_end_time = None
        self.total_training_time = None
        self.final_model_path = None
        
        # =====================================================================
        # 8. æ‰“å°æ¨ªå¹…
        # =====================================================================
        print_mode_banner(run_mode, self.mode_config)
        
        # =====================================================================
        # 9. æ¸…ç†è°ƒè¯•æ•°æ®ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # =====================================================================
        if run_mode == "debug":
            cleanup_debug(self.mode_config)
        
        # =====================================================================
        # 10. ä¿å­˜é…ç½®å¿«ç…§
        # =====================================================================
        self._save_config_snapshot()
    
    def _save_config_snapshot(self):
        """ä¿å­˜é…ç½®å¿«ç…§"""
        snapshot = {
            'run_mode': self.run_mode,
            'train_side': self.train_side,
            'train_algo': self.train_algo,
            'opponent_config': self.opponent_config,
            'experiment_name': self.experiment_name,
            'stage_name': self.stage_name,
            'generation': self.generation,
            'version': self.version,
            'device': self.device,
            'seed': self.seed,
            'notes': self.notes,
            'env_config': self.env_config,
            'algo_config': self.algo_config,
            'training_config': self.training_config,
            'mode_config': self.mode_config
        }
        
        config_filename = self.naming.generate_config_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        save_config_snapshot(
            config=snapshot,
            save_dir=self.experiment_dir,
            name=config_filename.replace('.yaml', '')
        )
    
    def setup(self):
        """
        è®¾ç½®è®­ç»ƒç¯å¢ƒå’Œæ¨¡å‹
        åœ¨è®­ç»ƒå‰å¿…é¡»è°ƒç”¨æ­¤æ–¹æ³•
        """
        self.logger.log_banner("ğŸ”§ è®¾ç½®è®­ç»ƒç¯å¢ƒ", "=")
        
        # =====================================================================
        # 1. åˆ›å»ºç¯å¢ƒç®¡ç†å™¨
        # =====================================================================
        self.logger.info("åˆ›å»ºç¯å¢ƒç®¡ç†å™¨...")
        self.env_manager = WaterworldEnvManager(self.env_config)
        
        # =====================================================================
        # 2. åˆ›å»ºå¯¹æ‰‹ç­–ç•¥
        # =====================================================================
        self.logger.info(f"åˆ›å»ºå¯¹æ‰‹ç­–ç•¥ (ç±»å‹: {self.opponent_config.get('type')})...")
        self.opponent_policies = create_opponent_policies(
            opponent_config=self.opponent_config,
            env_manager=self.env_manager,
            device=self.device
        )
        
        self.logger.info(f"  âœ“ åˆ›å»ºäº† {len(self.opponent_policies)} ä¸ªå¯¹æ‰‹ç­–ç•¥")
        
        # =====================================================================
        # 3. åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        # =====================================================================
        self.logger.info(f"åˆ›å»ºè®­ç»ƒç¯å¢ƒ (å¹¶è¡Œæ•°: {self.training_config['n_envs']})...")
        self.train_env = create_training_env(
            env_config=self.env_config,
            train_side=self.train_side,
            opponent_policies=self.opponent_policies,
            n_envs=self.training_config['n_envs']
        )
        
        # =====================================================================
        # 4. åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        # =====================================================================
        if self.training_config['eval_freq'] > 0:
            self.logger.info("åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
            self.eval_env = create_training_env(
                env_config=self.env_config,
                train_side=self.train_side,
                opponent_policies=self.opponent_policies,
                n_envs=1  # è¯„ä¼°ç”¨å•ç¯å¢ƒ
            )
        
        # =====================================================================
        # 5. åˆ›å»ºç®—æ³•
        # =====================================================================
        self.logger.info(f"åˆ›å»ºç®—æ³•: {self.train_algo}...")
        
        # è·å–ç©ºé—´ä¿¡æ¯
        obs_space = self.env_manager.get_observation_space(self.train_side)
        action_space = self.env_manager.get_action_space(self.train_side)
        
        # åˆ›å»ºç®—æ³•å®ä¾‹
        self.algorithm = create_algorithm(
            algo_name=self.train_algo,
            observation_space=obs_space,
            action_space=action_space,
            config=self.algo_config,
            device=self.device
        )
        
        # åˆ›å»ºæ¨¡å‹
        tensorboard_log = str(self.tensorboard_dir) if self.training_config['tensorboard_enabled'] else None
        
        self.algorithm.create_model(
            env=self.train_env,
            tensorboard_log=tensorboard_log,
            verbose=self.training_config['verbose']
        )
        
        self.logger.info("  âœ“ ç®—æ³•åˆ›å»ºå®Œæˆ")
        
        # =====================================================================
        # 6. è®°å½•é…ç½®
        # =====================================================================
        self.logger.log_config({
            'è®­ç»ƒæ–¹': self.train_side,
            'è®­ç»ƒç®—æ³•': self.train_algo,
            'ç‰ˆæœ¬': self.version,
            'æ€»æ­¥æ•°': self.training_config['total_timesteps'],
            'å¹¶è¡Œç¯å¢ƒ': self.training_config['n_envs'],
            'è¯„ä¼°é¢‘ç‡': self.training_config['eval_freq'],
            'æ£€æŸ¥ç‚¹é¢‘ç‡': self.training_config['checkpoint_freq'],
            'è®¾å¤‡': self.device,
            'éšæœºç§å­': self.seed
        }, title="è®­ç»ƒé…ç½®")
        
        self.logger.log_banner("âœ… ç¯å¢ƒè®¾ç½®å®Œæˆ", "=")
    
    def train(self):
        """æ‰§è¡Œè®­ç»ƒ"""
        
        # =====================================================================
        # 1. æ£€æŸ¥æ˜¯å¦å·²è®¾ç½®
        # =====================================================================
        if self.algorithm is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ setup() æ–¹æ³•è®¾ç½®ç¯å¢ƒå’Œæ¨¡å‹")
        
        # =====================================================================
        # 2. æ‰“å°è®­ç»ƒå¼€å§‹ä¿¡æ¯
        # =====================================================================
        opponent_info = self.naming.format_opponent_info(self.opponent_config)
        print_training_start(
            algo=self.train_algo,
            side=self.train_side,
            version=self.version,
            opponent_info=opponent_info
        )
        
        self.logger.log_banner(f"ğŸš€ å¼€å§‹è®­ç»ƒ {self.train_algo}_{self.train_side}_{self.version}", "=")
        
        # =====================================================================
        # 3. åˆ›å»ºå›è°ƒ
        # =====================================================================
        callbacks = create_callbacks(
            train_side=self.train_side,
            checkpoint_path=self.checkpoint_dir,
            eval_env=self.eval_env,
            config=self.training_config,
            on_freeze=None  # å¯ä»¥æ·»åŠ å†»ç»“å›è°ƒ
        )
        
        # =====================================================================
        # 4. æ‰§è¡Œè®­ç»ƒ
        # =====================================================================
        self.training_start_time = time.time()
        
        try:
            self.algorithm.train(
                env=self.train_env,
                total_timesteps=self.training_config['total_timesteps'],
                callback=callbacks
            )
            
            self.training_end_time = time.time()
            self.total_training_time = self.training_end_time - self.training_start_time
            
        except KeyboardInterrupt:
            self.logger.warning("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            self.training_end_time = time.time()
            self.total_training_time = self.training_end_time - self.training_start_time
            raise
        
        except Exception as e:
            self.logger.error(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
        
        # =====================================================================
        # 5. æ‰“å°è®­ç»ƒå®Œæˆä¿¡æ¯
        # =====================================================================
        print_training_complete(
            algo=self.train_algo,
            side=self.train_side,
            total_steps=self.training_config['total_timesteps'],
            time_elapsed=self.total_training_time
        )
        
        self.logger.log_banner("âœ… è®­ç»ƒå®Œæˆ", "=")
    
    def save_model(self, save_to_pool: bool = False, pool_name: Optional[str] = None):
        """
        ä¿å­˜æœ€ç»ˆæ¨¡å‹
        
        Args:
            save_to_pool: æ˜¯å¦ä¿å­˜åˆ°å›ºå®šæ± 
            pool_name: æ± åç§°ï¼ˆå¦‚ prey_pool_v1ï¼‰
        """
        
        if not self.training_config['save_final_model']:
            self.logger.info("é…ç½®ç¦ç”¨äº†æ¨¡å‹ä¿å­˜ï¼Œè·³è¿‡")
            return
        
        self.logger.log_banner("ğŸ’¾ ä¿å­˜æ¨¡å‹", "-")
        
        # =====================================================================
        # 1. ä¿å­˜åˆ°æ¨¡å‹ç›®å½•
        # =====================================================================
        opponent_info = self.naming.format_opponent_info(self.opponent_config)
        
        model_filename = self.naming.generate_model_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version,
            opponent_info=opponent_info,
            run_mode=self.run_mode
        )
        
        model_path = self.model_dir / model_filename
        self.algorithm.save(str(model_path))
        self.final_model_path = model_path
        
        self.logger.info(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {model_path}")
        
        # =====================================================================
        # 2. ä¿å­˜åˆ°å›ºå®šæ± ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # =====================================================================
        if save_to_pool and pool_name:
            pool_dir = self.path_manager.get_fixed_pool_dir(pool_name)
            
            pool_model_filename = f"{self.train_algo}_{self.train_side}_{self.version}.zip"
            pool_model_path = pool_dir / pool_model_filename
            
            # å¤åˆ¶æ¨¡å‹åˆ°æ± 
            import shutil
            shutil.copy(str(model_path), str(pool_model_path))
            
            self.logger.info(f"âœ“ æ¨¡å‹å·²åŠ å…¥å›ºå®šæ± : {pool_model_path}")
            
            # æ›´æ–°æ± çš„metadata
            self._update_pool_metadata(pool_dir, pool_model_filename)
        
        self.logger.log_banner("", "-")
    
    def _update_pool_metadata(self, pool_dir: Path, model_filename: str):
        """æ›´æ–°å›ºå®šæ± çš„metadata"""
        metadata_path = pool_dir / "metadata.json"
        
        # åŠ è½½ç°æœ‰metadata
        if metadata_path.exists():
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {
                'pool_name': pool_dir.name,
                'created_at': datetime.now().isoformat(),
                'models': []
            }
        
        # æ·»åŠ æ–°æ¨¡å‹ä¿¡æ¯
        model_info = {
            'name': model_filename.replace('.zip', ''),
            'path': model_filename,
            'algorithm': self.train_algo,
            'training_steps': self.training_config['total_timesteps'],
            'trained_against': self.naming.format_opponent_info(self.opponent_config),
            'added_at': datetime.now().isoformat(),
            'eval_metrics': {}  # å¯ä»¥åœ¨è¯„ä¼°åå¡«å……
        }
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = [m for m in metadata['models'] if m['name'] == model_info['name']]
        if existing:
            # æ›´æ–°ç°æœ‰æ¡ç›®
            idx = metadata['models'].index(existing[0])
            metadata['models'][idx] = model_info
        else:
            # æ·»åŠ æ–°æ¡ç›®
            metadata['models'].append(model_info)
        
        # ä¿å­˜metadata
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    def evaluate(self, n_episodes: Optional[int] = None) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹
        
        Args:
            n_episodes: è¯„ä¼°episodeæ•°ï¼ˆNoneåˆ™ä½¿ç”¨é…ç½®é»˜è®¤å€¼ï¼‰
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        from stable_baselines3.common.evaluation import evaluate_policy
        
        if self.eval_env is None:
            self.logger.warning("è¯„ä¼°ç¯å¢ƒæœªåˆ›å»ºï¼Œæ— æ³•è¯„ä¼°")
            return {}
        
        n_episodes = n_episodes or self.training_config['n_eval_episodes']
        
        self.logger.log_banner(f"ğŸ“Š è¯„ä¼°æ¨¡å‹ ({n_episodes} episodes)", "-")
        
        episode_rewards, episode_lengths = evaluate_policy(
            self.algorithm.model,
            self.eval_env,
            n_eval_episodes=n_episodes,
            deterministic=self.training_config['deterministic_eval'],
            return_episode_rewards=True
        )
        
        results = {
            'mean_reward': float(np.mean(episode_rewards)),
            'std_reward': float(np.std(episode_rewards)),
            'mean_length': float(np.mean(episode_lengths)),
            'std_length': float(np.std(episode_lengths)),
            'min_reward': float(np.min(episode_rewards)),
            'max_reward': float(np.max(episode_rewards)),
            'n_episodes': n_episodes
        }
        
        self.logger.log_config(results, title="è¯„ä¼°ç»“æœ")
        self.logger.log_banner("", "-")
        
        return results
    
    def check_freeze_criteria(self, freeze_criteria: Dict[str, Any]) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å†»ç»“æ¡ä»¶
        
        Args:
            freeze_criteria: å†»ç»“æ¡ä»¶å­—å…¸
        
        Returns:
            æ˜¯å¦è¾¾åˆ°å†»ç»“æ¡ä»¶
        """
        # å…ˆè¯„ä¼°æ¨¡å‹
        eval_results = self.evaluate()
        
        if not eval_results:
            return False
        
        # æ£€æŸ¥æœ€ä½å¥–åŠ±
        min_reward = freeze_criteria.get('min_avg_reward', -np.inf)
        if eval_results['mean_reward'] < min_reward:
            self.logger.info(f"âŒ æœªè¾¾åˆ°æœ€ä½å¥–åŠ±: {eval_results['mean_reward']:.2f} < {min_reward:.2f}")
            return False
        
        # è§’è‰²ç‰¹å®šæ£€æŸ¥ï¼ˆç®€åŒ–ç‰ˆï¼Œå®Œæ•´ç‰ˆéœ€è¦åœ¨å›è°ƒä¸­å®ç°ï¼‰
        if self.train_side == "predator":
            min_catch_rate = freeze_criteria.get('min_catch_rate', 0.0)
            # è¿™é‡Œéœ€è¦ä»è¯„ä¼°ä¸­è·å–catch_rateï¼Œæš‚æ—¶ç”¨å¥–åŠ±ä»£æ›¿
            self.logger.info(f"âœ“ è¾¾åˆ°æœ€ä½å¥–åŠ±: {eval_results['mean_reward']:.2f} >= {min_reward:.2f}")
        
        elif self.train_side == "prey":
            min_survival_rate = freeze_criteria.get('min_survival_rate', 0.0)
            # è¿™é‡Œéœ€è¦ä»è¯„ä¼°ä¸­è·å–survival_rateï¼Œæš‚æ—¶ç”¨å¥–åŠ±ä»£æ›¿
            self.logger.info(f"âœ“ è¾¾åˆ°æœ€ä½å¥–åŠ±: {eval_results['mean_reward']:.2f} >= {min_reward:.2f}")
        
        return True
    
    def save_training_summary(self):
        """ä¿å­˜è®­ç»ƒæ‘˜è¦"""
        summary = {
            'experiment_name': self.experiment_name,
            'stage_name': self.stage_name,
            'train_side': self.train_side,
            'train_algo': self.train_algo,
            'version': self.version,
            'generation': self.generation,
            'run_mode': self.run_mode,
            'opponent': self.naming.format_opponent_info(self.opponent_config),
            'training_config': self.training_config,
            'training_time_seconds': self.total_training_time,
            'final_model_path': str(self.final_model_path) if self.final_model_path else None,
            'notes': self.notes
        }
        
        summary_filename = self.naming.generate_summary_filename(
            train_algo=self.train_algo,
            train_side=self.train_side,
            version=self.version
        )
        
        save_training_summary(
            summary=summary,
            save_dir=self.experiment_dir,
            name=summary_filename.replace('.json', '')
        )
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.logger.info("æ¸…ç†èµ„æº...")
        
        if self.train_env:
            self.train_env.close()
        
        if self.eval_env:
            self.eval_env.close()
        
        if self.env_manager:
            self.env_manager.close()
        
        self.logger.info("âœ“ èµ„æºæ¸…ç†å®Œæˆ")
    
    def run(
        self,
        save_to_pool: bool = False,
        pool_name: Optional[str] = None,
        check_freeze: bool = False,
        freeze_criteria: Optional[Dict[str, Any]] = None
    ):
        """
        å®Œæ•´è®­ç»ƒæµç¨‹ï¼ˆsetup â†’ train â†’ evaluate â†’ saveï¼‰
        
        Args:
            save_to_pool: æ˜¯å¦ä¿å­˜åˆ°å›ºå®šæ± 
            pool_name: æ± åç§°
            check_freeze: æ˜¯å¦æ£€æŸ¥å†»ç»“æ¡ä»¶
            freeze_criteria: å†»ç»“æ¡ä»¶
        """
        try:
            # 1. è®¾ç½®
            self.setup()
            
            # 2. è®­ç»ƒ
            self.train()
            
            # 3. æœ€ç»ˆè¯„ä¼°
            final_eval = self.evaluate()
            
            # 4. æ£€æŸ¥å†»ç»“æ¡ä»¶
            can_freeze = True
            if check_freeze and freeze_criteria:
                can_freeze = self.check_freeze_criteria(freeze_criteria)
                
                if can_freeze:
                    self.logger.log_banner("â„ï¸  æ¨¡å‹è¾¾åˆ°å†»ç»“æ ‡å‡†", "=")
                else:
                    self.logger.log_banner("âš ï¸  æ¨¡å‹æœªè¾¾åˆ°å†»ç»“æ ‡å‡†", "=")
            
            # 5. ä¿å­˜æ¨¡å‹
            should_save_to_pool = save_to_pool and (not check_freeze or can_freeze)
            self.save_model(save_to_pool=should_save_to_pool, pool_name=pool_name)
            
            # 6. ä¿å­˜è®­ç»ƒæ‘˜è¦
            self.save_training_summary()
            
            return final_eval
        
        finally:
            # 7. æ¸…ç†
            self.cleanup()


# å¯¼å…¥numpyï¼ˆå‰é¢å¿˜è®°äº†ï¼‰
import numpy as np