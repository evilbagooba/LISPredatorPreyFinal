"""
Stage 1.1: Preyé¢„çƒ­è®­ç»ƒ
è®­ç»ƒæ‰€æœ‰ç®—æ³•çš„Preyå¯¹æŠ—RANDOM Predator
"""

import argparse
import sys
from pathlib import Path

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.trainer import MultiAgentTrainer
from src.utils.config_loader import get_training_config, get_env_config  # â† åŠ ä¸Š get_env_config


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Stage 1.1: Preyé¢„çƒ­è®­ç»ƒ')
    
    parser.add_argument('--mode', type=str, default='prod',
                        choices=['debug', 'dryrun', 'test', 'prod'],
                        help='è¿è¡Œæ¨¡å¼')
    
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3'],
                        help='è¦è®­ç»ƒçš„ç®—æ³•åˆ—è¡¨')
    
    parser.add_argument('--env-config', type=str, default='waterworld_standard',
                        help='ç¯å¢ƒé…ç½®')
    
    parser.add_argument('--device', type=str, default='auto',
                        help='è®¡ç®—è®¾å¤‡')
    
    parser.add_argument('--timesteps', type=int, default=None,
                        help='æ€»è®­ç»ƒæ­¥æ•°ï¼ˆè¦†ç›–é…ç½®ï¼‰')
    
    return parser.parse_args()


def train_one_prey_algo(algo: str, args, stage_config: dict):
    """è®­ç»ƒå•ä¸ªPreyç®—æ³•"""
    
    print(f"\n{'='*70}")
    print(f"{'è®°å½•' if algo == 'RANDOM' else 'è®­ç»ƒ'} {algo}_prey vs RANDOM_predator")
    print(f"{'='*70}\n")
    
    # æ„å»ºå¯¹æ‰‹é…ç½®
    opponent_config = {
        'type': 'algorithm',
        'side': 'predator',
        'algorithm': 'RANDOM',
        'freeze': True
    }
    
    # ç¡®å®šè®­ç»ƒæ­¥æ•°
    if algo == 'RANDOM':
        # RANDOM åªéœ€è¦è¿è¡Œè¶³å¤Ÿçš„ episodes æ¥è®°å½•åŸºçº¿
        # ä¾‹å¦‚è¿è¡Œ 50k æ­¥ï¼ˆå¤§çº¦ 50 ä¸ª episodesï¼‰
        timesteps = 5000
    else:
        # æ­£å¸¸ç®—æ³•çš„è®­ç»ƒæ­¥æ•°
        timesteps = args.timesteps
    
    # åˆ›å»ºè®­ç»ƒå™¨
    # âœ… æ ¹æ®æ¨¡å¼é€‰æ‹©ç¯å¢ƒé…ç½®
    if args.mode == 'test':
        env_config_name = 'waterworld_fast'
        print(f"ğŸƒ æµ‹è¯•æ¨¡å¼ï¼Œä½¿ç”¨å¿«é€Ÿç¯å¢ƒ: max_cycles=500")
    else:
        env_config_name = 'waterworld_standard'
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiAgentTrainer(
        train_side='prey',
        train_algo=algo,
        opponent_config=opponent_config,
        experiment_name=f"{algo}_prey_warmup",
        stage_name=stage_config['stage']['name'],
        generation=stage_config['stage']['generation'],
        version='v1',
        run_mode=args.mode,
        env_config=get_env_config(env_config_name),  # âœ… ä½¿ç”¨åŠ¨æ€ç¯å¢ƒ
        total_timesteps=timesteps,
        device=args.device
    )
    
    # è·å–å†»ç»“æ¡ä»¶
    freeze_config = stage_config.get('freeze_on_success', {})
    freeze_criteria = freeze_config.get('criteria', {})
    
    # è¿è¡Œè®­ç»ƒ
    # RANDOM ä¸éœ€è¦ä¿å­˜åˆ°æ± 
    save_to_pool = freeze_config.get('enabled', False) if algo != 'RANDOM' else False
    
    eval_results = trainer.run(
        save_to_pool=save_to_pool,
        pool_name=freeze_config.get('save_to_pool'),
        check_freeze=freeze_config.get('enabled', False) if algo != 'RANDOM' else False,
        freeze_criteria=freeze_criteria
    )
    
    return eval_results

def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åŠ è½½Stage 1.1é…ç½®
    stage_config = get_training_config('stage1_1_prey_warmup')
    
    # è·å–ç®—æ³•åˆ—è¡¨
    algos_to_train = args.algos or stage_config.get('algorithms_to_train', ['PPO', 'A2C', 'SAC', 'TD3'])
    
    print(f"\n{'='*70}")
    print(f"ğŸ¯ Stage 1.1: Preyé¢„çƒ­è®­ç»ƒ")
    print(f"{'='*70}")
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"è®­ç»ƒç®—æ³•: {', '.join(algos_to_train)}")
    print(f"{'='*70}\n")
    
    # ========== æ–°å¢ï¼šå…ˆè¿è¡Œ RANDOM ä½œä¸ºåŸºçº¿ ==========
    print(f"\n{'='*70}")
    print(f"æ­¥éª¤ 0: è®°å½• RANDOM Baseline")
    print(f"{'='*70}\n")
    
    # å°† RANDOM æ·»åŠ åˆ°è®­ç»ƒåˆ—è¡¨çš„æœ€å‰é¢
    all_algos = ['RANDOM'] + algos_to_train
    
    # ========== è®­ç»ƒæ‰€æœ‰ç®—æ³•ï¼ˆåŒ…æ‹¬ RANDOMï¼‰==========
    results = {}
    for algo in all_algos:
        try:
            eval_results = train_one_prey_algo(algo, args, stage_config)
            results[algo] = eval_results
        except KeyboardInterrupt:
            print(f"\nâš ï¸  {algo} è®­ç»ƒè¢«ä¸­æ–­")
            break
        except Exception as e:
            print(f"\nâŒ {algo} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # æ‰“å°æ±‡æ€»ï¼ˆçªå‡ºæ˜¾ç¤ºä¸ RANDOM çš„å¯¹æ¯”ï¼‰
    print(f"\n{'='*70}")
    print(f"âœ… Stage 1.1 å®Œæˆ")
    print(f"{'='*70}")
    
    # æ˜¾ç¤º RANDOM åŸºçº¿
    if 'RANDOM' in results and results['RANDOM']:
        baseline_reward = results['RANDOM'].get('mean_reward', 0)
        print(f"\nğŸ“Š Random Baseline: {baseline_reward:.2f}")
        
        print(f"\nè®­ç»ƒç»“æœï¼ˆç›¸å¯¹äº Randomï¼‰:")
        for algo in algos_to_train:  # åªæ˜¾ç¤ºè®­ç»ƒçš„ç®—æ³•
            if algo in results and results[algo]:
                reward = results[algo].get('mean_reward', 0)
                improvement = ((reward - baseline_reward) / abs(baseline_reward) * 100) if baseline_reward != 0 else 0
                status = "âœ…" if improvement > 0 else "âŒ"
                print(f"  {status} {algo}: {reward:.2f} ({improvement:+.1f}% vs Random)")
    else:
        print(f"\nè®­ç»ƒç»“æœ:")
        for algo, result in results.items():
            if result:
                print(f"  {algo}: å¹³å‡å¥–åŠ± = {result.get('mean_reward', 'N/A'):.2f}")
    
    print(f"{'='*70}\n")

if __name__ == "__main__":
    main()