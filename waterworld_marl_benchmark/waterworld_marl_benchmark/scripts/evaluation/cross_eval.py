"""
äº¤å‰è¯„ä¼°ä¸»è„šæœ¬
æ‰§è¡Œ 5Ã—5 ç®—æ³•å¯¹æˆ˜çŸ©é˜µè¯„ä¼°
"""

import sys
import argparse
import pickle
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from eval_single_matchup import evaluate_single_matchup
from metrics_calculator import MetricsCalculator
from src.utils.config_loader import config_loader


def parse_args():
    parser = argparse.ArgumentParser(description='äº¤å‰è¯„ä¼°')
    parser.add_argument('--mode', type=str, default='test',
                        choices=['test', 'dryrun', 'prod'],
                        help='è¯„ä¼°æ¨¡å¼')
    parser.add_argument('--n-episodes', type=int, default=None,
                        help='æ¯ä¸ªç»„åˆçš„è¯„ä¼°episodeæ•°')
    parser.add_argument('--algos', type=str, nargs='+',
                        default=['PPO', 'A2C', 'SAC', 'TD3', 'RANDOM'],
                        help='è¦è¯„ä¼°çš„ç®—æ³•åˆ—è¡¨')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='è¾“å‡ºç›®å½•')
    return parser.parse_args()


def find_model_path(base_dir: Path, algo: str, side: str) -> Path:
    """
    æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
    
    ä¼˜å…ˆçº§ï¼š
    1. æœ€æ–°çš„æ¨¡å‹ï¼ˆæŒ‰æ—¶é—´æˆ³ï¼‰
    2. å›ºå®šæ± ä¸­çš„æ¨¡å‹
    """
    # æœç´¢è·¯å¾„
    search_patterns = [
        base_dir / f"stage1.*_{side}_*" / f"{algo}_{side}_*.zip",
        base_dir / f"*{side}*" / f"{algo}_{side}_*.zip",
        base_dir / f"**/{algo}_{side}_*.zip",
    ]
    
    found_models = []
    for pattern in search_patterns:
        found_models.extend(base_dir.glob(str(pattern.relative_to(base_dir))))
    
    if not found_models:
        return None
    
    # è¿”å›æœ€æ–°çš„
    found_models.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    return found_models[0]


def main():
    args = parse_args()
    
    # åŠ è½½é…ç½®
    eval_config = config_loader.load_yaml('evaluation/cross_eval.yaml')
    mode_config = eval_config['evaluation']['model_pools'][args.mode]
    
    # å‚æ•°
    algos = args.algos
    n_episodes = args.n_episodes or eval_config['evaluation']['n_episodes']
    output_dir = Path(args.output_dir) if args.output_dir else \
                 Path(f"{args.mode}_outputs/evaluation_results")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = output_dir / f"cross_eval_{timestamp}"
    run_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*70)
    print("ğŸ§ª äº¤å‰è¯„ä¼° (Cross-Algorithm Evaluation)")
    print("="*70)
    print(f"æ¨¡å¼:        {args.mode}")
    print(f"ç®—æ³•:        {', '.join(algos)}")
    print(f"Episodes:    {n_episodes}")
    print(f"è¾“å‡ºç›®å½•:    {run_dir}")
    print("="*70 + "\n")
    
    # æŸ¥æ‰¾æ‰€æœ‰æ¨¡å‹
    print("ğŸ“‚ æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶...")
    
    saved_models_base = Path(mode_config['saved_models_base'])
    
    predator_models = {}
    prey_models = {}
    
    for algo in algos:
        if algo == 'RANDOM':
            predator_models[algo] = None
            prey_models[algo] = None
            print(f"  âœ“ {algo}: ä½¿ç”¨éšæœºç­–ç•¥")
            continue
        
        # æŸ¥æ‰¾ predator
        pred_path = find_model_path(saved_models_base, algo, 'predator')
        if pred_path:
            predator_models[algo] = pred_path
            print(f"  âœ“ {algo}_predator: {pred_path.name}")
        else:
            print(f"  âœ— {algo}_predator: æœªæ‰¾åˆ°")
            predator_models[algo] = None
        
        # æŸ¥æ‰¾ prey
        prey_path = find_model_path(saved_models_base, algo, 'prey')
        if prey_path:
            prey_models[algo] = prey_path
            print(f"  âœ“ {algo}_prey: {prey_path.name}")
        else:
            print(f"  âœ— {algo}_prey: æœªæ‰¾åˆ°")
            prey_models[algo] = None
    
    # æ‰§è¡Œäº¤å‰è¯„ä¼°
    print(f"\n{'='*70}")
    print(f"ğŸ¯ å¼€å§‹äº¤å‰è¯„ä¼° ({len(algos)}Ã—{len(algos)} = {len(algos)**2} ç»„åˆ)")
    print(f"{'='*70}\n")
    
    results_matrix = {}
    total_matchups = len(algos) * len(algos)
    current_matchup = 0
    
    for pred_algo in algos:
        results_matrix[pred_algo] = {}
        
        for prey_algo in algos:
            current_matchup += 1
            print(f"\n[{current_matchup}/{total_matchups}] "
                  f"è¯„ä¼°: {pred_algo}_pred vs {prey_algo}_prey")
            print("-" * 70)
            
            # è·å–æ¨¡å‹è·¯å¾„
            pred_path = predator_models.get(pred_algo)
            prey_path = prey_models.get(prey_algo)
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
            if pred_path is None and pred_algo != 'RANDOM':
                print(f"  âš ï¸  è·³è¿‡: {pred_algo}_predator æ¨¡å‹æœªæ‰¾åˆ°")
                results_matrix[pred_algo][prey_algo] = None
                continue
            
            if prey_path is None and prey_algo != 'RANDOM':
                print(f"  âš ï¸  è·³è¿‡: {prey_algo}_prey æ¨¡å‹æœªæ‰¾åˆ°")
                results_matrix[pred_algo][prey_algo] = None
                continue
            
            # æ‰§è¡Œè¯„ä¼°
            try:
                metrics = evaluate_single_matchup(
                    predator_model_path=pred_path,
                    prey_model_path=prey_path,
                    predator_algo=pred_algo,
                    prey_algo=prey_algo,
                    env_config_name='waterworld_fast',
                    n_episodes=n_episodes,
                    deterministic=True,
                    verbose=1
                )
                
                results_matrix[pred_algo][prey_algo] = metrics
                
            except Exception as e:
                print(f"  âŒ è¯„ä¼°å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                results_matrix[pred_algo][prey_algo] = None
    
    # ä¿å­˜åŸå§‹ç»“æœ
    print(f"\n{'='*70}")
    print("ğŸ’¾ ä¿å­˜ç»“æœ...")
    print(f"{'='*70}")
    
    # ä¿å­˜ä¸ºpickleï¼ˆå®Œæ•´æ•°æ®ï¼‰
    raw_results_path = run_dir / "raw_results.pkl"
    with open(raw_results_path, 'wb') as f:
        pickle.dump(results_matrix, f)
    print(f"  âœ“ åŸå§‹ç»“æœ: {raw_results_path}")
    
    # ä¿å­˜ä¸ºJSONï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
    json_results = {}
    for pred_algo, prey_dict in results_matrix.items():
        json_results[pred_algo] = {}
        for prey_algo, metrics in prey_dict.items():
            if metrics is not None:
                # åªä¿å­˜ä¸»è¦æŒ‡æ ‡
                json_results[pred_algo][prey_algo] = {
                    'catch_rate': metrics['catch_rate'],
                    'survival_rate': metrics['survival_rate'],
                    'pred_avg_reward': metrics['pred_avg_reward'],
                    'prey_avg_reward': metrics['prey_avg_reward'],
                    'balance_score': metrics['balance_score'],
                    'is_ood': metrics['is_ood']
                }
            else:
                json_results[pred_algo][prey_algo] = None
    
    json_path = run_dir / "results_summary.json"
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_results, f, indent=2, ensure_ascii=False)
    print(f"  âœ“ JSONæ‘˜è¦: {json_path}")
    
    # è®¡ç®—è‡ªé€‚åº”æ€§å¾—åˆ†
    print(f"\n{'='*70}")
    print("ğŸ“Š è®¡ç®—è‡ªé€‚åº”æ€§å¾—åˆ†...")
    print(f"{'='*70}\n")
    
    # è¿‡æ»¤æ‰Noneç»“æœ
    valid_results = {}
    for pred_algo, prey_dict in results_matrix.items():
        if pred_algo == 'RANDOM':
            continue
        valid_results[pred_algo] = {}
        for prey_algo, metrics in prey_dict.items():
            if metrics is not None:
                valid_results[pred_algo][prey_algo] = metrics
    
    if valid_results:
        adaptability_scores = MetricsCalculator.compute_adaptability_scores(valid_results)
        ranking = MetricsCalculator.compute_ranking(adaptability_scores)
        
        # æ‰“å°æ’å
        print(f"{'='*80}")
        print("Algorithm Adaptability Ranking (Predator)")
        print(f"{'='*80}")
        print(f"{'Rank':<6} {'Algorithm':<10} {'In-Dist':<10} {'OOD Avg':<10} "
              f"{'Adapt':<8} {'Drop':<8} {'Std':<8}")
        print("-"*80)
        
        for scores in ranking:
            print(f"{scores['rank']:<6} {scores['algorithm']:<10} "
                  f"{scores['in_dist_performance']:<10.3f} "
                  f"{scores['ood_avg_performance']:<10.3f} "
                  f"{scores['adaptability_score']:<8.3f} "
                  f"{scores['performance_drop']:<8.3f} "
                  f"{scores['ood_std']:<8.3f}")
        
        print(f"{'='*80}\n")
        
        # ä¿å­˜è‡ªé€‚åº”æ€§å¾—åˆ†
        adapt_path = run_dir / "adaptability_scores.json"
        with open(adapt_path, 'w', encoding='utf-8') as f:
            json.dump({
                'scores': adaptability_scores,
                'ranking': ranking
            }, f, indent=2, ensure_ascii=False)
        print(f"  âœ“ è‡ªé€‚åº”æ€§å¾—åˆ†: {adapt_path}")
    
    # ç”Ÿæˆæ€§èƒ½çŸ©é˜µï¼ˆCSVæ ¼å¼ï¼‰
    print(f"\n{'='*70}")
    print("ğŸ“‹ ç”Ÿæˆæ€§èƒ½çŸ©é˜µ...")
    print(f"{'='*70}\n")
    
    # Catch RateçŸ©é˜µ
    catch_rate_matrix = []
    header = ['Predator\\Prey'] + algos
    catch_rate_matrix.append(header)
    
    for pred_algo in algos:
        row = [pred_algo]
        for prey_algo in algos:
            metrics = results_matrix[pred_algo].get(prey_algo)
            if metrics:
                row.append(f"{metrics['catch_rate']:.3f}")
            else:
                row.append("N/A")
        catch_rate_matrix.append(row)
    
    # ä¿å­˜ä¸ºCSV
    csv_path = run_dir / "catch_rate_matrix.csv"
    import csv
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerows(catch_rate_matrix)
    print(f"  âœ“ Catch RateçŸ©é˜µ: {csv_path}")
    
    # æ‰“å°çŸ©é˜µé¢„è§ˆ
    print("\n  Catch Rate Matrix Preview:")
    print("  " + "-" * 60)
    for row in catch_rate_matrix[:6]:  # åªæ˜¾ç¤ºå‰6è¡Œ
        print("  " + " | ".join(f"{cell:>12}" for cell in row))
    print("  " + "-" * 60)
    
    # ä¿å­˜é…ç½®å¿«ç…§
    config_snapshot = {
        'mode': args.mode,
        'algorithms': algos,
        'n_episodes': n_episodes,
        'timestamp': timestamp,
        'model_paths': {
            'predators': {k: str(v) if v else None for k, v in predator_models.items()},
            'preys': {k: str(v) if v else None for k, v in prey_models.items()}
        }
    }
    
    config_path = run_dir / "eval_config.json"
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_snapshot, f, indent=2, ensure_ascii=False)
    print(f"  âœ“ é…ç½®å¿«ç…§: {config_path}")
    
    # å®Œæˆ
    print(f"\n{'='*70}")
    print("âœ… äº¤å‰è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*70}")
    print(f"\nğŸ“ ç»“æœä¿å­˜åœ¨: {run_dir}")
    print("\nä¸‹ä¸€æ­¥ï¼š")
    print(f"  1. æŸ¥çœ‹ç»“æœ: cat {json_path}")
    print(f"  2. ç”Ÿæˆå¯è§†åŒ–: python scripts/analysis/plot_results.py --input {run_dir}")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()