"""
Waterworld: Flexible Training System
æ”¯æŒä»»æ„ agent ç»„åˆçš„è®­ç»ƒé…ç½® + TensorBoardé›†æˆ
"""

from pettingzoo.sisl import waterworld_v4
import supersuit as ss
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecEnv, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import torch
import matplotlib.pyplot as plt
from scipy.ndimage import uniform_filter1d
import gymnasium as gym
from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Union
import os
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter


# ============================================================================
# ç­–ç•¥æ¥å£ï¼šæ”¯æŒå¤šç§å›ºå®šç­–ç•¥
# ============================================================================

class AgentPolicy(ABC):
    """å›ºå®š Agent ç­–ç•¥çš„æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def get_action(self, obs):
        """
        æ ¹æ®è§‚å¯Ÿè·å–åŠ¨ä½œ
        
        Args:
            obs: è§‚å¯Ÿå€¼ (obs_dim,)
            
        Returns:
            action: åŠ¨ä½œ (action_dim,)
        """
        pass
    
    @abstractmethod
    def reset(self):
        """é‡ç½®ç­–ç•¥çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        pass
    
    def __repr__(self):
        return f"{self.__class__.__name__}()"


class RandomPolicy(AgentPolicy):
    """éšæœºç­–ç•¥"""
    
    def __init__(self, action_dim=2, low=-1.0, high=1.0):
        self.action_dim = action_dim
        self.low = low
        self.high = high
    
    def get_action(self, obs):
        return np.random.uniform(
            low=self.low,
            high=self.high,
            size=self.action_dim
        ).astype(np.float32)
    
    def reset(self):
        pass


class TrainedModelPolicy(AgentPolicy):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ä½œä¸ºç­–ç•¥"""
    
    def __init__(self, model_path, device='cpu'):
        """
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (.zip æ–‡ä»¶)
            device: 'cpu' æˆ– 'cuda'
        """
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")
        
        self.model = PPO.load(model_path, device=device)
        self.model_path = model_path
        print(f"    Loaded model: {model_path}")
    
    def get_action(self, obs):
        action, _ = self.model.predict(obs, deterministic=True)
        return action
    
    def reset(self):
        pass
    
    def __repr__(self):
        return f"TrainedModelPolicy('{os.path.basename(self.model_path)}')"


class RuleBasedPolicy(AgentPolicy):
    """åŸºäºè§„åˆ™çš„ç­–ç•¥ï¼ˆå¯æ‰©å±•ï¼‰"""
    
    def __init__(self, rule_type='stay'):
        """
        Args:
            rule_type: è§„åˆ™ç±»å‹
                - 'stay': ä¿æŒé™æ­¢
                - 'forward': å‘å‰ç§»åŠ¨
                - 'circle': åœ†å‘¨è¿åŠ¨
        """
        self.rule_type = rule_type
        self.step_count = 0
    
    def get_action(self, obs):
        self.step_count += 1
        
        if self.rule_type == 'stay':
            return np.array([0.0, 0.0], dtype=np.float32)
        
        elif self.rule_type == 'forward':
            return np.array([1.0, 0.0], dtype=np.float32)
        
        elif self.rule_type == 'circle':
            angle = self.step_count * 0.1
            return np.array([np.cos(angle), np.sin(angle)], dtype=np.float32)
        
        else:
            return np.array([0.0, 0.0], dtype=np.float32)
    
    def reset(self):
        self.step_count = 0
    
    def __repr__(self):
        return f"RuleBasedPolicy('{self.rule_type}')"


# ============================================================================
# Agent é…ç½®ç³»ç»Ÿ
# ============================================================================

class AgentConfig:
    """Agent é…ç½®ç±»"""
    
    def __init__(self, agent_idx, agent_type, agent_name, role, policy=None):
        """
        Args:
            agent_idx: Agent åœ¨ç¯å¢ƒä¸­çš„å…¨å±€ç´¢å¼•
            agent_type: 'predator' æˆ– 'prey'
            agent_name: Agent åç§°ï¼Œä¾‹å¦‚ 'predator_0'
            role: 'training' æˆ– 'fixed'
            policy: AgentPolicy å®ä¾‹ (role='fixed' æ—¶å¿…é¡»æä¾›)
        """
        self.agent_idx = agent_idx
        self.agent_type = agent_type
        self.agent_name = agent_name
        self.role = role
        self.policy = policy
        
        # éªŒè¯
        if role not in ['training', 'fixed']:
            raise ValueError(f"role must be 'training' or 'fixed', got: {role}")
        
        if role == 'fixed' and policy is None:
            raise ValueError(f"policy must be provided when role='fixed' for agent {agent_name}")
        
        if role == 'training' and policy is not None:
            raise ValueError(f"policy should be None when role='training' for agent {agent_name}")
    
    def __repr__(self):
        if self.role == 'training':
            return f"AgentConfig({self.agent_name}, role=training)"
        else:
            return f"AgentConfig({self.agent_name}, role=fixed, policy={self.policy})"


def create_agent_configs(
    n_predators: int,
    n_preys: int,
    train_predators: Optional[List[int]] = None,
    train_preys: Optional[List[int]] = None,
    predator_policies: Optional[Union[AgentPolicy, Dict[int, AgentPolicy]]] = None,
    prey_policies: Optional[Union[AgentPolicy, Dict[int, AgentPolicy]]] = None
) -> List[AgentConfig]:
    """
    åˆ›å»º Agent é…ç½®åˆ—è¡¨
    
    Args:
        n_predators: Predator æ€»æ•°
        n_preys: Prey æ€»æ•°
        train_predators: è¦è®­ç»ƒçš„ predator ç´¢å¼•åˆ—è¡¨ï¼ˆ0-basedï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨å›ºå®š
        train_preys: è¦è®­ç»ƒçš„ prey ç´¢å¼•åˆ—è¡¨ï¼ˆ0-basedï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨å›ºå®š
        predator_policies: Predator çš„å›ºå®šç­–ç•¥
            - AgentPolicy: æ‰€æœ‰å›ºå®š predators ä½¿ç”¨ç›¸åŒç­–ç•¥
            - Dict[int, AgentPolicy]: æ¯ä¸ª predator ä½¿ç”¨ä¸åŒç­–ç•¥ {predator_idx: policy}
        prey_policies: Prey çš„å›ºå®šç­–ç•¥ï¼ˆåŒä¸Šï¼‰
    
    Returns:
        List[AgentConfig]: Agent é…ç½®åˆ—è¡¨
    """
    configs = []
    
    # é»˜è®¤å€¼
    if train_predators is None:
        train_predators = []
    if train_preys is None:
        train_preys = []
    
    # éªŒè¯ç´¢å¼•èŒƒå›´
    if any(i >= n_predators or i < 0 for i in train_predators):
        raise ValueError(f"train_predators indices must be in [0, {n_predators-1}]")
    if any(i >= n_preys or i < 0 for i in train_preys):
        raise ValueError(f"train_preys indices must be in [0, {n_preys-1}]")
    
    # é…ç½® Predatorsï¼ˆç´¢å¼• 0 åˆ° n_predators-1ï¼‰
    for pred_idx in range(n_predators):
        agent_idx = pred_idx
        agent_name = f'predator_{pred_idx}'
        
        if pred_idx in train_predators:
            # è®­ç»ƒçš„ predator
            config = AgentConfig(
                agent_idx=agent_idx,
                agent_type='predator',
                agent_name=agent_name,
                role='training',
                policy=None
            )
        else:
            # å›ºå®šçš„ predator
            if predator_policies is None:
                raise ValueError(f"predator_policies must be provided for fixed predator {pred_idx}")
            
            # è·å–è¯¥ predator çš„ç­–ç•¥
            if isinstance(predator_policies, dict):
                if pred_idx not in predator_policies:
                    raise ValueError(f"No policy provided for predator {pred_idx} in predator_policies dict")
                policy = predator_policies[pred_idx]
            else:
                policy = predator_policies
            
            config = AgentConfig(
                agent_idx=agent_idx,
                agent_type='predator',
                agent_name=agent_name,
                role='fixed',
                policy=policy
            )
        
        configs.append(config)
    
    # é…ç½® Preysï¼ˆç´¢å¼• n_predators åˆ° n_predators+n_preys-1ï¼‰
    for prey_idx in range(n_preys):
        agent_idx = n_predators + prey_idx
        agent_name = f'prey_{prey_idx}'
        
        if prey_idx in train_preys:
            # è®­ç»ƒçš„ prey
            config = AgentConfig(
                agent_idx=agent_idx,
                agent_type='prey',
                agent_name=agent_name,
                role='training',
                policy=None
            )
        else:
            # å›ºå®šçš„ prey
            if prey_policies is None:
                raise ValueError(f"prey_policies must be provided for fixed prey {prey_idx}")
            
            # è·å–è¯¥ prey çš„ç­–ç•¥
            if isinstance(prey_policies, dict):
                if prey_idx not in prey_policies:
                    raise ValueError(f"No policy provided for prey {prey_idx} in prey_policies dict")
                policy = prey_policies[prey_idx]
            else:
                policy = prey_policies
            
            config = AgentConfig(
                agent_idx=agent_idx,
                agent_type='prey',
                agent_name=agent_name,
                role='fixed',
                policy=policy
            )
        
        configs.append(config)
    
    return configs


def print_agent_configs(configs: List[AgentConfig]):
    """æ‰“å° Agent é…ç½®ä¿¡æ¯"""
    print("\n" + "="*70)
    print("Agent Configuration")
    print("="*70)
    
    training_agents = [c for c in configs if c.role == 'training']
    fixed_agents = [c for c in configs if c.role == 'fixed']
    
    print(f"\nğŸ“Š Summary:")
    print(f"  Total Agents: {len(configs)}")
    print(f"  Training Agents: {len(training_agents)}")
    print(f"  Fixed Agents: {len(fixed_agents)}")
    
    if training_agents:
        print(f"\nğŸ¯ Training Agents ({len(training_agents)}):")
        for config in training_agents:
            print(f"  - {config.agent_name} (idx={config.agent_idx})")
    
    if fixed_agents:
        print(f"\nğŸ”’ Fixed Agents ({len(fixed_agents)}):")
        for config in fixed_agents:
            print(f"  - {config.agent_name} (idx={config.agent_idx}): {config.policy}")
    
    print("="*70)


# ============================================================================
# çµæ´»çš„ VecEnv åŒ…è£…å™¨
# ============================================================================

class FlexibleMixedAgentVecEnv(VecEnv):
    """
    æ”¯æŒä»»æ„ agent ç»„åˆè®­ç»ƒçš„è‡ªå®šä¹‰ VecEnv
    """

    def __init__(self, venv, agent_configs: List[AgentConfig]):
        """
        Args:
            venv: åŒ…è£…åçš„å‘é‡åŒ–ç¯å¢ƒ
            agent_configs: Agent é…ç½®åˆ—è¡¨
        """
        self.venv = venv
        self.agent_configs = agent_configs
        self.n_total_agents = len(agent_configs)
        
        # åˆ†ç¦»è®­ç»ƒå’Œå›ºå®šçš„ agents
        self.training_configs = [c for c in agent_configs if c.role == 'training']
        self.fixed_configs = [c for c in agent_configs if c.role == 'fixed']
        
        self.training_indices = [c.agent_idx for c in self.training_configs]
        self.fixed_indices = [c.agent_idx for c in self.fixed_configs]
        
        self.n_training = len(self.training_indices)
        self.n_fixed = len(self.fixed_indices)
        
        if self.n_training == 0:
            raise ValueError("Must have at least one training agent")
        
        # åˆ›å»ºç´¢å¼•åˆ°é…ç½®çš„æ˜ å°„
        self.idx_to_config = {c.agent_idx: c for c in agent_configs}
        
        # è·å–åŸå§‹ç©ºé—´
        original_obs_space = venv.observation_space
        original_action_space = venv.action_space
        
        # åˆ›å»ºæ–°çš„ VecEnvï¼Œnum_envs = è®­ç»ƒ agents çš„æ•°é‡
        super().__init__(
            num_envs=self.n_training,
            observation_space=original_obs_space,
            action_space=original_action_space
        )
        
        # ç¼“å­˜æœ€æ–°çš„è§‚å¯Ÿå€¼
        self.latest_obs = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\n  FlexibleMixedAgentVecEnv initialized:")
        print(f"    - Total agents: {self.n_total_agents}")
        print(f"    - Training agents: {self.n_training}")
        print(f"    - Fixed agents: {self.n_fixed}")

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        obs = self.venv.reset()
        self.latest_obs = obs
        
        # é‡ç½®æ‰€æœ‰å›ºå®šç­–ç•¥
        for config in self.fixed_configs:
            config.policy.reset()
        
        # è¿”å›è®­ç»ƒ agents çš„è§‚å¯Ÿ
        training_obs = obs[self.training_indices]
        return training_obs

    def step_async(self, actions):
        """
        ç»„åˆè®­ç»ƒ agents çš„åŠ¨ä½œå’Œå›ºå®š agents çš„åŠ¨ä½œ
        
        Args:
            actions: shape (n_training, action_dim) - è®­ç»ƒ agents çš„åŠ¨ä½œ
        """
        # ç”Ÿæˆå›ºå®š agents çš„åŠ¨ä½œ
        fixed_actions = np.zeros((self.n_fixed, 2), dtype=np.float32)
        for i, config in enumerate(self.fixed_configs):
            agent_idx = config.agent_idx
            obs = self.latest_obs[agent_idx] if self.latest_obs is not None else None
            fixed_actions[i] = config.policy.get_action(obs)
        
        # ç»„åˆæ‰€æœ‰åŠ¨ä½œ
        full_actions = np.zeros((self.n_total_agents, 2), dtype=np.float32)
        
        # å¡«å……è®­ç»ƒ agents çš„åŠ¨ä½œ
        for i, agent_idx in enumerate(self.training_indices):
            full_actions[agent_idx] = actions[i]
        
        # å¡«å……å›ºå®š agents çš„åŠ¨ä½œ
        for i, agent_idx in enumerate(self.fixed_indices):
            full_actions[agent_idx] = fixed_actions[i]
        
        # ä¼ é€’ç»™åº•å±‚ç¯å¢ƒ
        self.venv.step_async(full_actions)

    def step_wait(self):
        """è·å–ç¯å¢ƒç»“æœ"""
        obs, rewards, dones, infos = self.venv.step_wait()
        
        # ç¼“å­˜è§‚å¯Ÿå€¼
        self.latest_obs = obs
        
        # æå–è®­ç»ƒ agents çš„æ•°æ®
        training_obs = obs[self.training_indices]
        training_rewards = rewards[self.training_indices]
        training_dones = dones[self.training_indices]
        training_infos = [infos[i] for i in self.training_indices]
        
        return training_obs, training_rewards, training_dones, training_infos

    def close(self):
        """å…³é—­åº•å±‚ç¯å¢ƒ"""
        return self.venv.close()

    def get_attr(self, attr_name, indices=None):
        return self.venv.get_attr(attr_name, indices)

    def set_attr(self, attr_name, value, indices=None):
        return self.venv.set_attr(attr_name, value, indices)

    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):
        return self.venv.env_method(method_name, *method_args, indices=indices, **method_kwargs)

    def env_is_wrapped(self, wrapper_class, indices=None):
        return self.venv.env_is_wrapped(wrapper_class, indices)


# ============================================================================
# è®­ç»ƒç›‘æ§å›è°ƒ + TensorBoardé›†æˆ
# ============================================================================

class TrainingMonitorCallback(BaseCallback):
    """
    ç›‘æ§è®­ç»ƒè¿‡ç¨‹ + æ€§èƒ½æŒ‡æ ‡ + TensorBoardæ—¥å¿—
    
    è®°å½•æŒ‡æ ‡ï¼š
    - Individual Level: æ¯ä¸ªè®­ç»ƒagentçš„ç‹¬ç«‹æ•°æ®
    - Average Level: æ‰€æœ‰è®­ç»ƒagentçš„å¹³å‡æ•°æ®
    """

    def __init__(self, training_agent_names, log_dir=None, check_freq=1000, verbose=1):
        """
        Args:
            training_agent_names: è®­ç»ƒ agents çš„åç§°åˆ—è¡¨
            log_dir: TensorBoardæ—¥å¿—ç›®å½•ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨TensorBoardï¼‰
            check_freq: æ‰“å°é¢‘ç‡
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.training_agent_names = training_agent_names
        self.check_freq = check_freq
        
        # === å…¨å±€ç»Ÿè®¡ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰ ===
        self.episode_rewards = []  # æ‰€æœ‰agentçš„æ€»å¥–åŠ±
        self.episode_lengths = []
        
        # === æ¯ä¸ªAgentçš„ç‹¬ç«‹æ•°æ® ===
        self.agent_episode_data = {}
        for agent_name in training_agent_names:
            self.agent_episode_data[agent_name] = {
                'cumulative_reward': 0.0,      # ç´¯ç§¯å¥–åŠ±
                'survival_time': 0,            # å­˜æ´»æ­¥æ•°
                'is_dead': False,              # æ˜¯å¦å·²æ­»äº¡
                'final_metrics': None,         # æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
            }
        
        # === å½“å‰Episodeçš„æ­¥æ•° ===
        self.current_ep_length = 0
        
        # === TensorBoard Writer ===
        self.writer = None
        if log_dir:
            self.writer = SummaryWriter(log_dir)
            print(f"\n  ğŸ“Š TensorBoard logging enabled")
            print(f"     Log directory: {log_dir}")
            print(f"     Run: tensorboard --logdir={log_dir}")

    def _on_step(self):
        """æ¯ä¸ªstepçš„å›è°ƒ"""
        rewards = self.locals['rewards']  # shape: (n_training,)
        dones = self.locals['dones']      # shape: (n_training,)
        infos = self.locals.get('infos', [])
        
        self.current_ep_length += 1
        
        # === ä¸ºæ¯ä¸ªè®­ç»ƒagentç´¯ç§¯æ•°æ® ===
        for i, agent_name in enumerate(self.training_agent_names):
            agent_data = self.agent_episode_data[agent_name]
            
            # ç´¯ç§¯å¥–åŠ±ï¼ˆåŒ…æ‹¬æ­»äº¡åçš„0ï¼‰
            agent_data['cumulative_reward'] += float(rewards[i])
            
            # æ£€æµ‹é¦–æ¬¡æ­»äº¡
            if dones[i] and not agent_data['is_dead']:
                agent_data['is_dead'] = True
                agent_data['survival_time'] = self.current_ep_length
                
                # ç«‹å³ä¿å­˜æ­»äº¡æ—¶çš„performance_metricsï¼ˆé˜²æ­¢è¢«æ¸…é›¶ï¼‰
                if i < len(infos) and isinstance(infos[i], dict):
                    pm = infos[i].get('performance_metrics')
                    if pm:
                        agent_data['final_metrics'] = pm.copy()
        
        # === æ£€æµ‹Episodeç»“æŸï¼ˆæ‰€æœ‰agentéƒ½doneï¼‰ ===
        if np.all(dones):
            self._on_episode_end(infos)
        
        return True
    
    def _on_episode_end(self, infos):
        """å½“Episodeç»“æŸæ—¶çš„å¤„ç†"""
        episode_num = len(self.episode_rewards) + 1
        
        # === æ”¶é›†æ‰€æœ‰agentçš„æ•°æ® ===
        all_returns = []
        all_hunting_rates = []
        all_escape_rates = []
        all_foraging_rates = []
        all_survival_times = []
        
        for i, agent_name in enumerate(self.training_agent_names):
            agent_data = self.agent_episode_data[agent_name]
            
            # 1. Episode Return
            ep_return = agent_data['cumulative_reward']
            all_returns.append(ep_return)
            
            if self.writer:
                self.writer.add_scalar(
                    f'Individual/{agent_name}/episode_return',
                    ep_return,
                    episode_num
                )
            
            # 2. Survival Time
            survival_time = agent_data['survival_time'] if agent_data['survival_time'] > 0 else self.current_ep_length
            all_survival_times.append(survival_time)
            
            if self.writer:
                self.writer.add_scalar(
                    f'Individual/{agent_name}/survival_time',
                    survival_time,
                    episode_num
                )
            
            # 3. Performance Metrics
            # ä¼˜å…ˆä½¿ç”¨æ­»äº¡æ—¶ä¿å­˜çš„ï¼Œå¦åˆ™ä»å½“å‰infoè¯»å–
            metrics = agent_data['final_metrics']
            if metrics is None and i < len(infos) and isinstance(infos[i], dict):
                metrics = infos[i].get('performance_metrics', {})
            
            if metrics:
                hunting_rate = metrics.get('hunting_rate', 0.0)
                escape_rate = metrics.get('escape_rate', 0.0)
                foraging_rate = metrics.get('foraging_rate', 0.0)
                
                all_hunting_rates.append(hunting_rate)
                all_escape_rates.append(escape_rate)
                all_foraging_rates.append(foraging_rate)
                
                if self.writer:
                    self.writer.add_scalar(
                        f'Individual/{agent_name}/hunting_rate',
                        hunting_rate,
                        episode_num
                    )
                    self.writer.add_scalar(
                        f'Individual/{agent_name}/escape_rate',
                        escape_rate,
                        episode_num
                    )
                    self.writer.add_scalar(
                        f'Individual/{agent_name}/foraging_rate',
                        foraging_rate,
                        episode_num
                    )
        
        # === è®°å½•å¹³å‡å€¼ ===
        if self.writer:
            if all_returns:
                self.writer.add_scalar('Average/episode_return', np.mean(all_returns), episode_num)
            if all_hunting_rates:
                self.writer.add_scalar('Average/hunting_rate', np.mean(all_hunting_rates), episode_num)
            if all_escape_rates:
                self.writer.add_scalar('Average/escape_rate', np.mean(all_escape_rates), episode_num)
            if all_foraging_rates:
                self.writer.add_scalar('Average/foraging_rate', np.mean(all_foraging_rates), episode_num)
            if all_survival_times:
                self.writer.add_scalar('Average/survival_time', np.mean(all_survival_times), episode_num)
        
        # === è®°å½•å…¨å±€ç»Ÿè®¡ï¼ˆå‘åå…¼å®¹ï¼‰ ===
        total_return = sum(all_returns)
        self.episode_rewards.append(total_return)
        self.episode_lengths.append(self.current_ep_length)
        
        # === æ‰“å°è¿›åº¦ï¼ˆæ¯10ä¸ªepisodeï¼‰ ===
        if episode_num % 10 == 0:
            self._print_progress(episode_num, all_returns, all_hunting_rates, all_escape_rates, all_survival_times)
        
        # === é‡ç½®æ‰€æœ‰agentçš„æ•°æ® ===
        for agent_name in self.training_agent_names:
            self.agent_episode_data[agent_name] = {
                'cumulative_reward': 0.0,
                'survival_time': 0,
                'is_dead': False,
                'final_metrics': None
            }
        
        self.current_ep_length = 0
    
    def _print_progress(self, episode_num, returns, hunting_rates, escape_rates, survival_times):
        """æ‰“å°è®­ç»ƒè¿›åº¦"""
        recent_n = min(10, episode_num)
        recent_rewards = self.episode_rewards[-recent_n:]
        
        print(f"\n[Training] Episode {episode_num}:")
        print(f"  Agents: {', '.join(self.training_agent_names)}")
        print(f"  Avg Total Reward: {np.mean(recent_rewards):.2f}")
        print(f"  Avg Episode Length: {np.mean(self.episode_lengths[-recent_n:]):.0f}")
        
        # æ‰“å°å½“å‰episodeçš„individualæ•°æ®
        print(f"\n  ğŸ“Š Current Episode Metrics:")
        for i, agent_name in enumerate(self.training_agent_names):
            print(f"    {agent_name}:")
            if i < len(returns):
                print(f"      Return: {returns[i]:.2f}")
            if i < len(hunting_rates):
                print(f"      ğŸ¯ Hunting: {hunting_rates[i]:.3f}")
            if i < len(escape_rates):
                print(f"      ğŸƒ Escape: {escape_rates[i]:.3f}")
            if i < len(survival_times):
                print(f"      â±ï¸  Survival: {survival_times[i]} steps")
        
        # æ‰“å°å¹³å‡å€¼
        if returns:
            print(f"\n  ğŸ“ˆ Average Across Agents:")
            print(f"      Return: {np.mean(returns):.2f}")
            if hunting_rates:
                print(f"      ğŸ¯ Hunting: {np.mean(hunting_rates):.3f}")
            if escape_rates:
                print(f"      ğŸƒ Escape: {np.mean(escape_rates):.3f}")
            if survival_times:
                print(f"      â±ï¸  Survival: {np.mean(survival_times):.0f} steps")
    
    def on_training_end(self):
        """è®­ç»ƒç»“æŸæ—¶çš„æ¸…ç†"""
        if self.writer:
            self.writer.close()
            print("\n  âœ“ TensorBoard writer closed")


# ============================================================================
# è®­ç»ƒæ›²çº¿ç»˜åˆ¶
# ============================================================================

def plot_training_curve(episode_rewards, training_info, save_path='training_curve.png'):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    
    Args:
        episode_rewards: Episode å¥–åŠ±åˆ—è¡¨
        training_info: è®­ç»ƒä¿¡æ¯å­—ç¬¦ä¸²
        save_path: ä¿å­˜è·¯å¾„
    """
    episodes = np.arange(1, len(episode_rewards) + 1)
    rewards = np.array(episode_rewards)

    plt.figure(figsize=(12, 6))

    plt.plot(episodes, rewards, alpha=0.3, color='blue', label='Raw Rewards')

    if len(rewards) >= 10:
        window_size = min(10, len(rewards))
        smoothed = uniform_filter1d(rewards, size=window_size, mode='nearest')
        plt.plot(episodes, smoothed, color='red', linewidth=2, 
                label=f'Moving Average (window={window_size})')

    if len(rewards) >= 50:
        window_size = min(50, len(rewards))
        trend = uniform_filter1d(rewards, size=window_size, mode='nearest')
        plt.plot(episodes, trend, color='green', linewidth=2, linestyle='--', 
                label=f'Trend (window={window_size})')

    plt.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)

    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Episode Reward', fontsize=12)
    plt.title(f'PPO Training: {training_info}', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)

    stats_text = f'Episodes: {len(rewards)}\n'
    stats_text += f'Mean: {np.mean(rewards):.2f}\n'
    stats_text += f'Std: {np.std(rewards):.2f}\n'
    stats_text += f'Max: {np.max(rewards):.2f}\n'
    stats_text += f'Min: {np.min(rewards):.2f}'

    n = len(rewards)
    if n >= 10:
        early = rewards[:max(1, n//10)]
        late = rewards[-max(1, n//10):]
        improvement = np.mean(late) - np.mean(early)
        stats_text += f'\nImprovement: {improvement:+.2f}'

    plt.text(0.02, 0.98, stats_text,
             transform=plt.gca().transAxes,
             fontsize=10,
             verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“ˆ Training curve saved to: {save_path}")
    plt.close()


# ============================================================================
# ç¯å¢ƒåˆ›å»ºå’Œå‡†å¤‡
# ============================================================================

def create_env(n_predators, n_preys, agent_configs):
    """åˆ›å»ºç¯å¢ƒ"""
    print("\n" + "="*70)
    print("Creating Waterworld Environment")
    print("="*70)

    total_agents = n_predators + n_preys
    
    # æ„å»º agent_algorithms åˆ—è¡¨
    agent_algos = []
    for config in agent_configs:
        if config.role == 'training':
            agent_algos.append("PPO")
        else:
            agent_algos.append("Fixed")

    env = waterworld_v4.parallel_env(
        render_mode=None,
        n_predators=n_predators,
        n_preys=n_preys,
        n_evaders=1,
        n_obstacles=2,
        obstacle_coord=[(0.2, 0.2), (0.8, 0.2)],
        sensor_range=0.5,  # å¢åŠ ä¼ æ„Ÿå™¨èŒƒå›´
        n_poisons=1,
        agent_algorithms=agent_algos,
        max_cycles=1000,
        static_food=True,
        static_poison=True,
    )

    print(f"\nEnvironment Details:")
    print(f"  Predators: {n_predators}")
    print(f"  Preys: {n_preys}")
    print(f"  Total Agents: {total_agents}")
    print(f"  Food: 180 (static)")
    print(f"  Poison: 10 (static)")
    print(f"  Observation Dim: {env.observation_space('prey_0').shape}")
    print(f"  Action Dim: {env.action_space('prey_0').shape}")

    return env


def prepare_env_for_training(env, agent_configs):
    """å‡†å¤‡è®­ç»ƒç¯å¢ƒ"""
    print("\n" + "="*70)
    print("Converting Environment Format")
    print("="*70)

    # æ ‡å‡†è½¬æ¢
    env = ss.black_death_v3(env)
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    env = ss.concat_vec_envs_v1(env, 1, num_cpus=1, base_class='stable_baselines3')

    print("  âœ“ Standard conversion complete")
    print(f"  âœ“ num_envs: {env.num_envs}")

    # åº”ç”¨çµæ´»çš„æ··åˆç¯å¢ƒåŒ…è£…å™¨
    env = FlexibleMixedAgentVecEnv(env, agent_configs)

    # æ·»åŠ ç›‘æ§
    env = VecMonitor(env)
    print("  âœ“ Environment preparation complete")

    return env


# ============================================================================
# è®­ç»ƒå’Œè¯„ä¼°
# ============================================================================

def train_ppo(env, agent_configs, total_timesteps=1000000, log_dir=None):
    """
    ä½¿ç”¨ PPO è®­ç»ƒ
    
    Args:
        env: è®­ç»ƒç¯å¢ƒ
        agent_configs: Agenté…ç½®åˆ—è¡¨
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        log_dir: TensorBoardæ—¥å¿—ç›®å½•ï¼ˆNoneè¡¨ç¤ºä¸è®°å½•ï¼‰
    """
    training_configs = [c for c in agent_configs if c.role == 'training']
    training_names = [c.agent_name for c in training_configs]
    
    print("\n" + "="*70)
    print("Starting PPO Training")
    print("="*70)
    print(f"Training Agents: {', '.join(training_names)}")
    print(f"Total Timesteps: {total_timesteps:,}")
    print(f"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    
    if log_dir:
        print(f"TensorBoard Log: {log_dir}")

    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        device='cpu'
    )

    # åˆ›å»ºå¸¦TensorBoardçš„callback
    callback = TrainingMonitorCallback(
        training_agent_names=training_names,
        log_dir=log_dir,
        check_freq=1000
    )

    print("\nğŸš€ Starting training...")

    model.learn(
        total_timesteps=total_timesteps,
        callback=callback,
        progress_bar=True
    )

    print("\nâœ“ Training complete!")

    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
    if callback.episode_rewards:
        rewards = np.array(callback.episode_rewards)
        print("\n" + "="*70)
        print("Training Statistics")
        print("="*70)
        print(f"Total Episodes: {len(rewards)}")
        print(f"Mean Reward: {np.mean(rewards):.2f}")
        print(f"Max Reward: {np.max(rewards):.2f}")
        print(f"Min Reward: {np.min(rewards):.2f}")

        n = len(rewards)
        if n >= 10:
            early = rewards[:max(1, n//10)]
            late = rewards[-max(1, n//10):]
            improvement = np.mean(late) - np.mean(early)

            print(f"\nLearning Analysis:")
            print(f"  Early Mean (first 10%): {np.mean(early):.2f}")
            print(f"  Late Mean (last 10%): {np.mean(late):.2f}")
            print(f"  Improvement: {improvement:+.2f}")

            if improvement > 5:
                print("  Conclusion: âœ“ Effective Learning")
            elif improvement > -5:
                print("  Conclusion: ~ Limited Learning")
            else:
                print("  Conclusion: âœ— No Effective Learning")

    return model, callback


def evaluate_model(model, env, n_episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("\n" + "="*70)
    print(f"Evaluating Model ({n_episodes} episodes)")
    print("="*70)

    episode_rewards = []

    for ep in range(n_episodes):
        obs = env.reset()
        ep_reward = 0
        ep_length = 0

        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            ep_reward += np.sum(reward)
            ep_length += 1

            if np.any(done):
                break

        episode_rewards.append(ep_reward)
        print(f"  Episode {ep+1}: Reward={ep_reward:.2f}, Length={ep_length}")

    print(f"\nEvaluation Results:")
    print(f"  Mean Reward: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"  Max Reward: {np.max(episode_rewards):.2f}")
    print(f"  Min Reward: {np.min(episode_rewards):.2f}")

    return episode_rewards


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("Waterworld: Flexible Training System + TensorBoard")
    print("="*70)

    # ========================================
    # é…ç½®åŒºåŸŸï¼šçµæ´»é…ç½®è®­ç»ƒåœºæ™¯
    # ========================================
    
    # ç¯å¢ƒé…ç½®
    N_PREDATORS = 3
    N_PREYS = 30
    TOTAL_TIMESTEPS = 100000
    
    # ========================================
    # åœºæ™¯ 2: è®­ç»ƒéƒ¨åˆ† Predators
    # ========================================
    agent_configs = create_agent_configs(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        train_predators=[1, 2],  # è®­ç»ƒ predator 1, 2
        train_preys=None,
        predator_policies=RandomPolicy(),
        prey_policies=RandomPolicy()
    )
    
    # ========================================
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print_agent_configs(agent_configs)
    
    # ç”Ÿæˆè®­ç»ƒä¿¡æ¯å­—ç¬¦ä¸²ï¼ˆç”¨äºæ–‡ä»¶åå’Œå›¾è¡¨ï¼‰
    training_configs = [c for c in agent_configs if c.role == 'training']
    training_predators = [c for c in training_configs if c.agent_type == 'predator']
    training_preys = [c for c in training_configs if c.agent_type == 'prey']
    
    training_info_parts = []
    if training_predators:
        pred_names = [c.agent_name for c in training_predators]
        training_info_parts.append(f"Predators[{','.join([n.split('_')[1] for n in pred_names])}]")
    if training_preys:
        prey_names = [c.agent_name for c in training_preys]
        training_info_parts.append(f"Preys[{','.join([n.split('_')[1] for n in prey_names])}]")
    
    training_info = "_".join(training_info_parts)
    
    # âœ… ç”ŸæˆTensorBoardæ—¥å¿—ç›®å½•ï¼ˆç»“æ„åŒ–ï¼‰
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join("logs", "waterworld", training_info, f"run_{timestamp}")
    
    model_filename = f'model_{training_info}'
    curve_filename = f'training_curve_{training_info}.png'
    
    print(f"\nğŸ“ Files will be saved as:")
    print(f"  - Model: {model_filename}.zip")
    print(f"  - Curve: {curve_filename}")
    print(f"  - TensorBoard: {log_dir}")
    
    # 1. åˆ›å»ºç¯å¢ƒ
    raw_env = create_env(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        agent_configs=agent_configs
    )

    # 2. å‡†å¤‡è®­ç»ƒç¯å¢ƒ
    env = prepare_env_for_training(raw_env, agent_configs)

    # 3. è®­ç»ƒï¼ˆå¸¦TensorBoardï¼‰
    model, callback = train_ppo(
        env, 
        agent_configs, 
        total_timesteps=TOTAL_TIMESTEPS,
        log_dir=log_dir  # âœ… ä¼ å…¥æ—¥å¿—ç›®å½•
    )

    # 4. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if callback.episode_rewards:
        print("\n" + "="*70)
        print("Generating Training Curve")
        print("="*70)
        plot_training_curve(
            callback.episode_rewards,
            training_info=training_info.replace('_', ' + '),
            save_path=curve_filename
        )

    # 5. è¯„ä¼°
    episode_rewards = evaluate_model(model, env, n_episodes=10)

    # 6. ä¿å­˜æ¨¡å‹
    model.save(model_filename)
    print(f"\nğŸ’¾ Model saved: {model_filename}.zip")

    # 7. æœ€ç»ˆæ€»ç»“
    print("\n" + "="*70)
    print("Training Complete Summary")
    print("="*70)

    if callback.episode_rewards:
        rewards = np.array(callback.episode_rewards)
        eval_rewards = np.array(episode_rewards)

        print(f"\nğŸ¯ Training Configuration:")
        if training_predators:
            print(f"  Training Predators: {[c.agent_name for c in training_predators]}")
        if training_preys:
            print(f"  Training Preys: {[c.agent_name for c in training_preys]}")

        print(f"\nğŸ“Š Training Performance:")
        print(f"  Mean Reward: {np.mean(rewards):.2f}")
        print(f"  Max Reward: {np.max(rewards):.2f}")
        print(f"  Std Reward: {np.std(rewards):.2f}")

        print(f"\nğŸ§ª Evaluation Performance:")
        print(f"  Mean Reward: {np.mean(eval_rewards):.2f}")
        print(f"  Max Reward: {np.max(eval_rewards):.2f}")
        print(f"  Std Reward: {np.std(eval_rewards):.2f}")

        n = len(rewards)
        if n >= 10:
            early = rewards[:max(1, n//10)]
            late = rewards[-max(1, n//10):]
            improvement = np.mean(late) - np.mean(early)
            
            print(f"\nğŸ“ˆ Learning Progress:")
            print(f"  Early Mean (first 10%): {np.mean(early):.2f}")
            print(f"  Late Mean (last 10%): {np.mean(late):.2f}")
            print(f"  Improvement: {improvement:+.2f}")
            
            if improvement > 5:
                print(f"  Status: âœ“ Training agents learned effectively")
            elif improvement > -5:
                print(f"  Status: ~ Limited learning observed")
            else:
                print(f"  Status: âœ— No significant learning")

    env.close()
    
    print("\n" + "="*70)
    print("ğŸ‰ All Done!")
    print("="*70)
    print(f"\nğŸ“ Generated Files:")
    print(f"  âœ“ {model_filename}.zip")
    print(f"  âœ“ {curve_filename}")
    print(f"  âœ“ TensorBoard logs: {log_dir}")
    print("\nğŸ’¡ View TensorBoard:")
    print(f"  tensorboard --logdir={os.path.join('logs', 'waterworld')}")
    print("\nğŸ’¡ Tips:")
    print("  - Modify agent_configs in main() to try different training scenarios")
    print("  - Use saved models with TrainedModelPolicy() for hierarchical training")
    print("  - Create custom policies by extending AgentPolicy class")


if __name__ == "__main__":
    main()