{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb52067d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 环境信息测试 ===\n",
      "\n",
      "1. 基本环境信息:\n",
      "   环境名称: waterworld_v4\n",
      "   智能体数量: 20\n",
      "   智能体列表: ['predator_0', 'predator_1', 'predator_2', 'predator_3', 'predator_4']...\n",
      "\n",
      "2. 动作空间信息:\n",
      "   动作空间类型: <class 'gymnasium.spaces.box.Box'>\n",
      "   动作空间: Box(-1.0, 1.0, (2,), float32)\n",
      "   连续动作维度: (2,)\n",
      "   动作范围: low=[-1. -1.], high=[1. 1.]\n",
      "   动作类型: float32\n",
      "\n",
      "3. 观察空间信息:\n",
      "   观察空间类型: <class 'gymnasium.spaces.box.Box'>\n",
      "   观察空间: Box(-inf, inf, (302,), float32)\n",
      "   观察维度: (302,)\n",
      "   观察范围: low=[-inf -inf -inf -inf -inf]..., high=[inf inf inf inf inf]...\n",
      "   观察类型: float32\n",
      "\n",
      "4. 实际动作和观察采样:\n",
      "   智能体: predator_0\n",
      "   观察形状: (302,)\n",
      "   观察类型: <class 'numpy.ndarray'>\n",
      "   观察范围: [-26.209, 10.000]\n",
      "   动作: [0.20213628 0.22434653]\n",
      "   动作类型: <class 'numpy.ndarray'>\n",
      "   动作形状: (2,)\n",
      "   奖励: 0\n",
      "\n",
      "   智能体: predator_1\n",
      "   观察形状: (302,)\n",
      "   观察类型: <class 'numpy.ndarray'>\n",
      "   观察范围: [-34.306, 9.000]\n",
      "   动作: [-0.28497612  0.37577546]\n",
      "   动作类型: <class 'numpy.ndarray'>\n",
      "   动作形状: (2,)\n",
      "   奖励: 0\n",
      "\n",
      "   智能体: predator_2\n",
      "   观察形状: (302,)\n",
      "   观察类型: <class 'numpy.ndarray'>\n",
      "   观察范围: [-22.876, 1.000]\n",
      "   动作: [ 0.3333335 -0.9188344]\n",
      "   动作类型: <class 'numpy.ndarray'>\n",
      "   动作形状: (2,)\n",
      "   奖励: 0\n",
      "\n",
      "5. 转换为连续动作空间的考虑:\n",
      "   当前离散动作 Discrete(9) 可能对应:\n",
      "   0: 无动作 [0, 0]\n",
      "   1: 上      [0, 1]\n",
      "   2: 下      [0, -1]\n",
      "   3: 左      [-1, 0]\n",
      "   4: 右      [1, 0]\n",
      "   5: 左上    [-0.707, 0.707]\n",
      "   6: 右上    [0.707, 0.707]\n",
      "   7: 左下    [-0.707, -0.707]\n",
      "   8: 右下    [0.707, -0.707]\n",
      "\n",
      "   连续动作空间 Box(2) 的范围建议: [-1, 1] x [-1, 1]\n",
      "   表示 [x_velocity, y_velocity] 或 [delta_x, delta_y]\n",
      "\n",
      "=== 连续动作空间转换分析 ===\n",
      "\n",
      "1. 网络架构需要修改的地方:\n",
      "   - Actor网络输出层: 从 Linear(hidden, 9) 改为 Linear(hidden, 2)\n",
      "   - 激活函数: 需要在输出层添加 tanh 或其他限制函数\n",
      "   - 动作范围: 通常使用 [-1, 1] 然后映射到实际范围\n",
      "\n",
      "2. 策略算法需要修改的地方:\n",
      "   - 分布函数: 从 Categorical(logits) 改为 Normal(mu, sigma)\n",
      "   - PPO: 需要处理连续动作的概率比率计算\n",
      "   - A2C/NPG/TRPO: 同样需要连续动作分布\n",
      "   - 探索策略: 从epsilon-greedy改为高斯噪声\n",
      "\n",
      "3. 主要代码修改点:\n",
      "   - dist_fn: lambda x: Categorical(logits=x) → lambda x: Normal(x[..., :2], x[..., 2:].exp())\n",
      "   - 网络输出: 需要输出均值和方差 (或log_std)\n",
      "   - 动作采样: 从整数采样改为连续值采样\n",
      "   - 动作clipping: 确保动作在有效范围内\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.sisl import waterworld_v4\n",
    "import supersuit as ss\n",
    "from collections import defaultdict\n",
    "import gymnasium\n",
    "import numpy as np\n",
    "\n",
    "def test_environment_info():\n",
    "    \"\"\"测试环境信息，包括动作空间、观察空间等关键信息\"\"\"\n",
    "    \n",
    "    print(\"=== 环境信息测试 ===\\n\")\n",
    "    \n",
    "    # 准备环境参数\n",
    "    agent_algos = [\"PPO\", \"PPO\", \"DQN\", \"DQN\", \"A2C\"] * 4\n",
    "    env = waterworld_v4.env(\n",
    "        render_mode=None,  # 测试时不渲染\n",
    "        n_predators=5,\n",
    "        n_preys=15,\n",
    "        n_evaders=1,\n",
    "        n_obstacles=2,\n",
    "        obstacle_coord=[(0.2, 0.2), (0.8, 0.2)],\n",
    "        n_poisons=20,\n",
    "        agent_algorithms=agent_algos\n",
    "    )\n",
    "    \n",
    "    # 黑死亡包装\n",
    "    env = ss.black_death_v3(env)\n",
    "    \n",
    "    # 重置环境\n",
    "    obs = env.reset(seed=42)\n",
    "    \n",
    "    print(\"1. 基本环境信息:\")\n",
    "    print(f\"   环境名称: {env.metadata.get('name', 'waterworld_v4')}\")\n",
    "    print(f\"   智能体数量: {len(env.agents)}\")\n",
    "    print(f\"   智能体列表: {env.agents[:5]}...\")  # 只显示前5个\n",
    "    print()\n",
    "    \n",
    "    # 检查第一个智能体的空间信息\n",
    "    first_agent = env.agents[0]\n",
    "    \n",
    "    print(\"2. 动作空间信息:\")\n",
    "    action_space = env.action_space(first_agent)\n",
    "    print(f\"   动作空间类型: {type(action_space)}\")\n",
    "    print(f\"   动作空间: {action_space}\")\n",
    "    \n",
    "    if isinstance(action_space, gymnasium.spaces.Discrete):\n",
    "        print(f\"   离散动作数量: {action_space.n}\")\n",
    "        print(f\"   动作范围: 0 到 {action_space.n-1}\")\n",
    "        print(\"   动作含义可能为: [无动作, 上, 下, 左, 右, 左上, 右上, 左下, 右下]\")\n",
    "    elif isinstance(action_space, gymnasium.spaces.Box):\n",
    "        print(f\"   连续动作维度: {action_space.shape}\")\n",
    "        print(f\"   动作范围: low={action_space.low}, high={action_space.high}\")\n",
    "        print(f\"   动作类型: {action_space.dtype}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. 观察空间信息:\")\n",
    "    observation_space = env.observation_space(first_agent)\n",
    "    print(f\"   观察空间类型: {type(observation_space)}\")\n",
    "    print(f\"   观察空间: {observation_space}\")\n",
    "    \n",
    "    if isinstance(observation_space, gymnasium.spaces.Box):\n",
    "        print(f\"   观察维度: {observation_space.shape}\")\n",
    "        print(f\"   观察范围: low={observation_space.low[:5]}..., high={observation_space.high[:5]}...\")\n",
    "        print(f\"   观察类型: {observation_space.dtype}\")\n",
    "    elif isinstance(observation_space, gymnasium.spaces.Dict):\n",
    "        print(\"   字典观察空间包含:\")\n",
    "        for key, space in observation_space.spaces.items():\n",
    "            print(f\"     {key}: {space}\")\n",
    "    print()\n",
    "    \n",
    "    # 执行几步并检查实际的动作和观察\n",
    "    print(\"4. 实际动作和观察采样:\")\n",
    "    step_count = 0\n",
    "    for agent in env.agent_iter():\n",
    "        if step_count >= 3:  # 只测试前3步\n",
    "            break\n",
    "            \n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "        \n",
    "        print(f\"   智能体: {agent}\")\n",
    "        if observation is not None:\n",
    "            print(f\"   观察形状: {np.array(observation).shape}\")\n",
    "            print(f\"   观察类型: {type(observation)}\")\n",
    "            if isinstance(observation, np.ndarray):\n",
    "                print(f\"   观察范围: [{observation.min():.3f}, {observation.max():.3f}]\")\n",
    "        \n",
    "        # 选择动作\n",
    "        if termination or truncation:\n",
    "            action = None\n",
    "            print(f\"   动作: {action} (终止)\")\n",
    "        else:\n",
    "            action = env.action_space(agent).sample()\n",
    "            print(f\"   动作: {action}\")\n",
    "            print(f\"   动作类型: {type(action)}\")\n",
    "            if isinstance(action, np.ndarray):\n",
    "                print(f\"   动作形状: {action.shape}\")\n",
    "        \n",
    "        print(f\"   奖励: {reward}\")\n",
    "        print()\n",
    "        \n",
    "        env.step(action)\n",
    "        step_count += 1\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    print(\"5. 转换为连续动作空间的考虑:\")\n",
    "    print(\"   当前离散动作 Discrete(9) 可能对应:\")\n",
    "    print(\"   0: 无动作 [0, 0]\")\n",
    "    print(\"   1: 上      [0, 1]\") \n",
    "    print(\"   2: 下      [0, -1]\")\n",
    "    print(\"   3: 左      [-1, 0]\")\n",
    "    print(\"   4: 右      [1, 0]\")\n",
    "    print(\"   5: 左上    [-0.707, 0.707]\")\n",
    "    print(\"   6: 右上    [0.707, 0.707]\")\n",
    "    print(\"   7: 左下    [-0.707, -0.707]\")\n",
    "    print(\"   8: 右下    [0.707, -0.707]\")\n",
    "    print()\n",
    "    print(\"   连续动作空间 Box(2) 的范围建议: [-1, 1] x [-1, 1]\")\n",
    "    print(\"   表示 [x_velocity, y_velocity] 或 [delta_x, delta_y]\")\n",
    "\n",
    "def analyze_continuous_action_requirements():\n",
    "    \"\"\"分析连续动作空间的要求\"\"\"\n",
    "    print(\"\\n=== 连续动作空间转换分析 ===\\n\")\n",
    "    \n",
    "    print(\"1. 网络架构需要修改的地方:\")\n",
    "    print(\"   - Actor网络输出层: 从 Linear(hidden, 9) 改为 Linear(hidden, 2)\")\n",
    "    print(\"   - 激活函数: 需要在输出层添加 tanh 或其他限制函数\")\n",
    "    print(\"   - 动作范围: 通常使用 [-1, 1] 然后映射到实际范围\")\n",
    "    print()\n",
    "    \n",
    "    print(\"2. 策略算法需要修改的地方:\")\n",
    "    print(\"   - 分布函数: 从 Categorical(logits) 改为 Normal(mu, sigma)\")\n",
    "    print(\"   - PPO: 需要处理连续动作的概率比率计算\")\n",
    "    print(\"   - A2C/NPG/TRPO: 同样需要连续动作分布\")\n",
    "    print(\"   - 探索策略: 从epsilon-greedy改为高斯噪声\")\n",
    "    print()\n",
    "    \n",
    "    print(\"3. 主要代码修改点:\")\n",
    "    print(\"   - dist_fn: lambda x: Categorical(logits=x) → lambda x: Normal(x[..., :2], x[..., 2:].exp())\")\n",
    "    print(\"   - 网络输出: 需要输出均值和方差 (或log_std)\")\n",
    "    print(\"   - 动作采样: 从整数采样改为连续值采样\")\n",
    "    print(\"   - 动作clipping: 确保动作在有效范围内\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_environment_info()\n",
    "    analyze_continuous_action_requirements()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
