{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "133885fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from typing import (\n",
    "    TYPE_CHECKING,\n",
    "    Any,\n",
    "    Dict,\n",
    "    List,\n",
    "    Optional,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box, Discrete, MultiDiscrete, MultiBinary\n",
    "from gymnasium.spaces import Dict as GymDict\n",
    "from gymnasium.spaces import Tuple as GymTuple\n",
    "import numpy as np\n",
    "import tree  # pip install dm_tree\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.air.integrations.wandb import WandbLoggerCallback, WANDB_ENV_VAR\n",
    "from ray.rllib.core import DEFAULT_MODULE_ID, Columns\n",
    "from ray.rllib.env.wrappers.atari_wrappers import is_atari, wrap_deepmind\n",
    "from ray.rllib.utils.annotations import OldAPIStack\n",
    "from ray.rllib.utils.framework import try_import_jax, try_import_tf, try_import_torch\n",
    "from ray.rllib.utils.metrics import (\n",
    "    DIFF_NUM_GRAD_UPDATES_VS_SAMPLER_POLICY,\n",
    "    ENV_RUNNER_RESULTS,\n",
    "    EPISODE_RETURN_MEAN,\n",
    "    EVALUATION_RESULTS,\n",
    "    NUM_ENV_STEPS_TRAINED,\n",
    "    NUM_ENV_STEPS_SAMPLED_LIFETIME,\n",
    ")\n",
    "from ray.rllib.utils.typing import ResultDict\n",
    "from ray.rllib.utils.error import UnsupportedSpaceException\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.result import TRAINING_ITERATION\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from ray.rllib.algorithms import Algorithm, AlgorithmConfig\n",
    "    from ray.rllib.offline.dataset_reader import DatasetReader\n",
    "\n",
    "jax, _ = try_import_jax()\n",
    "tf1, tf, tfv = try_import_tf()\n",
    "torch, _ = try_import_torch()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c3eff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def add_rllib_example_script_args(\n",
    "    parser: Optional[argparse.ArgumentParser] = None,\n",
    "    default_reward: float = 100.0,\n",
    "    default_iters: int = 200,\n",
    "    default_timesteps: int = 100000,\n",
    ") -> argparse.ArgumentParser:\n",
    "    \"\"\"Adds RLlib-typical (and common) examples scripts command line args to a parser.\n",
    "\n",
    "    TODO (sven): This function should be used by most of our examples scripts, which\n",
    "     already mostly have this logic in them (but written out).\n",
    "\n",
    "    Args:\n",
    "        parser: The parser to add the arguments to. If None, create a new one.\n",
    "        default_reward: The default value for the --stop-reward option.\n",
    "        default_iters: The default value for the --stop-iters option.\n",
    "        default_timesteps: The default value for the --stop-timesteps option.\n",
    "\n",
    "    Returns:\n",
    "        The altered (or newly created) parser object.\n",
    "    \"\"\"\n",
    "    if parser is None:\n",
    "        parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Algo and Algo config options.\n",
    "    parser.add_argument(\n",
    "        \"--algo\", type=str, default=\"SAC\", help=\"The RLlib-registered algorithm to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--enable-new-api-stack\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to use the `enable_rl_module_and_learner` config setting.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--framework\",\n",
    "        choices=[\"tf\", \"tf2\", \"torch\"],\n",
    "        default=\"torch\",\n",
    "        help=\"The DL framework specifier.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--env\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The gym.Env identifier to run the experiment with.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-env-runners\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"The number of (remote) EnvRunners to use for the experiment.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-envs-per-env-runner\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"The number of (vectorized) environments per EnvRunner. Note that \"\n",
    "        \"this is identical to the batch size for (inference) action computations.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-agents\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"If 0 (default), will run as single-agent. If > 0, will run as \"\n",
    "        \"multi-agent with the environment simply cloned n times and each agent acting \"\n",
    "        \"independently at every single timestep. The overall reward for this \"\n",
    "        \"experiment is then the sum over all individual agents' rewards.\",\n",
    "    )\n",
    "\n",
    "    # Evaluation options.\n",
    "    parser.add_argument(\n",
    "        \"--evaluation-num-env-runners\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"The number of evaluation (remote) EnvRunners to use for the experiment.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--evaluation-interval\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Every how many iterations to run one round of evaluation. \"\n",
    "        \"Use 0 (default) to disable evaluation.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--evaluation-duration\",\n",
    "        type=lambda v: v if v == \"auto\" else int(v),\n",
    "        default=10,\n",
    "        help=\"The number of evaluation units to run each evaluation round. \"\n",
    "        \"Use `--evaluation-duration-unit` to count either in 'episodes' \"\n",
    "        \"or 'timesteps'. If 'auto', will run as many as possible during train pass (\"\n",
    "        \"`--evaluation-parallel-to-training` must be set then).\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--evaluation-duration-unit\",\n",
    "        type=str,\n",
    "        default=\"episodes\",\n",
    "        choices=[\"episodes\", \"timesteps\"],\n",
    "        help=\"The evaluation duration unit to count by. One of 'episodes' or \"\n",
    "        \"'timesteps'. This unit will be run `--evaluation-duration` times in each \"\n",
    "        \"evaluation round. If `--evaluation-duration=auto`, this setting does not \"\n",
    "        \"matter.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--evaluation-parallel-to-training\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to run evaluation parallel to training. This might help speed up \"\n",
    "        \"your overall iteration time. Be aware that when using this option, your \"\n",
    "        \"reported evaluation results are referring to one iteration before the current \"\n",
    "        \"one.\",\n",
    "    )\n",
    "\n",
    "    # RLlib logging options.\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The output directory to write trajectories to, which are collected by \"\n",
    "        \"the algo's EnvRunners.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--log-level\",\n",
    "        type=str,\n",
    "        default=None,  # None -> use default\n",
    "        choices=[\"INFO\", \"DEBUG\", \"WARN\", \"ERROR\"],\n",
    "        help=\"The log-level to be used by the RLlib logger.\",\n",
    "    )\n",
    "\n",
    "    # tune.Tuner options.\n",
    "    parser.add_argument(\n",
    "        \"--no-tune\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to NOT use tune.Tuner(), but rather a simple for-loop calling \"\n",
    "        \"`algo.train()` repeatedly until one of the stop criteria is met.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-samples\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"How many (tune.Tuner.fit()) experiments to execute - if possible in \"\n",
    "        \"parallel.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max-concurrent-trials\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"How many (tune.Tuner) trials to run concurrently.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--verbose\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"The verbosity level for the `tune.Tuner()` running the experiment.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint-freq\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"The frequency (in training iterations) with which to create checkpoints. \"\n",
    "            \"Note that if --wandb-key is provided, all checkpoints will \"\n",
    "            \"automatically be uploaded to WandB.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoint-at-end\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether to create a checkpoint at the very end of the experiment. \"\n",
    "            \"Note that if --wandb-key is provided, all checkpoints will \"\n",
    "            \"automatically be uploaded to WandB.\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # WandB logging options.\n",
    "    parser.add_argument(\n",
    "        \"--wandb-key\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The WandB API key to use for uploading results.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb-project\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The WandB project name to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--wandb-run-name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The WandB run name to use.\",\n",
    "    )\n",
    "\n",
    "    # Experiment stopping and testing criteria.\n",
    "    parser.add_argument(\n",
    "        \"--stop-reward\",\n",
    "        type=float,\n",
    "        default=default_reward,\n",
    "        help=\"Reward at which the script should stop training.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--stop-iters\",\n",
    "        type=int,\n",
    "        default=default_iters,\n",
    "        help=\"The number of iterations to train.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--stop-timesteps\",\n",
    "        type=int,\n",
    "        default=default_timesteps,\n",
    "        help=\"The number of (environment sampling) timesteps to train.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--as-test\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether this script should be run as a test. If set, --stop-reward must \"\n",
    "        \"be achieved within --stop-timesteps AND --stop-iters, otherwise this \"\n",
    "        \"script will throw an exception at the end.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--as-release-test\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether this script should be run as a release test. If set, \"\n",
    "        \"all that applies to the --as-test option is true, plus, a short JSON summary \"\n",
    "        \"will be written into a results file whose location is given by the ENV \"\n",
    "        \"variable `TEST_OUTPUT_JSON`.\",\n",
    "    )\n",
    "\n",
    "    # Learner scaling options.\n",
    "    parser.add_argument(\n",
    "        \"--num-learners\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"The number of Learners to use. If `None`, use the algorithm's default \"\n",
    "        \"value.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-cpus-per-learner\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "        help=\"The number of CPUs per Learner to use. If `None`, use the algorithm's \"\n",
    "        \"default value.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-gpus-per-learner\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "        help=\"The number of GPUs per Learner to use. If `None` and there are enough \"\n",
    "        \"GPUs for all required Learners (--num-learners), use a value of 1, \"\n",
    "        \"otherwise 0.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num-aggregator-actors-per-learner\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"The number of Aggregator actors to use per Learner. If `None`, use the \"\n",
    "        \"algorithm's default value.\",\n",
    "    )\n",
    "\n",
    "    # Ray init options.\n",
    "    parser.add_argument(\"--num-cpus\", type=int, default=0)\n",
    "    parser.add_argument(\n",
    "        \"--local-mode\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Init Ray in local mode for easier debugging.\",\n",
    "    )\n",
    "\n",
    "    # Old API stack: config.num_gpus.\n",
    "    parser.add_argument(\n",
    "        \"--num-gpus\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"The number of GPUs to use (only on the old API stack).\",\n",
    "    )\n",
    "\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7af215af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def check(x, y, decimals=5, atol=None, rtol=None, false=False):\n",
    "    \"\"\"\n",
    "    Checks two structures (dict, tuple, list,\n",
    "    np.array, float, int, etc..) for (almost) numeric identity.\n",
    "    All numbers in the two structures have to match up to `decimal` digits\n",
    "    after the floating point. Uses assertions.\n",
    "\n",
    "    Args:\n",
    "        x: The value to be compared (to the expectation: `y`). This\n",
    "            may be a Tensor.\n",
    "        y: The expected value to be compared to `x`. This must not\n",
    "            be a tf-Tensor, but may be a tf/torch-Tensor.\n",
    "        decimals: The number of digits after the floating point up to\n",
    "            which all numeric values have to match.\n",
    "        atol: Absolute tolerance of the difference between x and y\n",
    "            (overrides `decimals` if given).\n",
    "        rtol: Relative tolerance of the difference between x and y\n",
    "            (overrides `decimals` if given).\n",
    "        false: Whether to check that x and y are NOT the same.\n",
    "    \"\"\"\n",
    "    # A dict type.\n",
    "    if isinstance(x, dict):\n",
    "        assert isinstance(y, dict), \"ERROR: If x is dict, y needs to be a dict as well!\"\n",
    "        y_keys = set(x.keys())\n",
    "        for key, value in x.items():\n",
    "            assert key in y, f\"ERROR: y does not have x's key='{key}'! y={y}\"\n",
    "            check(value, y[key], decimals=decimals, atol=atol, rtol=rtol, false=false)\n",
    "            y_keys.remove(key)\n",
    "        assert not y_keys, \"ERROR: y contains keys ({}) that are not in x! y={}\".format(\n",
    "            list(y_keys), y\n",
    "        )\n",
    "    # A tuple type.\n",
    "    elif isinstance(x, (tuple, list)):\n",
    "        assert isinstance(\n",
    "            y, (tuple, list)\n",
    "        ), \"ERROR: If x is tuple/list, y needs to be a tuple/list as well!\"\n",
    "        assert len(y) == len(\n",
    "            x\n",
    "        ), \"ERROR: y does not have the same length as x ({} vs {})!\".format(\n",
    "            len(y), len(x)\n",
    "        )\n",
    "        for i, value in enumerate(x):\n",
    "            check(value, y[i], decimals=decimals, atol=atol, rtol=rtol, false=false)\n",
    "    # Boolean comparison.\n",
    "    elif isinstance(x, (np.bool_, bool)):\n",
    "        if false is True:\n",
    "            assert bool(x) is not bool(y), f\"ERROR: x ({x}) is y ({y})!\"\n",
    "        else:\n",
    "            assert bool(x) is bool(y), f\"ERROR: x ({x}) is not y ({y})!\"\n",
    "    # Nones or primitives (excluding int vs float, which should be compared with\n",
    "    # tolerance/decimals as well).\n",
    "    elif (\n",
    "        x is None\n",
    "        or y is None\n",
    "        or isinstance(x, str)\n",
    "        or (isinstance(x, int) and isinstance(y, int))\n",
    "    ):\n",
    "        if false is True:\n",
    "            assert x != y, f\"ERROR: x ({x}) is the same as y ({y})!\"\n",
    "        else:\n",
    "            assert x == y, f\"ERROR: x ({x}) is not the same as y ({y})!\"\n",
    "    # String/byte comparisons.\n",
    "    elif (\n",
    "        hasattr(x, \"dtype\") and (x.dtype == object or str(x.dtype).startswith(\"<U\"))\n",
    "    ) or isinstance(x, bytes):\n",
    "        try:\n",
    "            np.testing.assert_array_equal(x, y)\n",
    "            if false is True:\n",
    "                assert False, f\"ERROR: x ({x}) is the same as y ({y})!\"\n",
    "        except AssertionError as e:\n",
    "            if false is False:\n",
    "                raise e\n",
    "    # Everything else (assume numeric or tf/torch.Tensor).\n",
    "    # Also includes int vs float comparison, which is performed with tolerance/decimals.\n",
    "    else:\n",
    "        if tf1 is not None:\n",
    "            # y should never be a Tensor (y=expected value).\n",
    "            if isinstance(y, (tf1.Tensor, tf1.Variable)):\n",
    "                # In eager mode, numpyize tensors.\n",
    "                if tf.executing_eagerly():\n",
    "                    y = y.numpy()\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        \"`y` (expected value) must not be a Tensor. \"\n",
    "                        \"Use numpy.ndarray instead\"\n",
    "                    )\n",
    "            if isinstance(x, (tf1.Tensor, tf1.Variable)):\n",
    "                # In eager mode, numpyize tensors.\n",
    "                if tf1.executing_eagerly():\n",
    "                    x = x.numpy()\n",
    "                # Otherwise, use a new tf-session.\n",
    "                else:\n",
    "                    with tf1.Session() as sess:\n",
    "                        x = sess.run(x)\n",
    "                        return check(\n",
    "                            x, y, decimals=decimals, atol=atol, rtol=rtol, false=false\n",
    "                        )\n",
    "        if torch is not None:\n",
    "            if isinstance(x, torch.Tensor):\n",
    "                x = x.detach().cpu().numpy()\n",
    "            if isinstance(y, torch.Tensor):\n",
    "                y = y.detach().cpu().numpy()\n",
    "\n",
    "        # Stats objects.\n",
    "        from ray.rllib.utils.metrics.stats import Stats\n",
    "\n",
    "        if isinstance(x, Stats):\n",
    "            x = x.peek()\n",
    "        if isinstance(y, Stats):\n",
    "            y = y.peek()\n",
    "\n",
    "        # Using decimals.\n",
    "        if atol is None and rtol is None:\n",
    "            # Assert equality of both values.\n",
    "            try:\n",
    "                np.testing.assert_almost_equal(x, y, decimal=decimals)\n",
    "            # Both values are not equal.\n",
    "            except AssertionError as e:\n",
    "                # Raise error in normal case.\n",
    "                if false is False:\n",
    "                    raise e\n",
    "            # Both values are equal.\n",
    "            else:\n",
    "                # If false is set -> raise error (not expected to be equal).\n",
    "                if false is True:\n",
    "                    assert False, f\"ERROR: x ({x}) is the same as y ({y})!\"\n",
    "\n",
    "        # Using atol/rtol.\n",
    "        else:\n",
    "            # Provide defaults for either one of atol/rtol.\n",
    "            if atol is None:\n",
    "                atol = 0\n",
    "            if rtol is None:\n",
    "                rtol = 1e-7\n",
    "            try:\n",
    "                np.testing.assert_allclose(x, y, atol=atol, rtol=rtol)\n",
    "            except AssertionError as e:\n",
    "                if false is False:\n",
    "                    raise e\n",
    "            else:\n",
    "                if false is True:\n",
    "                    assert False, f\"ERROR: x ({x}) is the same as y ({y})!\"\n",
    "\n",
    "\n",
    "def check_compute_single_action(\n",
    "    algorithm, include_state=False, include_prev_action_reward=False\n",
    "):\n",
    "    \"\"\"Tests different combinations of args for algorithm.compute_single_action.\n",
    "\n",
    "    Args:\n",
    "        algorithm: The Algorithm object to test.\n",
    "        include_state: Whether to include the initial state of the Policy's\n",
    "            Model in the `compute_single_action` call.\n",
    "        include_prev_action_reward: Whether to include the prev-action and\n",
    "            -reward in the `compute_single_action` call.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If anything unexpected happens.\n",
    "    \"\"\"\n",
    "    # Have to import this here to avoid circular dependency.\n",
    "    from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID, SampleBatch\n",
    "\n",
    "    # Some Algorithms may not abide to the standard API.\n",
    "    pid = DEFAULT_POLICY_ID\n",
    "    try:\n",
    "        # Multi-agent: Pick any learnable policy (or DEFAULT_POLICY if it's the only\n",
    "        # one).\n",
    "        pid = next(iter(algorithm.env_runner.get_policies_to_train()))\n",
    "        pol = algorithm.get_policy(pid)\n",
    "    except AttributeError:\n",
    "        pol = algorithm.policy\n",
    "    # Get the policy's model.\n",
    "    model = pol.model\n",
    "\n",
    "    action_space = pol.action_space\n",
    "\n",
    "    def _test(\n",
    "        what, method_to_test, obs_space, full_fetch, explore, timestep, unsquash, clip\n",
    "    ):\n",
    "        call_kwargs = {}\n",
    "        if what is algorithm:\n",
    "            call_kwargs[\"full_fetch\"] = full_fetch\n",
    "            call_kwargs[\"policy_id\"] = pid\n",
    "\n",
    "        obs = obs_space.sample()\n",
    "        if isinstance(obs_space, Box):\n",
    "            obs = np.clip(obs, -1.0, 1.0)\n",
    "        state_in = None\n",
    "        if include_state:\n",
    "            state_in = model.get_initial_state()\n",
    "            if not state_in:\n",
    "                state_in = []\n",
    "                i = 0\n",
    "                while f\"state_in_{i}\" in model.view_requirements:\n",
    "                    state_in.append(\n",
    "                        model.view_requirements[f\"state_in_{i}\"].space.sample()\n",
    "                    )\n",
    "                    i += 1\n",
    "        action_in = action_space.sample() if include_prev_action_reward else None\n",
    "        reward_in = 1.0 if include_prev_action_reward else None\n",
    "\n",
    "        if method_to_test == \"input_dict\":\n",
    "            assert what is pol\n",
    "\n",
    "            input_dict = {SampleBatch.OBS: obs}\n",
    "            if include_prev_action_reward:\n",
    "                input_dict[SampleBatch.PREV_ACTIONS] = action_in\n",
    "                input_dict[SampleBatch.PREV_REWARDS] = reward_in\n",
    "            if state_in:\n",
    "                if what.config.get(\"enable_rl_module_and_learner\", False):\n",
    "                    input_dict[\"state_in\"] = state_in\n",
    "                else:\n",
    "                    for i, s in enumerate(state_in):\n",
    "                        input_dict[f\"state_in_{i}\"] = s\n",
    "            input_dict_batched = SampleBatch(\n",
    "                tree.map_structure(lambda s: np.expand_dims(s, 0), input_dict)\n",
    "            )\n",
    "            action = pol.compute_actions_from_input_dict(\n",
    "                input_dict=input_dict_batched,\n",
    "                explore=explore,\n",
    "                timestep=timestep,\n",
    "                **call_kwargs,\n",
    "            )\n",
    "            # Unbatch everything to be able to compare against single\n",
    "            # action below.\n",
    "            # ARS and ES return action batches as lists.\n",
    "            if isinstance(action[0], list):\n",
    "                action = (np.array(action[0]), action[1], action[2])\n",
    "            action = tree.map_structure(lambda s: s[0], action)\n",
    "\n",
    "            try:\n",
    "                action2 = pol.compute_single_action(\n",
    "                    input_dict=input_dict,\n",
    "                    explore=explore,\n",
    "                    timestep=timestep,\n",
    "                    **call_kwargs,\n",
    "                )\n",
    "                # Make sure these are the same, unless we have exploration\n",
    "                # switched on (or noisy layers).\n",
    "                if not explore and not pol.config.get(\"noisy\"):\n",
    "                    check(action, action2)\n",
    "            except TypeError:\n",
    "                pass\n",
    "        else:\n",
    "            action = what.compute_single_action(\n",
    "                obs,\n",
    "                state_in,\n",
    "                prev_action=action_in,\n",
    "                prev_reward=reward_in,\n",
    "                explore=explore,\n",
    "                timestep=timestep,\n",
    "                unsquash_action=unsquash,\n",
    "                clip_action=clip,\n",
    "                **call_kwargs,\n",
    "            )\n",
    "\n",
    "        state_out = None\n",
    "        if state_in or full_fetch or what is pol:\n",
    "            action, state_out, _ = action\n",
    "        if state_out:\n",
    "            for si, so in zip(tree.flatten(state_in), tree.flatten(state_out)):\n",
    "                if tf.is_tensor(si):\n",
    "                    # If si is a tensor of Dimensions, we need to convert it\n",
    "                    # We expect this to be the case for TF RLModules who's initial\n",
    "                    # states are Tf Tensors.\n",
    "                    si_shape = si.shape.as_list()\n",
    "                else:\n",
    "                    si_shape = list(si.shape)\n",
    "                check(si_shape, so.shape)\n",
    "\n",
    "        if unsquash is None:\n",
    "            unsquash = what.config[\"normalize_actions\"]\n",
    "        if clip is None:\n",
    "            clip = what.config[\"clip_actions\"]\n",
    "\n",
    "        # Test whether unsquash/clipping works on the Algorithm's\n",
    "        # compute_single_action method: Both flags should force the action\n",
    "        # to be within the space's bounds.\n",
    "        if method_to_test == \"single\" and what == algorithm:\n",
    "            if not action_space.contains(action) and (\n",
    "                clip or unsquash or not isinstance(action_space, Box)\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"Returned action ({action}) of algorithm/policy {what} \"\n",
    "                    f\"not in Env's action_space {action_space}\"\n",
    "                )\n",
    "            # We are operating in normalized space: Expect only smaller action\n",
    "            # values.\n",
    "            if (\n",
    "                isinstance(action_space, Box)\n",
    "                and not unsquash\n",
    "                and what.config.get(\"normalize_actions\")\n",
    "                and np.any(np.abs(action) > 15.0)\n",
    "            ):\n",
    "                raise ValueError(\n",
    "                    f\"Returned action ({action}) of algorithm/policy {what} \"\n",
    "                    \"should be in normalized space, but seems too large/small \"\n",
    "                    \"for that!\"\n",
    "                )\n",
    "\n",
    "    # Loop through: Policy vs Algorithm; Different API methods to calculate\n",
    "    # actions; unsquash option; clip option; full fetch or not.\n",
    "    for what in [pol, algorithm]:\n",
    "        if what is algorithm:\n",
    "            # Get the obs-space from Workers.env (not Policy) due to possible\n",
    "            # pre-processor up front.\n",
    "            worker_set = getattr(algorithm, \"env_runner_group\", None)\n",
    "            assert worker_set\n",
    "            if not worker_set.local_env_runner:\n",
    "                obs_space = algorithm.get_policy(pid).observation_space\n",
    "            else:\n",
    "                obs_space = worker_set.local_env_runner.for_policy(\n",
    "                    lambda p: p.observation_space, policy_id=pid\n",
    "                )\n",
    "            obs_space = getattr(obs_space, \"original_space\", obs_space)\n",
    "        else:\n",
    "            obs_space = pol.observation_space\n",
    "\n",
    "        for method_to_test in [\"single\"] + ([\"input_dict\"] if what is pol else []):\n",
    "            for explore in [True, False]:\n",
    "                for full_fetch in [False, True] if what is algorithm else [False]:\n",
    "                    timestep = random.randint(0, 100000)\n",
    "                    for unsquash in [True, False, None]:\n",
    "                        for clip in [False] if unsquash else [True, False, None]:\n",
    "                            print(\"-\" * 80)\n",
    "                            print(f\"what={what}\")\n",
    "                            print(f\"method_to_test={method_to_test}\")\n",
    "                            print(f\"explore={explore}\")\n",
    "                            print(f\"full_fetch={full_fetch}\")\n",
    "                            print(f\"unsquash={unsquash}\")\n",
    "                            print(f\"clip={clip}\")\n",
    "                            _test(\n",
    "                                what,\n",
    "                                method_to_test,\n",
    "                                obs_space,\n",
    "                                full_fetch,\n",
    "                                explore,\n",
    "                                timestep,\n",
    "                                unsquash,\n",
    "                                clip,\n",
    "                            )\n",
    "\n",
    "\n",
    "def check_inference_w_connectors(policy, env_name, max_steps: int = 100):\n",
    "    \"\"\"Checks whether the given policy can infer actions from an env with connectors.\n",
    "\n",
    "    Args:\n",
    "        policy: The policy to check.\n",
    "        env_name: Name of the environment to check\n",
    "        max_steps: The maximum number of steps to run the environment for.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the policy cannot infer actions from the environment.\n",
    "    \"\"\"\n",
    "    # Avoids circular import\n",
    "    from ray.rllib.utils.policy import local_policy_inference\n",
    "\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Potentially wrap the env like we do in RolloutWorker\n",
    "    if is_atari(env):\n",
    "        env = wrap_deepmind(\n",
    "            env,\n",
    "            dim=policy.config[\"model\"][\"dim\"],\n",
    "            framestack=policy.config[\"model\"].get(\"framestack\"),\n",
    "        )\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    reward, terminated, truncated = 0.0, False, False\n",
    "    ts = 0\n",
    "    while not terminated and not truncated and ts < max_steps:\n",
    "        action_out = local_policy_inference(\n",
    "            policy,\n",
    "            env_id=0,\n",
    "            agent_id=0,\n",
    "            obs=obs,\n",
    "            reward=reward,\n",
    "            terminated=terminated,\n",
    "            truncated=truncated,\n",
    "            info=info,\n",
    "        )\n",
    "        obs, reward, terminated, truncated, info = env.step(action_out[0][0])\n",
    "\n",
    "        ts += 1\n",
    "\n",
    "\n",
    "def check_learning_achieved(\n",
    "    tune_results: \"tune.ResultGrid\",\n",
    "    min_value: float,\n",
    "    evaluation: Optional[bool] = None,\n",
    "    metric: str = f\"{ENV_RUNNER_RESULTS}/episode_return_mean\",\n",
    "):\n",
    "    \"\"\"Throws an error if `min_reward` is not reached within tune_results.\n",
    "\n",
    "    Checks the last iteration found in tune_results for its\n",
    "    \"episode_return_mean\" value and compares it to `min_reward`.\n",
    "\n",
    "    Args:\n",
    "        tune_results: The tune.Tuner().fit() returned results object.\n",
    "        min_reward: The min reward that must be reached.\n",
    "        evaluation: If True, use `evaluation/env_runners/[metric]`, if False, use\n",
    "            `env_runners/[metric]`, if None, use evaluation sampler results if\n",
    "            available otherwise, use train sampler results.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `min_reward` not reached.\n",
    "    \"\"\"\n",
    "    # Get maximum value of `metrics` over all trials\n",
    "    # (check if at least one trial achieved some learning, not just the final one).\n",
    "    recorded_values = []\n",
    "    for _, row in tune_results.get_dataframe().iterrows():\n",
    "        if evaluation or (\n",
    "            evaluation is None and f\"{EVALUATION_RESULTS}/{metric}\" in row\n",
    "        ):\n",
    "            recorded_values.append(row[f\"{EVALUATION_RESULTS}/{metric}\"])\n",
    "        else:\n",
    "            recorded_values.append(row[metric])\n",
    "    best_value = max(recorded_values)\n",
    "    if best_value < min_value:\n",
    "        raise ValueError(f\"`{metric}` of {min_value} not reached!\")\n",
    "    print(f\"`{metric}` of {min_value} reached! ok\")\n",
    "\n",
    "\n",
    "def check_off_policyness(\n",
    "    results: ResultDict,\n",
    "    upper_limit: float,\n",
    "    lower_limit: float = 0.0,\n",
    ") -> Optional[float]:\n",
    "    \"\"\"Verifies that the off-policy'ness of some update is within some range.\n",
    "\n",
    "    Off-policy'ness is defined as the average (across n workers) diff\n",
    "    between the number of gradient updates performed on the policy used\n",
    "    for sampling vs the number of gradient updates that have been performed\n",
    "    on the trained policy (usually the one on the local worker).\n",
    "\n",
    "    Uses the published DIFF_NUM_GRAD_UPDATES_VS_SAMPLER_POLICY metric inside\n",
    "    a training results dict and compares to the given bounds.\n",
    "\n",
    "    Note: Only works with single-agent results thus far.\n",
    "\n",
    "    Args:\n",
    "        results: The training results dict.\n",
    "        upper_limit: The upper limit to for the off_policy_ness value.\n",
    "        lower_limit: The lower limit to for the off_policy_ness value.\n",
    "\n",
    "    Returns:\n",
    "        The off-policy'ness value (described above).\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the value is out of bounds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Have to import this here to avoid circular dependency.\n",
    "    from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "    from ray.rllib.utils.metrics.learner_info import LEARNER_INFO\n",
    "\n",
    "    # Assert that the off-policy'ness is within the given bounds.\n",
    "    learner_info = results[\"info\"][LEARNER_INFO]\n",
    "    if DEFAULT_POLICY_ID not in learner_info:\n",
    "        return None\n",
    "    off_policy_ness = learner_info[DEFAULT_POLICY_ID][\n",
    "        DIFF_NUM_GRAD_UPDATES_VS_SAMPLER_POLICY\n",
    "    ]\n",
    "    # Roughly: Reaches up to 0.4 for 2 rollout workers and up to 0.2 for\n",
    "    # 1 rollout worker.\n",
    "    if not (lower_limit <= off_policy_ness <= upper_limit):\n",
    "        raise AssertionError(\n",
    "            f\"`off_policy_ness` ({off_policy_ness}) is outside the given bounds \"\n",
    "            f\"({lower_limit} - {upper_limit})!\"\n",
    "        )\n",
    "\n",
    "    return off_policy_ness\n",
    "\n",
    "\n",
    "def check_train_results_new_api_stack(train_results: ResultDict) -> None:\n",
    "    \"\"\"Checks proper structure of a Algorithm.train() returned dict.\n",
    "\n",
    "    Args:\n",
    "        train_results: The train results dict to check.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If `train_results` doesn't have the proper structure or\n",
    "            data in it.\n",
    "    \"\"\"\n",
    "    # Import these here to avoid circular dependencies.\n",
    "    from ray.rllib.utils.metrics import (\n",
    "        ENV_RUNNER_RESULTS,\n",
    "        FAULT_TOLERANCE_STATS,\n",
    "        LEARNER_RESULTS,\n",
    "        TIMERS,\n",
    "    )\n",
    "\n",
    "    # Assert that some keys are where we would expect them.\n",
    "    for key in [\n",
    "        ENV_RUNNER_RESULTS,\n",
    "        FAULT_TOLERANCE_STATS,\n",
    "        LEARNER_RESULTS,\n",
    "        TIMERS,\n",
    "        TRAINING_ITERATION,\n",
    "        \"config\",\n",
    "    ]:\n",
    "        assert (\n",
    "            key in train_results\n",
    "        ), f\"'{key}' not found in `train_results` ({train_results})!\"\n",
    "\n",
    "    # Make sure, `config` is an actual dict, not an AlgorithmConfig object.\n",
    "    assert isinstance(\n",
    "        train_results[\"config\"], dict\n",
    "    ), \"`config` in results not a python dict!\"\n",
    "\n",
    "    from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "\n",
    "    is_multi_agent = (\n",
    "        AlgorithmConfig()\n",
    "        .update_from_dict({\"policies\": train_results[\"config\"][\"policies\"]})\n",
    "        .is_multi_agent\n",
    "    )\n",
    "\n",
    "    # Check in particular the \"info\" dict.\n",
    "    learner_results = train_results[LEARNER_RESULTS]\n",
    "\n",
    "    # Make sure we have a `DEFAULT_MODULE_ID key if we are not in a\n",
    "    # multi-agent setup.\n",
    "    if not is_multi_agent:\n",
    "        assert len(learner_results) == 0 or DEFAULT_MODULE_ID in learner_results, (\n",
    "            f\"'{DEFAULT_MODULE_ID}' not found in \"\n",
    "            f\"train_results['{LEARNER_RESULTS}']!\"\n",
    "        )\n",
    "\n",
    "    for module_id, module_metrics in learner_results.items():\n",
    "        # The ModuleID can be __all_modules__ in multi-agent case when the new learner\n",
    "        # stack is enabled.\n",
    "        if module_id == \"__all_modules__\":\n",
    "            continue\n",
    "\n",
    "        # On the new API stack, policy has no LEARNER_STATS_KEY under it anymore.\n",
    "        for key, value in module_metrics.items():\n",
    "            # Min- and max-stats should be single values.\n",
    "            if key.endswith(\"_min\") or key.endswith(\"_max\"):\n",
    "                assert np.isscalar(value), f\"'key' value not a scalar ({value})!\"\n",
    "\n",
    "    return train_results\n",
    "\n",
    "\n",
    "@OldAPIStack\n",
    "def check_train_results(train_results: ResultDict):\n",
    "    \"\"\"Checks proper structure of a Algorithm.train() returned dict.\n",
    "\n",
    "    Args:\n",
    "        train_results: The train results dict to check.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If `train_results` doesn't have the proper structure or\n",
    "            data in it.\n",
    "    \"\"\"\n",
    "    # Import these here to avoid circular dependencies.\n",
    "    from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "    from ray.rllib.utils.metrics.learner_info import LEARNER_INFO, LEARNER_STATS_KEY\n",
    "\n",
    "    # Assert that some keys are where we would expect them.\n",
    "    for key in [\n",
    "        \"config\",\n",
    "        \"custom_metrics\",\n",
    "        ENV_RUNNER_RESULTS,\n",
    "        \"info\",\n",
    "        \"iterations_since_restore\",\n",
    "        \"num_healthy_workers\",\n",
    "        \"perf\",\n",
    "        \"time_since_restore\",\n",
    "        \"time_this_iter_s\",\n",
    "        \"timers\",\n",
    "        \"time_total_s\",\n",
    "        TRAINING_ITERATION,\n",
    "    ]:\n",
    "        assert (\n",
    "            key in train_results\n",
    "        ), f\"'{key}' not found in `train_results` ({train_results})!\"\n",
    "\n",
    "    for key in [\n",
    "        \"episode_len_mean\",\n",
    "        \"episode_reward_max\",\n",
    "        \"episode_reward_mean\",\n",
    "        \"episode_reward_min\",\n",
    "        \"hist_stats\",\n",
    "        \"policy_reward_max\",\n",
    "        \"policy_reward_mean\",\n",
    "        \"policy_reward_min\",\n",
    "        \"sampler_perf\",\n",
    "    ]:\n",
    "        assert key in train_results[ENV_RUNNER_RESULTS], (\n",
    "            f\"'{key}' not found in `train_results[ENV_RUNNER_RESULTS]` \"\n",
    "            f\"({train_results[ENV_RUNNER_RESULTS]})!\"\n",
    "        )\n",
    "\n",
    "    # Make sure, `config` is an actual dict, not an AlgorithmConfig object.\n",
    "    assert isinstance(\n",
    "        train_results[\"config\"], dict\n",
    "    ), \"`config` in results not a python dict!\"\n",
    "\n",
    "    from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "\n",
    "    is_multi_agent = (\n",
    "        AlgorithmConfig()\n",
    "        .update_from_dict({\"policies\": train_results[\"config\"][\"policies\"]})\n",
    "        .is_multi_agent\n",
    "    )\n",
    "\n",
    "    # Check in particular the \"info\" dict.\n",
    "    info = train_results[\"info\"]\n",
    "    assert LEARNER_INFO in info, f\"'learner' not in train_results['infos'] ({info})!\"\n",
    "    assert (\n",
    "        \"num_steps_trained\" in info or NUM_ENV_STEPS_TRAINED in info\n",
    "    ), f\"'num_(env_)?steps_trained' not in train_results['infos'] ({info})!\"\n",
    "\n",
    "    learner_info = info[LEARNER_INFO]\n",
    "\n",
    "    # Make sure we have a default_policy key if we are not in a\n",
    "    # multi-agent setup.\n",
    "    if not is_multi_agent:\n",
    "        # APEX algos sometimes have an empty learner info dict (no metrics\n",
    "        # collected yet).\n",
    "        assert len(learner_info) == 0 or DEFAULT_POLICY_ID in learner_info, (\n",
    "            f\"'{DEFAULT_POLICY_ID}' not found in \"\n",
    "            f\"train_results['infos']['learner'] ({learner_info})!\"\n",
    "        )\n",
    "\n",
    "    for pid, policy_stats in learner_info.items():\n",
    "        if pid == \"batch_count\":\n",
    "            continue\n",
    "\n",
    "        # the pid can be __all__ in multi-agent case when the new learner stack is\n",
    "        # enabled.\n",
    "        if pid == \"__all__\":\n",
    "            continue\n",
    "\n",
    "        # On the new API stack, policy has no LEARNER_STATS_KEY under it anymore.\n",
    "        if LEARNER_STATS_KEY in policy_stats:\n",
    "            learner_stats = policy_stats[LEARNER_STATS_KEY]\n",
    "        else:\n",
    "            learner_stats = policy_stats\n",
    "        for key, value in learner_stats.items():\n",
    "            # Min- and max-stats should be single values.\n",
    "            if key.startswith(\"min_\") or key.startswith(\"max_\"):\n",
    "                assert np.isscalar(value), f\"'key' value not a scalar ({value})!\"\n",
    "\n",
    "    return train_results\n",
    "\n",
    "\n",
    "# TODO (simon): Use this function in the `run_rllib_example_experiment` when\n",
    "# `no_tune` is `True`.\n",
    "def should_stop(\n",
    "    stop: Dict[str, Any], results: ResultDict, keep_ray_up: bool = False\n",
    ") -> bool:\n",
    "    \"\"\"Checks stopping criteria on `ResultDict`\n",
    "\n",
    "    Args:\n",
    "        stop: Dictionary of stopping criteria. Each criterium is a mapping of\n",
    "            a metric in the `ResultDict` of the algorithm to a certain criterium.\n",
    "        results: An RLlib `ResultDict` containing all results from a training step.\n",
    "        keep_ray_up: Optionally shutting down the runnin Ray instance.\n",
    "\n",
    "    Returns: True, if any stopping criterium is fulfilled. Otherwise, False.\n",
    "    \"\"\"\n",
    "    for key, threshold in stop.items():\n",
    "        val = results\n",
    "        for k in key.split(\"/\"):\n",
    "            k = k.strip()\n",
    "            # If k exists in the current level, continue down;\n",
    "            # otherwise, set val to None and break out of this inner loop.\n",
    "            if isinstance(val, dict) and k in val:\n",
    "                val = val[k]\n",
    "            else:\n",
    "                val = None\n",
    "                break\n",
    "\n",
    "        # If the key was not found, simply skip to the next criterion.\n",
    "        if val is None:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Check that val is numeric and meets the threshold.\n",
    "            if not np.isnan(val) and val >= threshold:\n",
    "                print(f\"Stop criterion ({key}={threshold}) fulfilled!\")\n",
    "                if not keep_ray_up:\n",
    "                    ray.shutdown()\n",
    "                return True\n",
    "        except TypeError:\n",
    "            # If val isn't numeric, skip this criterion.\n",
    "            continue\n",
    "\n",
    "    # If none of the criteria are fulfilled, return False.\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425fae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO (sven): Make this the de-facto, well documented, and unified utility for most of\n",
    "#  our tests:\n",
    "#  - CI (label: \"learning_tests\")\n",
    "#  - release tests (benchmarks)\n",
    "#  - example scripts\n",
    "def run_rllib_example_script_experiment(\n",
    "    base_config: \"AlgorithmConfig\",\n",
    "    args: Optional[argparse.Namespace] = None,\n",
    "    *,\n",
    "    stop: Optional[Dict] = None,\n",
    "    success_metric: Optional[Dict] = None,\n",
    "    trainable: Optional[Type] = None,\n",
    "    tune_callbacks: Optional[List] = None,\n",
    "    keep_config: bool = False,\n",
    "    keep_ray_up: bool = False,\n",
    "    scheduler=None,\n",
    "    progress_reporter=None,\n",
    ") -> Union[ResultDict, tune.result_grid.ResultGrid]:\n",
    "    \"\"\"Given an algorithm config and some command line args, runs an experiment.\n",
    "\n",
    "    There are some constraints on what properties must be defined in `args`.\n",
    "    It should ideally be generated via calling\n",
    "    `args = add_rllib_example_script_args()`, which can be found in this very module\n",
    "    here.\n",
    "\n",
    "    The function sets up an Algorithm object from the given config (altered by the\n",
    "    contents of `args`), then runs the Algorithm via Tune (or manually, if\n",
    "    `args.no_tune` is set to True) using the stopping criteria in `stop`.\n",
    "\n",
    "    At the end of the experiment, if `args.as_test` is True, checks, whether the\n",
    "    Algorithm reached the `success_metric` (if None, use `env_runners/\n",
    "    episode_return_mean` with a minimum value of `args.stop_reward`).\n",
    "\n",
    "    See https://github.com/ray-project/ray/tree/master/rllib/examples for an overview\n",
    "    of all supported command line options.\n",
    "\n",
    "    Args:\n",
    "        base_config: The AlgorithmConfig object to use for this experiment. This base\n",
    "            config will be automatically \"extended\" based on some of the provided\n",
    "            `args`. For example, `args.num_env_runners` is used to set\n",
    "            `config.num_env_runners`, etc..\n",
    "        args: A argparse.Namespace object, ideally returned by calling\n",
    "            `args = add_rllib_example_script_args()`. It must have the following\n",
    "            properties defined: `stop_iters`, `stop_reward`, `stop_timesteps`,\n",
    "            `no_tune`, `verbose`, `checkpoint_freq`, `as_test`. Optionally, for WandB\n",
    "            logging: `wandb_key`, `wandb_project`, `wandb_run_name`.\n",
    "        stop: An optional dict mapping ResultDict key strings (using \"/\" in case of\n",
    "            nesting, e.g. \"env_runners/episode_return_mean\" for referring to\n",
    "            `result_dict['env_runners']['episode_return_mean']` to minimum\n",
    "            values, reaching of which will stop the experiment). Default is:\n",
    "            {\n",
    "            \"env_runners/episode_return_mean\": args.stop_reward,\n",
    "            \"training_iteration\": args.stop_iters,\n",
    "            \"num_env_steps_sampled_lifetime\": args.stop_timesteps,\n",
    "            }\n",
    "        success_metric: Only relevant if `args.as_test` is True.\n",
    "            A dict mapping a single(!) ResultDict key string (using \"/\" in\n",
    "            case of nesting, e.g. \"env_runners/episode_return_mean\" for referring\n",
    "            to `result_dict['env_runners']['episode_return_mean']` to a single(!)\n",
    "            minimum value to be reached in order for the experiment to count as\n",
    "            successful. If `args.as_test` is True AND this `success_metric` is not\n",
    "            reached with the bounds defined by `stop`, will raise an Exception.\n",
    "        trainable: The Trainable sub-class to run in the tune.Tuner. If None (default),\n",
    "            use the registered RLlib Algorithm class specified by args.algo.\n",
    "        tune_callbacks: A list of Tune callbacks to configure with the tune.Tuner.\n",
    "            In case `args.wandb_key` is provided, appends a WandB logger to this\n",
    "            list.\n",
    "        keep_config: Set this to True, if you don't want this utility to change the\n",
    "            given `base_config` in any way and leave it as-is. This is helpful\n",
    "            for those example scripts which demonstrate how to set config settings\n",
    "            that are otherwise taken care of automatically in this function (e.g.\n",
    "            `num_env_runners`).\n",
    "\n",
    "    Returns:\n",
    "        The last ResultDict from a --no-tune run OR the tune.Tuner.fit()\n",
    "        results.\n",
    "    \"\"\"\n",
    "    if args is None:\n",
    "        parser = add_rllib_example_script_args()\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    # If run --as-release-test, --as-test must also be set.\n",
    "    if args.as_release_test:\n",
    "        args.as_test = True\n",
    "\n",
    "    # Initialize Ray.\n",
    "    ray.init(\n",
    "        num_cpus=args.num_cpus or None,\n",
    "        local_mode=args.local_mode,\n",
    "        ignore_reinit_error=True,\n",
    "    )\n",
    "\n",
    "    # Define one or more stopping criteria.\n",
    "    if stop is None:\n",
    "        stop = {\n",
    "            f\"{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\": args.stop_reward,\n",
    "            f\"{ENV_RUNNER_RESULTS}/{NUM_ENV_STEPS_SAMPLED_LIFETIME}\": (\n",
    "                args.stop_timesteps\n",
    "            ),\n",
    "            TRAINING_ITERATION: args.stop_iters,\n",
    "        }\n",
    "\n",
    "    config = base_config\n",
    "\n",
    "    # Enhance the `base_config`, based on provided `args`.\n",
    "    if not keep_config:\n",
    "        # Set the framework.\n",
    "        config.framework(args.framework)\n",
    "\n",
    "        # Add an env specifier (only if not already set in config)?\n",
    "        if args.env is not None and config.env is None:\n",
    "            config.environment(args.env)\n",
    "\n",
    "        # Disable the new API stack?\n",
    "        if not args.enable_new_api_stack:\n",
    "            config.api_stack(\n",
    "                enable_rl_module_and_learner=False,\n",
    "                enable_env_runner_and_connector_v2=False,\n",
    "            )\n",
    "\n",
    "        # Define EnvRunner scaling and behavior.\n",
    "        if args.num_env_runners is not None:\n",
    "            config.env_runners(num_env_runners=args.num_env_runners)\n",
    "        if args.num_envs_per_env_runner is not None:\n",
    "            config.env_runners(num_envs_per_env_runner=args.num_envs_per_env_runner)\n",
    "\n",
    "        # Define compute resources used automatically (only using the --num-learners\n",
    "        # and --num-gpus-per-learner args).\n",
    "        # New stack.\n",
    "        if config.enable_rl_module_and_learner:\n",
    "            if args.num_gpus is not None and args.num_gpus > 0:\n",
    "                raise ValueError(\n",
    "                    \"--num-gpus is not supported on the new API stack! To train on \"\n",
    "                    \"GPUs, use the command line options `--num-gpus-per-learner=1` and \"\n",
    "                    \"`--num-learners=[your number of available GPUs]`, instead.\"\n",
    "                )\n",
    "\n",
    "            # Do we have GPUs available in the cluster?\n",
    "            num_gpus_available = ray.cluster_resources().get(\"GPU\", 0)\n",
    "            # Number of actual Learner instances (including the local Learner if\n",
    "            # `num_learners=0`).\n",
    "            num_actual_learners = (\n",
    "                args.num_learners\n",
    "                if args.num_learners is not None\n",
    "                else config.num_learners\n",
    "            ) or 1  # 1: There is always a local Learner, if num_learners=0.\n",
    "            # How many were hard-requested by the user\n",
    "            # (through explicit `--num-gpus-per-learner >= 1`).\n",
    "            num_gpus_requested = (args.num_gpus_per_learner or 0) * num_actual_learners\n",
    "            # Number of GPUs needed, if `num_gpus_per_learner=None` (auto).\n",
    "            num_gpus_needed_if_available = (\n",
    "                args.num_gpus_per_learner\n",
    "                if args.num_gpus_per_learner is not None\n",
    "                else 1\n",
    "            ) * num_actual_learners\n",
    "            # Define compute resources used.\n",
    "            config.resources(num_gpus=0)  # old API stack setting\n",
    "            if args.num_learners is not None:\n",
    "                config.learners(num_learners=args.num_learners)\n",
    "\n",
    "            # User wants to use aggregator actors per Learner.\n",
    "            if args.num_aggregator_actors_per_learner is not None:\n",
    "                config.learners(\n",
    "                    num_aggregator_actors_per_learner=(\n",
    "                        args.num_aggregator_actors_per_learner\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            # User wants to use GPUs if available, but doesn't hard-require them.\n",
    "            if args.num_gpus_per_learner is None:\n",
    "                if num_gpus_available >= num_gpus_needed_if_available:\n",
    "                    config.learners(num_gpus_per_learner=1)\n",
    "                else:\n",
    "                    config.learners(num_gpus_per_learner=0)\n",
    "            # User hard-requires n GPUs, but they are not available -> Error.\n",
    "            elif num_gpus_available < num_gpus_requested:\n",
    "                raise ValueError(\n",
    "                    \"You are running your script with --num-learners=\"\n",
    "                    f\"{args.num_learners} and --num-gpus-per-learner=\"\n",
    "                    f\"{args.num_gpus_per_learner}, but your cluster only has \"\n",
    "                    f\"{num_gpus_available} GPUs!\"\n",
    "                )\n",
    "\n",
    "            # All required GPUs are available -> Use them.\n",
    "            else:\n",
    "                config.learners(num_gpus_per_learner=args.num_gpus_per_learner)\n",
    "\n",
    "            # Set CPUs per Learner.\n",
    "            if args.num_cpus_per_learner is not None:\n",
    "                config.learners(num_cpus_per_learner=args.num_cpus_per_learner)\n",
    "\n",
    "        # Old stack (override only if arg was provided by user).\n",
    "        elif args.num_gpus is not None:\n",
    "            config.resources(num_gpus=args.num_gpus)\n",
    "\n",
    "        # Evaluation setup.\n",
    "        if args.evaluation_interval > 0:\n",
    "            config.evaluation(\n",
    "                evaluation_num_env_runners=args.evaluation_num_env_runners,\n",
    "                evaluation_interval=args.evaluation_interval,\n",
    "                evaluation_duration=args.evaluation_duration,\n",
    "                evaluation_duration_unit=args.evaluation_duration_unit,\n",
    "                evaluation_parallel_to_training=args.evaluation_parallel_to_training,\n",
    "            )\n",
    "\n",
    "        # Set the log-level (if applicable).\n",
    "        if args.log_level is not None:\n",
    "            config.debugging(log_level=args.log_level)\n",
    "\n",
    "        # Set the output dir (if applicable).\n",
    "        if args.output is not None:\n",
    "            config.offline_data(output=args.output)\n",
    "\n",
    "    # Run the experiment w/o Tune (directly operate on the RLlib Algorithm object).\n",
    "    if args.no_tune:\n",
    "        assert not args.as_test and not args.as_release_test\n",
    "        algo = config.build()\n",
    "        for i in range(stop.get(TRAINING_ITERATION, args.stop_iters)):\n",
    "            results = algo.train()\n",
    "            if ENV_RUNNER_RESULTS in results:\n",
    "                mean_return = results[ENV_RUNNER_RESULTS].get(\n",
    "                    EPISODE_RETURN_MEAN, np.nan\n",
    "                )\n",
    "                print(f\"iter={i} R={mean_return}\", end=\"\")\n",
    "            if (\n",
    "                EVALUATION_RESULTS in results\n",
    "                and ENV_RUNNER_RESULTS in results[EVALUATION_RESULTS]\n",
    "            ):\n",
    "                Reval = results[EVALUATION_RESULTS][ENV_RUNNER_RESULTS][\n",
    "                    EPISODE_RETURN_MEAN\n",
    "                ]\n",
    "                print(f\" R(eval)={Reval}\", end=\"\")\n",
    "            print()\n",
    "            for key, threshold in stop.items():\n",
    "                val = results\n",
    "                for k in key.split(\"/\"):\n",
    "                    try:\n",
    "                        val = val[k]\n",
    "                    except KeyError:\n",
    "                        val = None\n",
    "                        break\n",
    "                if val is not None and not np.isnan(val) and val >= threshold:\n",
    "                    print(f\"Stop criterium ({key}={threshold}) fulfilled!\")\n",
    "                    if not keep_ray_up:\n",
    "                        ray.shutdown()\n",
    "                    return results\n",
    "\n",
    "        if not keep_ray_up:\n",
    "            ray.shutdown()\n",
    "        return results\n",
    "\n",
    "    # Run the experiment using Ray Tune.\n",
    "\n",
    "    # Log results using WandB.\n",
    "    tune_callbacks = tune_callbacks or []\n",
    "    if hasattr(args, \"wandb_key\") and (\n",
    "        args.wandb_key is not None or WANDB_ENV_VAR in os.environ\n",
    "    ):\n",
    "        wandb_key = args.wandb_key or os.environ[WANDB_ENV_VAR]\n",
    "        project = args.wandb_project or (\n",
    "            args.algo.lower() + \"-\" + re.sub(\"\\\\W+\", \"-\", str(config.env).lower())\n",
    "        )\n",
    "        tune_callbacks.append(\n",
    "            WandbLoggerCallback(\n",
    "                api_key=wandb_key,\n",
    "                project=project,\n",
    "                upload_checkpoints=True,\n",
    "                **({\"name\": args.wandb_run_name} if args.wandb_run_name else {}),\n",
    "            )\n",
    "        )\n",
    "    # Auto-configure a CLIReporter (to log the results to the console).\n",
    "    # Use better ProgressReporter for multi-agent cases: List individual policy rewards.\n",
    "    if progress_reporter is None and args.num_agents > 0:\n",
    "        progress_reporter = CLIReporter(\n",
    "            metric_columns={\n",
    "                **{\n",
    "                    TRAINING_ITERATION: \"iter\",\n",
    "                    \"time_total_s\": \"total time (s)\",\n",
    "                    NUM_ENV_STEPS_SAMPLED_LIFETIME: \"ts\",\n",
    "                    f\"{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\": \"combined return\",\n",
    "                },\n",
    "                **{\n",
    "                    (\n",
    "                        f\"{ENV_RUNNER_RESULTS}/module_episode_returns_mean/\" f\"{pid}\"\n",
    "                    ): f\"return {pid}\"\n",
    "                    for pid in config.policies\n",
    "                },\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Force Tuner to use old progress output as the new one silently ignores our custom\n",
    "    # `CLIReporter`.\n",
    "    os.environ[\"RAY_AIR_NEW_OUTPUT\"] = \"0\"\n",
    "\n",
    "    # Run the actual experiment (using Tune).\n",
    "    start_time = time.time()\n",
    "    results = tune.Tuner(\n",
    "        trainable or config.algo_class,\n",
    "        param_space=config,\n",
    "        run_config=tune.RunConfig(\n",
    "            stop=stop,\n",
    "            verbose=args.verbose,\n",
    "            callbacks=tune_callbacks,\n",
    "            checkpoint_config=tune.CheckpointConfig(\n",
    "                checkpoint_frequency=args.checkpoint_freq,\n",
    "                checkpoint_at_end=args.checkpoint_at_end,\n",
    "            ),\n",
    "            progress_reporter=progress_reporter,\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            num_samples=args.num_samples,\n",
    "            max_concurrent_trials=args.max_concurrent_trials,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "    ).fit()\n",
    "    time_taken = time.time() - start_time\n",
    "\n",
    "    if not keep_ray_up:\n",
    "        ray.shutdown()\n",
    "\n",
    "    # Error out, if Tuner.fit() failed to run. Otherwise, erroneous examples might pass\n",
    "    # the CI tests w/o us knowing that they are broken (b/c some examples do not have\n",
    "    # a --as-test flag and/or any passing criteris).\n",
    "    if results.errors:\n",
    "        raise RuntimeError(\n",
    "            \"Running the example script resulted in one or more errors! \"\n",
    "            f\"{[e.args[0].args[2] for e in results.errors]}\"\n",
    "        )\n",
    "\n",
    "    # If run as a test, check whether we reached the specified success criteria.\n",
    "    test_passed = False\n",
    "    if args.as_test:\n",
    "        # Success metric not provided, try extracting it from `stop`.\n",
    "        if success_metric is None:\n",
    "            for try_it in [\n",
    "                f\"{EVALUATION_RESULTS}/{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\",\n",
    "                f\"{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\",\n",
    "            ]:\n",
    "                if try_it in stop:\n",
    "                    success_metric = {try_it: stop[try_it]}\n",
    "                    break\n",
    "            if success_metric is None:\n",
    "                success_metric = {\n",
    "                    f\"{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\": args.stop_reward,\n",
    "                }\n",
    "        # TODO (sven): Make this work for more than one metric (AND-logic?).\n",
    "        # Get maximum value of `metric` over all trials\n",
    "        # (check if at least one trial achieved some learning, not just the final one).\n",
    "        success_metric_key, success_metric_value = next(iter(success_metric.items()))\n",
    "        best_value = max(\n",
    "            row[success_metric_key] for _, row in results.get_dataframe().iterrows()\n",
    "        )\n",
    "        if best_value >= success_metric_value:\n",
    "            test_passed = True\n",
    "            print(f\"`{success_metric_key}` of {success_metric_value} reached! ok\")\n",
    "\n",
    "        if args.as_release_test:\n",
    "            trial = results._experiment_analysis.trials[0]\n",
    "            stats = trial.last_result\n",
    "            stats.pop(\"config\", None)\n",
    "            json_summary = {\n",
    "                \"time_taken\": float(time_taken),\n",
    "                \"trial_states\": [trial.status],\n",
    "                \"last_update\": float(time.time()),\n",
    "                \"stats\": stats,\n",
    "                \"passed\": [test_passed],\n",
    "                \"not_passed\": [not test_passed],\n",
    "                \"failures\": {str(trial): 1} if not test_passed else {},\n",
    "            }\n",
    "            with open(\n",
    "                os.environ.get(\"TEST_OUTPUT_JSON\", \"/tmp/learning_test.json\"),\n",
    "                \"wt\",\n",
    "            ) as f:\n",
    "                try:\n",
    "                    json.dump(json_summary, f)\n",
    "                # Something went wrong writing json. Try again w/ simplified stats.\n",
    "                except Exception:\n",
    "                    from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "                    simplified_stats = {\n",
    "                        k: stats[k] for k in Algorithm._progress_metrics if k in stats\n",
    "                    }\n",
    "                    json_summary[\"stats\"] = simplified_stats\n",
    "                    json.dump(json_summary, f)\n",
    "\n",
    "        if not test_passed:\n",
    "            raise ValueError(\n",
    "                f\"`{success_metric_key}` of {success_metric_value} not reached!\"\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "796933ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_same_batch(batch1, batch2) -> None:\n",
    "    \"\"\"Check if both batches are (almost) identical.\n",
    "\n",
    "    For MultiAgentBatches, the step count and individual policy's\n",
    "    SampleBatches are checked for identity. For SampleBatches, identity is\n",
    "    checked as the almost numerical key-value-pair identity between batches\n",
    "    with ray.rllib.utils.test_utils.check(). unroll_id is compared only if\n",
    "    both batches have an unroll_id.\n",
    "\n",
    "    Args:\n",
    "        batch1: Batch to compare against batch2\n",
    "        batch2: Batch to compare against batch1\n",
    "    \"\"\"\n",
    "    # Avoids circular import\n",
    "    from ray.rllib.policy.sample_batch import MultiAgentBatch, SampleBatch\n",
    "\n",
    "    assert type(batch1) is type(\n",
    "        batch2\n",
    "    ), \"Input batches are of different types {} and {}\".format(\n",
    "        str(type(batch1)), str(type(batch2))\n",
    "    )\n",
    "\n",
    "    def check_sample_batches(_batch1, _batch2, _policy_id=None):\n",
    "        unroll_id_1 = _batch1.get(\"unroll_id\", None)\n",
    "        unroll_id_2 = _batch2.get(\"unroll_id\", None)\n",
    "        # unroll IDs only have to fit if both batches have them\n",
    "        if unroll_id_1 is not None and unroll_id_2 is not None:\n",
    "            assert unroll_id_1 == unroll_id_2\n",
    "\n",
    "        batch1_keys = set()\n",
    "        for k, v in _batch1.items():\n",
    "            # unroll_id is compared above already\n",
    "            if k == \"unroll_id\":\n",
    "                continue\n",
    "            check(v, _batch2[k])\n",
    "            batch1_keys.add(k)\n",
    "\n",
    "        batch2_keys = set(_batch2.keys())\n",
    "        # unroll_id is compared above already\n",
    "        batch2_keys.discard(\"unroll_id\")\n",
    "        _difference = batch1_keys.symmetric_difference(batch2_keys)\n",
    "\n",
    "        # Cases where one batch has info and the other has not\n",
    "        if _policy_id:\n",
    "            assert not _difference, (\n",
    "                \"SampleBatches for policy with ID {} \"\n",
    "                \"don't share information on the \"\n",
    "                \"following information: \\n{}\"\n",
    "                \"\".format(_policy_id, _difference)\n",
    "            )\n",
    "        else:\n",
    "            assert not _difference, (\n",
    "                \"SampleBatches don't share information \"\n",
    "                \"on the following information: \\n{}\"\n",
    "                \"\".format(_difference)\n",
    "            )\n",
    "\n",
    "    if type(batch1) is SampleBatch:\n",
    "        check_sample_batches(batch1, batch2)\n",
    "    elif type(batch1) is MultiAgentBatch:\n",
    "        assert batch1.count == batch2.count\n",
    "        batch1_ids = set()\n",
    "        for policy_id, policy_batch in batch1.policy_batches.items():\n",
    "            check_sample_batches(\n",
    "                policy_batch, batch2.policy_batches[policy_id], policy_id\n",
    "            )\n",
    "            batch1_ids.add(policy_id)\n",
    "\n",
    "        # Case where one ma batch has info on a policy the other has not\n",
    "        batch2_ids = set(batch2.policy_batches.keys())\n",
    "        difference = batch1_ids.symmetric_difference(batch2_ids)\n",
    "        assert (\n",
    "            not difference\n",
    "        ), f\"MultiAgentBatches don't share the following information: \\n{difference}.\"\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported batch type \" + str(type(batch1)))\n",
    "\n",
    "\n",
    "def check_reproducibilty(\n",
    "    algo_class: Type[\"Algorithm\"],\n",
    "    algo_config: \"AlgorithmConfig\",\n",
    "    *,\n",
    "    fw_kwargs: Dict[str, Any],\n",
    "    training_iteration: int = 1,\n",
    ") -> None:\n",
    "    # TODO @kourosh: we can get rid of examples/deterministic_training.py once\n",
    "    # this is added to all algorithms\n",
    "    \"\"\"Check if the algorithm is reproducible across different testing conditions:\n",
    "\n",
    "        frameworks: all input frameworks\n",
    "        num_gpus: int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\"))\n",
    "        num_workers: 0 (only local workers) or\n",
    "                     4 ((1) local workers + (4) remote workers)\n",
    "        num_envs_per_env_runner: 2\n",
    "\n",
    "    Args:\n",
    "        algo_class: Algorithm class to test.\n",
    "        algo_config: Base config to use for the algorithm.\n",
    "        fw_kwargs: Framework iterator keyword arguments.\n",
    "        training_iteration: Number of training iterations to run.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        It raises an AssertionError if the algorithm is not reproducible.\n",
    "    \"\"\"\n",
    "    from ray.rllib.policy.sample_batch import DEFAULT_POLICY_ID\n",
    "    from ray.rllib.utils.metrics.learner_info import LEARNER_INFO\n",
    "\n",
    "    stop_dict = {TRAINING_ITERATION: training_iteration}\n",
    "    # use 0 and 2 workers (for more that 4 workers we have to make sure the instance\n",
    "    # type in ci build has enough resources)\n",
    "    for num_workers in [0, 2]:\n",
    "        algo_config = (\n",
    "            algo_config.debugging(seed=42).env_runners(\n",
    "                num_env_runners=num_workers, num_envs_per_env_runner=2\n",
    "            )\n",
    "            # new API\n",
    "            .learners(\n",
    "                num_gpus_per_learner=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "            )\n",
    "            # old API\n",
    "            .resources(\n",
    "                num_gpus=int(os.environ.get(\"RLLIB_NUM_GPUS\", \"0\")),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Testing reproducibility of {algo_class.__name__}\"\n",
    "            f\" with {num_workers} workers\"\n",
    "        )\n",
    "        print(\"/// config\")\n",
    "        pprint.pprint(algo_config.to_dict())\n",
    "        # test tune.Tuner().fit() reproducibility\n",
    "        results1 = tune.Tuner(\n",
    "            algo_class,\n",
    "            param_space=algo_config.to_dict(),\n",
    "            run_config=tune.RunConfig(stop=stop_dict, verbose=1),\n",
    "        ).fit()\n",
    "        results1 = results1.get_best_result().metrics\n",
    "\n",
    "        results2 = tune.Tuner(\n",
    "            algo_class,\n",
    "            param_space=algo_config.to_dict(),\n",
    "            run_config=tune.RunConfig(stop=stop_dict, verbose=1),\n",
    "        ).fit()\n",
    "        results2 = results2.get_best_result().metrics\n",
    "\n",
    "        # Test rollout behavior.\n",
    "        check(\n",
    "            results1[ENV_RUNNER_RESULTS][\"hist_stats\"],\n",
    "            results2[ENV_RUNNER_RESULTS][\"hist_stats\"],\n",
    "        )\n",
    "        # As well as training behavior (minibatch sequence during SGD\n",
    "        # iterations).\n",
    "        # As well as training behavior (minibatch sequence during SGD\n",
    "        # iterations).\n",
    "        if algo_config.enable_rl_module_and_learner:\n",
    "            check(\n",
    "                results1[\"info\"][LEARNER_INFO][DEFAULT_POLICY_ID],\n",
    "                results2[\"info\"][LEARNER_INFO][DEFAULT_POLICY_ID],\n",
    "            )\n",
    "        else:\n",
    "            check(\n",
    "                results1[\"info\"][LEARNER_INFO][DEFAULT_POLICY_ID][\"learner_stats\"],\n",
    "                results2[\"info\"][LEARNER_INFO][DEFAULT_POLICY_ID][\"learner_stats\"],\n",
    "            )\n",
    "\n",
    "\n",
    "def get_cartpole_dataset_reader(batch_size: int = 1) -> \"DatasetReader\":\n",
    "    \"\"\"Returns a DatasetReader for the cartpole dataset.\n",
    "    Args:\n",
    "        batch_size: The batch size to use for the reader.\n",
    "    Returns:\n",
    "        A rllib DatasetReader for the cartpole dataset.\n",
    "    \"\"\"\n",
    "    from ray.rllib.algorithms import AlgorithmConfig\n",
    "    from ray.rllib.offline import IOContext\n",
    "    from ray.rllib.offline.dataset_reader import (\n",
    "        DatasetReader,\n",
    "        get_dataset_and_shards,\n",
    "    )\n",
    "\n",
    "    path = \"tests/data/cartpole/large.json\"\n",
    "    input_config = {\"format\": \"json\", \"paths\": path}\n",
    "    dataset, _ = get_dataset_and_shards(\n",
    "        AlgorithmConfig().offline_data(input_=\"dataset\", input_config=input_config)\n",
    "    )\n",
    "    ioctx = IOContext(\n",
    "        config=(\n",
    "            AlgorithmConfig()\n",
    "            .training(train_batch_size=batch_size)\n",
    "            .offline_data(actions_in_input_normalized=True)\n",
    "        ),\n",
    "        worker_index=0,\n",
    "    )\n",
    "    reader = DatasetReader(dataset, ioctx)\n",
    "    return reader\n",
    "\n",
    "\n",
    "class ModelChecker:\n",
    "    \"\"\"Helper class to compare architecturally identical Models across frameworks.\n",
    "\n",
    "    Holds a ModelConfig, such that individual models can be added simply via their\n",
    "    framework string (by building them with config.build(framework=...).\n",
    "    A call to `check()` forces all added models to be compared in terms of their\n",
    "    number of trainable and non-trainable parameters, as well as, their\n",
    "    computation results given a common weights structure and values and identical\n",
    "    inputs to the models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        # To compare number of params between frameworks.\n",
    "        self.param_counts = {}\n",
    "        # To compare computed outputs from fixed-weights-nets between frameworks.\n",
    "        self.output_values = {}\n",
    "\n",
    "        # We will pass an observation filled with this one random value through\n",
    "        # all DL networks (after they have been set to fixed-weights) to compare\n",
    "        # the computed outputs.\n",
    "        self.random_fill_input_value = np.random.uniform(-0.01, 0.01)\n",
    "\n",
    "        # Dict of models to check against each other.\n",
    "        self.models = {}\n",
    "\n",
    "    def add(self, framework: str = \"torch\", obs=True, state=False) -> Any:\n",
    "        \"\"\"Builds a new Model for the given framework.\"\"\"\n",
    "        model = self.models[framework] = self.config.build(framework=framework)\n",
    "\n",
    "        # Pass a B=1 observation through the model.\n",
    "        inputs = np.full(\n",
    "            [1] + ([1] if state else []) + list(self.config.input_dims),\n",
    "            self.random_fill_input_value,\n",
    "        )\n",
    "        if obs:\n",
    "            inputs = {Columns.OBS: inputs}\n",
    "        if state:\n",
    "            inputs[Columns.STATE_IN] = tree.map_structure(\n",
    "                lambda s: np.zeros(shape=[1] + list(s)), state\n",
    "            )\n",
    "        if framework == \"torch\":\n",
    "            from ray.rllib.utils.torch_utils import convert_to_torch_tensor\n",
    "\n",
    "            inputs = convert_to_torch_tensor(inputs)\n",
    "        # w/ old specs: inputs = model.input_specs.fill(self.random_fill_input_value)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Bring model into a reproducible, comparable state (so we can compare\n",
    "        # computations across frameworks). Use only a value-sequence of len=1 here\n",
    "        # as it could possibly be that the layers are stored in different order\n",
    "        # across the different frameworks.\n",
    "        model._set_to_dummy_weights(value_sequence=(self.random_fill_input_value,))\n",
    "\n",
    "        # Perform another forward pass.\n",
    "        comparable_outputs = model(inputs)\n",
    "\n",
    "        # Store the number of parameters for this framework's net.\n",
    "        self.param_counts[framework] = model.get_num_parameters()\n",
    "        # Store the fixed-weights-net outputs for this framework's net.\n",
    "        if framework == \"torch\":\n",
    "            self.output_values[framework] = tree.map_structure(\n",
    "                lambda s: s.detach().numpy() if s is not None else None,\n",
    "                comparable_outputs,\n",
    "            )\n",
    "        else:\n",
    "            self.output_values[framework] = tree.map_structure(\n",
    "                lambda s: s.numpy() if s is not None else None, comparable_outputs\n",
    "            )\n",
    "        return outputs\n",
    "\n",
    "    def check(self):\n",
    "        \"\"\"Compares all added Models with each other and possibly raises errors.\"\"\"\n",
    "\n",
    "        main_key = next(iter(self.models.keys()))\n",
    "        # Compare number of trainable and non-trainable params between all\n",
    "        # frameworks.\n",
    "        for c in self.param_counts.values():\n",
    "            check(c, self.param_counts[main_key])\n",
    "\n",
    "        # Compare dummy outputs by exact values given that all nets received the\n",
    "        # same input and all nets have the same (dummy) weight values.\n",
    "        for v in self.output_values.values():\n",
    "            check(v, self.output_values[main_key], atol=0.0005)\n",
    "\n",
    "\n",
    "def _get_mean_action_from_algorithm(alg: \"Algorithm\", obs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns the mean action computed by the given algorithm.\n",
    "\n",
    "    Note: This makes calls to `Algorithm.compute_single_action`\n",
    "\n",
    "    Args:\n",
    "        alg: The constructed algorithm to run inference on.\n",
    "        obs: The observation to compute the action for.\n",
    "\n",
    "    Returns:\n",
    "        The mean action computed by the algorithm over 5000 samples.\n",
    "\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for _ in range(5000):\n",
    "        out.append(float(alg.compute_single_action(obs)))\n",
    "    return np.mean(out)\n",
    "\n",
    "\n",
    "def check_supported_spaces(\n",
    "    alg: str,\n",
    "    config: \"AlgorithmConfig\",\n",
    "    train: bool = True,\n",
    "    check_bounds: bool = False,\n",
    "    frameworks: Optional[Tuple[str]] = None,\n",
    "    use_gpu: bool = False,\n",
    "):\n",
    "    \"\"\"Checks whether the given algorithm supports different action and obs spaces.\n",
    "\n",
    "        Performs the checks by constructing an rllib algorithm from the config and\n",
    "        checking to see that the model inside the policy is the correct one given\n",
    "        the action and obs spaces. For example if the action space is discrete and\n",
    "        the obs space is an image, then the model should be a vision network with\n",
    "        a categorical action distribution.\n",
    "\n",
    "    Args:\n",
    "        alg: The name of the algorithm to test.\n",
    "        config: The config to use for the algorithm.\n",
    "        train: Whether to train the algorithm for a few iterations.\n",
    "        check_bounds: Whether to check the bounds of the action space.\n",
    "        frameworks: The frameworks to test the algorithm with.\n",
    "        use_gpu: Whether to check support for training on a gpu.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # Do these imports here because otherwise we have circular imports.\n",
    "    from ray.rllib.examples.envs.classes.random_env import RandomEnv\n",
    "    from ray.rllib.models.torch.complex_input_net import (\n",
    "        ComplexInputNetwork as TorchComplexNet,\n",
    "    )\n",
    "    from ray.rllib.models.torch.fcnet import FullyConnectedNetwork as TorchFCNet\n",
    "    from ray.rllib.models.torch.visionnet import VisionNetwork as TorchVisionNet\n",
    "\n",
    "    action_spaces_to_test = {\n",
    "        # Test discrete twice here until we support multi_binary action spaces\n",
    "        \"discrete\": Discrete(5),\n",
    "        \"continuous\": Box(-1.0, 1.0, (5,), dtype=np.float32),\n",
    "        \"int_actions\": Box(0, 3, (2, 3), dtype=np.int32),\n",
    "        \"multidiscrete\": MultiDiscrete([1, 2, 3, 4]),\n",
    "        \"tuple\": GymTuple(\n",
    "            [Discrete(2), Discrete(3), Box(-1.0, 1.0, (5,), dtype=np.float32)]\n",
    "        ),\n",
    "        \"dict\": GymDict(\n",
    "            {\n",
    "                \"action_choice\": Discrete(3),\n",
    "                \"parameters\": Box(-1.0, 1.0, (1,), dtype=np.float32),\n",
    "                \"yet_another_nested_dict\": GymDict(\n",
    "                    {\"a\": GymTuple([Discrete(2), Discrete(3)])}\n",
    "                ),\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    observation_spaces_to_test = {\n",
    "        \"multi_binary\": MultiBinary([3, 10, 10]),\n",
    "        \"discrete\": Discrete(5),\n",
    "        \"continuous\": Box(-1.0, 1.0, (5,), dtype=np.float32),\n",
    "        \"vector2d\": Box(-1.0, 1.0, (5, 5), dtype=np.float32),\n",
    "        \"image\": Box(-1.0, 1.0, (84, 84, 1), dtype=np.float32),\n",
    "        \"tuple\": GymTuple([Discrete(10), Box(-1.0, 1.0, (5,), dtype=np.float32)]),\n",
    "        \"dict\": GymDict(\n",
    "            {\n",
    "                \"task\": Discrete(10),\n",
    "                \"position\": Box(-1.0, 1.0, (5,), dtype=np.float32),\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    "\n",
    "    # The observation spaces that we test RLModules with\n",
    "    rlmodule_supported_observation_spaces = [\n",
    "        \"multi_binary\",\n",
    "        \"discrete\",\n",
    "        \"continuous\",\n",
    "        \"image\",\n",
    "        \"tuple\",\n",
    "        \"dict\",\n",
    "    ]\n",
    "\n",
    "    # The action spaces that we test RLModules with\n",
    "    rlmodule_supported_action_spaces = [\"discrete\", \"continuous\"]\n",
    "\n",
    "    default_observation_space = default_action_space = \"discrete\"\n",
    "\n",
    "    config[\"log_level\"] = \"ERROR\"\n",
    "    config[\"env\"] = RandomEnv\n",
    "\n",
    "    def _do_check(alg, config, a_name, o_name):\n",
    "        # We need to copy here so that this validation does not affect the actual\n",
    "        # validation method call further down the line.\n",
    "        config_copy = config.copy()\n",
    "        config_copy.validate()\n",
    "        # If RLModules are enabled, we need to skip a few tests for now:\n",
    "        if config_copy.enable_rl_module_and_learner:\n",
    "            # Skip PPO cases in which RLModules don't support the given spaces yet.\n",
    "            if o_name not in rlmodule_supported_observation_spaces:\n",
    "                logger.warning(\n",
    "                    \"Skipping PPO test with RLModules for obs space {}\".format(o_name)\n",
    "                )\n",
    "                return\n",
    "            if a_name not in rlmodule_supported_action_spaces:\n",
    "                logger.warning(\n",
    "                    \"Skipping PPO test with RLModules for action space {}\".format(\n",
    "                        a_name\n",
    "                    )\n",
    "                )\n",
    "                return\n",
    "\n",
    "        fw = config[\"framework\"]\n",
    "        action_space = action_spaces_to_test[a_name]\n",
    "        obs_space = observation_spaces_to_test[o_name]\n",
    "        print(\n",
    "            \"=== Testing {} (fw={}) action_space={} obs_space={} ===\".format(\n",
    "                alg, fw, action_space, obs_space\n",
    "            )\n",
    "        )\n",
    "        t0 = time.time()\n",
    "        config.update_from_dict(\n",
    "            dict(\n",
    "                env_config=dict(\n",
    "                    action_space=action_space,\n",
    "                    observation_space=obs_space,\n",
    "                    reward_space=Box(1.0, 1.0, shape=(), dtype=np.float32),\n",
    "                    p_terminated=1.0,\n",
    "                    check_action_bounds=check_bounds,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        stat = \"ok\"\n",
    "\n",
    "        try:\n",
    "            algo = config.build()\n",
    "        except ray.exceptions.RayActorError as e:\n",
    "            if len(e.args) >= 2 and isinstance(e.args[2], UnsupportedSpaceException):\n",
    "                stat = \"unsupported\"\n",
    "            elif isinstance(e.args[0].args[2], UnsupportedSpaceException):\n",
    "                stat = \"unsupported\"\n",
    "            else:\n",
    "                raise\n",
    "        except UnsupportedSpaceException:\n",
    "            stat = \"unsupported\"\n",
    "        else:\n",
    "            if alg not in [\"SAC\", \"PPO\"]:\n",
    "                # 2D (image) input: Expect VisionNet.\n",
    "                if o_name in [\"atari\", \"image\"]:\n",
    "                    assert isinstance(algo.get_policy().model, TorchVisionNet)\n",
    "                # 1D input: Expect FCNet.\n",
    "                elif o_name == \"continuous\":\n",
    "                    assert isinstance(algo.get_policy().model, TorchFCNet)\n",
    "                # Could be either one: ComplexNet (if disabled Preprocessor)\n",
    "                # or FCNet (w/ Preprocessor).\n",
    "                elif o_name == \"vector2d\":\n",
    "                    assert isinstance(\n",
    "                        algo.get_policy().model, (TorchComplexNet, TorchFCNet)\n",
    "                    )\n",
    "            if train:\n",
    "                algo.train()\n",
    "            algo.stop()\n",
    "        print(\"Test: {}, ran in {}s\".format(stat, time.time() - t0))\n",
    "\n",
    "    if not frameworks:\n",
    "        frameworks = (\"tf2\", \"tf\", \"torch\")\n",
    "\n",
    "    _do_check_remote = ray.remote(_do_check)\n",
    "    _do_check_remote = _do_check_remote.options(num_gpus=1 if use_gpu else 0)\n",
    "    # Test all action spaces first.\n",
    "    for a_name in action_spaces_to_test.keys():\n",
    "        o_name = default_observation_space\n",
    "        ray.get(_do_check_remote.remote(alg, config, a_name, o_name))\n",
    "\n",
    "    # Now test all observation spaces.\n",
    "    for o_name in observation_spaces_to_test.keys():\n",
    "        a_name = default_action_space\n",
    "        ray.get(_do_check_remote.remote(alg, config, a_name, o_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c85f45cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "# from ray.rllib.utils.test_utils import (\n",
    "#     add_rllib_example_script_args,\n",
    "#     run_rllib_example_script_experiment,\n",
    "# )\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_iters=200,\n",
    "    default_timesteps=1000000,\n",
    "    default_reward=0.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f301b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--algo ALGO] [--enable-new-api-stack]\n",
      "                             [--framework {tf,tf2,torch}] [--env ENV]\n",
      "                             [--num-env-runners NUM_ENV_RUNNERS]\n",
      "                             [--num-envs-per-env-runner NUM_ENVS_PER_ENV_RUNNER]\n",
      "                             [--num-agents NUM_AGENTS]\n",
      "                             [--evaluation-num-env-runners EVALUATION_NUM_ENV_RUNNERS]\n",
      "                             [--evaluation-interval EVALUATION_INTERVAL]\n",
      "                             [--evaluation-duration EVALUATION_DURATION]\n",
      "                             [--evaluation-duration-unit {episodes,timesteps}]\n",
      "                             [--evaluation-parallel-to-training]\n",
      "                             [--output OUTPUT]\n",
      "                             [--log-level {INFO,DEBUG,WARN,ERROR}] [--no-tune]\n",
      "                             [--num-samples NUM_SAMPLES]\n",
      "                             [--max-concurrent-trials MAX_CONCURRENT_TRIALS]\n",
      "                             [--verbose VERBOSE]\n",
      "                             [--checkpoint-freq CHECKPOINT_FREQ]\n",
      "                             [--checkpoint-at-end] [--wandb-key WANDB_KEY]\n",
      "                             [--wandb-project WANDB_PROJECT]\n",
      "                             [--wandb-run-name WANDB_RUN_NAME]\n",
      "                             [--stop-reward STOP_REWARD]\n",
      "                             [--stop-iters STOP_ITERS]\n",
      "                             [--stop-timesteps STOP_TIMESTEPS] [--as-test]\n",
      "                             [--as-release-test] [--num-learners NUM_LEARNERS]\n",
      "                             [--num-cpus-per-learner NUM_CPUS_PER_LEARNER]\n",
      "                             [--num-gpus-per-learner NUM_GPUS_PER_LEARNER]\n",
      "                             [--num-aggregator-actors-per-learner NUM_AGGREGATOR_ACTORS_PER_LEARNER]\n",
      "                             [--num-cpus NUM_CPUS] [--local-mode]\n",
      "                             [--num-gpus NUM_GPUS]\n",
      "ipykernel_launcher.py: error: argument --framework: invalid choice: '/run/user/1000/jupyter/runtime/kernel-v3d5e014defd189b14e90564da406d18266f0fa1af.json' (choose from 'tf', 'tf2', 'torch')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "assert args.num_agents > 0, \"Must set --num-agents > 0 when running this script!\"\n",
    "\n",
    "# Here, we use the \"Agent Environment Cycle\" (AEC) PettingZoo environment type.\n",
    "# For a \"Parallel\" environment example, see the rock paper scissors examples\n",
    "# in this same repository folder.\n",
    "register_env(\"env\", lambda _: PettingZooEnv(waterworld_v4.env()))\n",
    "\n",
    "# Policies are called just like the agents (exact 1:1 mapping).\n",
    "policies = {f\"pursuer_{i}\" for i in range(args.num_agents)}\n",
    "\n",
    "base_config = (\n",
    "    get_trainable_cls(args.algo)\n",
    "    .get_default_config()\n",
    "    .environment(\"env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        # Exact 1:1 mapping from AgentID to ModuleID.\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .training(\n",
    "        vf_loss_coeff=0.005,\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={p: RLModuleSpec() for p in policies},\n",
    "        ),\n",
    "        model_config=DefaultModelConfig(vf_share_layers=True),\n",
    "    )\n",
    ")\n",
    "\n",
    "run_rllib_example_script_experiment(base_config, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e49a0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tianshou.algorithm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtianshou\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PPO, Algorithm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tianshou.algorithm'"
     ]
    }
   ],
   "source": [
    "from tianshou.algorithm import PPO, Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15152855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Dict('action_mask': Box(0, 1, (9,), int8), 'observation': Box(0, 1, (3, 3, 2), int8))\n",
      "Action Space: Discrete(9)\n",
      "Example State: ({'agent_id': 'player_1', 'obs': array([[[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]], dtype=int8), 'mask': [True, True, True, True, True, True, True, True, True]}, {})\n",
      "Agent: player_1\n",
      "Observation Space: Dict('action_mask': Box(0, 1, (9,), int8), 'observation': Box(0, 1, (3, 3, 2), int8))\n",
      "Action Space: Discrete(9)\n",
      "Agent: player_2\n",
      "Observation Space: Dict('action_mask': Box(0, 1, (9,), int8), 'observation': Box(0, 1, (3, 3, 2), int8))\n",
      "Action Space: Discrete(9)\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from tianshou.env import PettingZooEnv\n",
    "\n",
    "# 环境设置\n",
    "env = PettingZooEnv(tictactoe_v3.env(render_mode=\"human\"))\n",
    "\n",
    "# 获取观察空间和动作空间\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "# 输出环境信息\n",
    "print(\"Observation Space:\", observation_space)\n",
    "print(\"Action Space:\", action_space)\n",
    "\n",
    "# 获取一个示例状态\n",
    "state = env.reset()  # 环境的初始状态\n",
    "print(\"Example State:\", state)\n",
    "\n",
    "# 如果环境中包含多个智能体，我们还可以查看每个智能体的观察空间和动作空间\n",
    "for agent in env.agents:\n",
    "    print(f\"Agent: {agent}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Action Space: {env.action_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "930fe90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['']  # 禁止 Jupyter 自动解析命令行参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9c6f47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space: Dict('action_mask': Box(0, 1, (9,), int8), 'observation': Box(0, 1, (3, 3, 2), int8))\n",
      "Action Space: Discrete(9)\n",
      "Example State: ({'agent_id': 'player_1', 'obs': array([[[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]],\n",
      "\n",
      "       [[0, 0],\n",
      "        [0, 0],\n",
      "        [0, 0]]], dtype=int8), 'mask': [True, True, True, True, True, True, True, True, True]}, {})\n",
      "Agent: player_1\n",
      "Observation Space: Dict('action_mask': Box(0, 1, (9,), int8), 'observation': Box(0, 1, (3, 3, 2), int8))\n",
      "Action Space: Discrete(9)\n",
      "Agent: player_2\n",
      "Observation Space: Dict('action_mask': Box(0, 1, (9,), int8), 'observation': Box(0, 1, (3, 3, 2), int8))\n",
      "Action Space: Discrete(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gymnasium as gym\n",
    "from pettingzoo.classic import tictactoe_v3\n",
    "from tianshou.env import PettingZooEnv, DummyVectorEnv\n",
    "from tianshou.policy import PPOPolicy\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils.net.common import Net\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 环境设置\n",
    "env = PettingZooEnv(tictactoe_v3.env(render_mode=\"human\"))\n",
    "# 获取观察空间和动作空间\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "# 输出环境信息\n",
    "print(\"Observation Space:\", observation_space)\n",
    "print(\"Action Space:\", action_space)\n",
    "\n",
    "# 获取一个示例状态\n",
    "state = env.reset()  # 环境的初始状态\n",
    "print(\"Example State:\", state)\n",
    "\n",
    "# 如果环境中包含多个智能体，我们还可以查看每个智能体的观察空间和动作空间\n",
    "for agent in env.agents:\n",
    "    print(f\"Agent: {agent}\")\n",
    "    print(f\"Observation Space: {env.observation_space}\")\n",
    "    print(f\"Action Space: {env.action_space}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aac8431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/tianshou/trainer/base.py:160: DeprecationWarning: save_fn in trainer is marked as deprecated and will be removed in the future. Please use save_best_fn instead.\n",
      "  deprecation(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Linear.forward() got an unexpected keyword argument 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m     policy\u001b[38;5;241m.\u001b[39mset_eps(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[43moffpolicy_trainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_best_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 添加这个参数\u001b[39;49;00m\n\u001b[1;32m    108\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/tianshou/trainer/offpolicy.py:133\u001b[0m, in \u001b[0;36moffpolicy_trainer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moffpolicy_trainer\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for OffPolicyTrainer run method.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    It is identical to ``OffpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOffpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/tianshou/trainer/base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     info \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward_std\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/tianshou/trainer/base.py:252\u001b[0m, in \u001b[0;36mBaseTrainer.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/tianshou/trainer/base.py:237\u001b[0m, in \u001b[0;36mBaseTrainer.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector, AsyncCollector)  \u001b[38;5;66;03m# Issue 700\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector\u001b[38;5;241m.\u001b[39mreset_stat()\n\u001b[0;32m--> 237\u001b[0m test_result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_episode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepisode_per_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_metric\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward_std \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    243\u001b[0m     test_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrew\u001b[39m\u001b[38;5;124m\"\u001b[39m], test_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrew_std\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/tianshou/trainer/utils.py:27\u001b[0m, in \u001b[0;36mtest_episode\u001b[0;34m(policy, collector, test_fn, epoch, n_episode, logger, global_step, reward_metric)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m test_fn:\n\u001b[1;32m     26\u001b[0m     test_fn(epoch, global_step)\n\u001b[0;32m---> 27\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward_metric:\n\u001b[1;32m     29\u001b[0m     rew \u001b[38;5;241m=\u001b[39m reward_metric(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrews\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/tianshou/data/collector.py:279\u001b[0m, in \u001b[0;36mCollector.collect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m no_grad:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# faster than retain_grad version\u001b[39;00m\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# self.data.obs will be used by agent to get result\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, last_state)\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/tianshou/policy/modelfree/pg.py:120\u001b[0m, in \u001b[0;36mPGPolicy.forward\u001b[0;34m(self, batch, state, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    102\u001b[0m     batch: Batch,\n\u001b[1;32m    103\u001b[0m     state: Optional[Union[\u001b[38;5;28mdict\u001b[39m, Batch, np\u001b[38;5;241m.\u001b[39mndarray]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Batch:\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute action over the given batch data.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    :return: A :class:`~tianshou.data.Batch` which has 4 keys:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;03m        more detailed explanation.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     logits, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(logits, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    122\u001b[0m         dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_fn(\u001b[38;5;241m*\u001b[39mlogits)\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: Linear.forward() got an unexpected keyword argument 'state'"
     ]
    }
   ],
   "source": [
    "# 环境设置\n",
    "env = PettingZooEnv(tictactoe_v3.env(render_mode=\"human\"))\n",
    "train_envs = DummyVectorEnv([lambda: env for _ in range(10)])\n",
    "test_envs = DummyVectorEnv([lambda: env for _ in range(10)])\n",
    "\n",
    "# 策略网络定义\n",
    "observation_space = env.observation_space[\"observation\"]\n",
    "action_space = env.action_space\n",
    "\n",
    "# 修改 ActorCriticNet 网络的初始化\n",
    "class ActorCriticNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCriticNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        \n",
    "        # Actor部分：用于生成动作分布\n",
    "        self.actor_fc = torch.nn.Linear(128, action_dim)\n",
    "        \n",
    "        # Critic部分：用于计算状态的价值\n",
    "        self.critic_fc = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # 提取状态\n",
    "        x = batch.obs['observation']  # 使用 'observation' 作为输入\n",
    "        \n",
    "        # 对输入状态进行前向传递\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        \n",
    "        # Actor输出动作分布\n",
    "        action_probs = torch.softmax(self.actor_fc(x), dim=-1)\n",
    "        \n",
    "        # Critic输出状态的价值\n",
    "        value = self.critic_fc(x)\n",
    "        \n",
    "        return action_probs, value\n",
    "\n",
    "    def get_action(self, batch, action_mask):\n",
    "        action_probs, _ = self(batch)\n",
    "        \n",
    "        # 使用 action_mask 过滤掉不合法的动作\n",
    "        action_probs = action_probs * action_mask\n",
    "        action_probs = action_probs / action_probs.sum()  # 确保概率和为1\n",
    "        \n",
    "        # 选择动作\n",
    "        action = torch.multinomial(action_probs, 1)\n",
    "        return action\n",
    "\n",
    "# 修改初始化网络部分\n",
    "actor_critic_net = ActorCriticNet(observation_space.shape[0], action_space.n)\n",
    "\n",
    "# PPO策略设置\n",
    "ppo = PPOPolicy(\n",
    "    actor=actor_critic_net.actor_fc,  # actor部分\n",
    "    critic=actor_critic_net.critic_fc,  # critic部分\n",
    "    optim=torch.optim.Adam(actor_critic_net.parameters(), lr=1e-4),\n",
    "    dist_fn=torch.distributions.Categorical,\n",
    "    # gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    vf_coef=0.5,\n",
    "    ent_coef=0.01,\n",
    "    max_grad_norm=0.5,\n",
    "    eps_clip=0.2,\n",
    "    reward_normalization=True,\n",
    "    action_space=action_space\n",
    ")\n",
    "\n",
    "# 收集器设置\n",
    "train_collector = Collector(\n",
    "    ppo,\n",
    "    train_envs,\n",
    "    VectorReplayBuffer(20000, len(train_envs)),\n",
    "    exploration_noise=True\n",
    ")\n",
    "test_collector = Collector(ppo, test_envs)\n",
    "\n",
    "# 日志设置\n",
    "log_dir = \"./log\"\n",
    "writer = SummaryWriter(log_dir)\n",
    "logger = TensorboardLogger(writer)\n",
    "\n",
    "# 训练回调函数\n",
    "def save_best_fn(policy):\n",
    "    torch.save(policy.state_dict(), \"best_policy.pth\")\n",
    "\n",
    "def stop_fn(mean_rewards):\n",
    "    return mean_rewards >= 0.6\n",
    "\n",
    "def train_fn(epoch, env_step):\n",
    "    policy.set_eps(0.1)\n",
    "\n",
    "# 训练\n",
    "offpolicy_trainer(\n",
    "    ppo,\n",
    "    train_collector,\n",
    "    test_collector,\n",
    "    max_epoch=50,\n",
    "    step_per_epoch=1000,\n",
    "    step_per_collect=10,\n",
    "    update_per_step=0.1,\n",
    "    batch_size=64,\n",
    "    save_fn=save_best_fn,\n",
    "    stop_fn=stop_fn,\n",
    "    logger=logger,\n",
    "    train_fn=train_fn,\n",
    "    episode_per_test=10  # 添加这个参数\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
