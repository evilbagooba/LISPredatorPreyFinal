{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7fdac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:09:21,665\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-06-24 15:09:21,884\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ray.tune.result import TRAINING_ITERATION\n",
    "from ray.rllib.algorithms.callbacks import DefaultCallbacks\n",
    "from ray.rllib.core import (\n",
    "    COMPONENT_LEARNER,\n",
    "    COMPONENT_LEARNER_GROUP,\n",
    "    COMPONENT_RL_MODULE,\n",
    ")\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "from ray.rllib.examples.envs.classes.multi_agent import MultiAgentPendulum\n",
    "from ray.rllib.utils.metrics import (\n",
    "    ENV_RUNNER_RESULTS,\n",
    "    EPISODE_RETURN_MEAN,\n",
    "    NUM_ENV_STEPS_SAMPLED_LIFETIME,\n",
    ")\n",
    "from ray.rllib.utils.numpy import convert_to_numpy\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    check,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc62ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df1b97d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数设置完成\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = [\n",
    "    'notebook_script.py',\n",
    "    '--enable-new-api-stack',\n",
    "    '--num-agents=2',\n",
    "    # '--no-tune',\n",
    "    '--checkpoint-at-end',\n",
    "    '--stop-reward=200.0',\n",
    "\n",
    "]\n",
    "print(\"参数设置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad4773d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数解析完成: num_agents=2, algo=PPO\n"
     ]
    }
   ],
   "source": [
    "parser = add_rllib_example_script_args(\n",
    "    default_iters=2,\n",
    "    default_timesteps=10000,\n",
    "    default_reward=0.0,\n",
    ")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "assert args.num_agents > 0, \"Must set --num-agents > 0 when running this script!\"\n",
    "assert (\n",
    "    args.enable_new_api_stack\n",
    "), \"Must set --enable-new-api-stack when running this script!\"\n",
    "\n",
    "print(f\"参数解析完成: num_agents={args.num_agents}, algo={args.algo}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2fbdc5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:17:42,603\tINFO worker.py:1917 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-24 15:17:43 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-06-24_15-17-42_041583_547330/artifacts/2025-06-24_15-17-43/PPO_2025-06-24_15-17-43/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=555405)\u001b[0m 2025-06-24 15:17:44,553\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(MultiAgentEnvRunner pid=555491)\u001b[0m 2025-06-24 15:17:46,291\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=555405)\u001b[0m Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-24 15:17:48 (running for 00:00:05.21)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-06-24_15-17-42_041583_547330/artifacts/2025-06-24_15-17-43/PPO_2025-06-24_15-17-43/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>env_runner_group                               </th><th>env_runners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </th><th>fault_tolerance                                            </th><th>learners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       </th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_training_step_calls_per_iteration</th><th>perf                                                                                     </th><th>timers                                                                                                                                                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_f0114_00000</td><td>{&#x27;actor_manager_num_outstanding_async_reqs&#x27;: 0}</td><td>{&#x27;connector_pipeline_timer&#x27;: np.float64(0.0003068314981646836), &#x27;episode_return_min&#x27;: -313.726871349234, &#x27;env_reset_timer&#x27;: np.float64(0.0016084969975054264), &#x27;rlmodule_inference_timer&#x27;: np.float64(9.399915656965589e-05), &#x27;env_step_timer&#x27;: np.float64(0.0003813357210763395), &#x27;episode_return_mean&#x27;: -261.8934658410173, &#x27;env_to_module_connector&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;add_states_from_episodes_to_batch&#x27;: np.float64(3.952719020689228e-06), &#x27;batch_individual_items&#x27;: np.float64(1.2691392393109051e-05), &#x27;add_observations_from_episodes_to_batch&#x27;: np.float64(1.6158807069575928e-05), &#x27;numpy_to_tensor&#x27;: np.float64(2.2054553432598536e-05), &#x27;add_time_dim_to_batch_and_zero_pad&#x27;: np.float64(6.8665170436234445e-06), &#x27;agent_to_module_mapping&#x27;: np.float64(3.373805059319909e-06)}}, &#x27;connector_pipeline_timer&#x27;: np.float64(0.00010726873230809766)}, &#x27;module_to_env_connector&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;get_actions&#x27;: np.float64(8.311198329903125e-05), &#x27;normalize_and_clip_actions&#x27;: np.float64(3.5616559864388356e-05), &#x27;tensor_to_numpy&#x27;: np.float64(3.237276597851163e-05), &#x27;remove_single_ts_time_rank_from_batch&#x27;: np.float64(1.09656278159291e-06), &#x27;listify_data_for_vector_env&#x27;: np.float64(6.0032374115883e-06), &#x27;un_batch_to_individual_items&#x27;: np.float64(1.3689150840869613e-05), &#x27;module_to_agent_unmapping&#x27;: np.float64(2.748299035598287e-06)}}, &#x27;connector_pipeline_timer&#x27;: np.float64(0.00022956203345865544)}, &#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;add_time_dim_to_batch_and_zero_pad&#x27;: np.float64(1.4111501513980329e-05), &#x27;agent_to_module_mapping&#x27;: np.float64(5.716006853617728e-06), &#x27;add_states_from_episodes_to_batch&#x27;: np.float64(5.333502485882491e-06), &#x27;numpy_to_tensor&#x27;: np.float64(4.365100176073611e-05), &#x27;batch_individual_items&#x27;: np.float64(2.2173000616021454e-05), &#x27;add_observations_from_episodes_to_batch&#x27;: np.float64(2.950499765574932e-05)}}, &#x27;episode_len_mean&#x27;: 1000.0, &#x27;sample&#x27;: np.float64(1.974725075719689), &#x27;num_agent_steps_sampled&#x27;: {&#x27;pursuer_0&#x27;: 2000.0, &#x27;pursuer_1&#x27;: 2004.0}, &#x27;num_episodes&#x27;: 4.0, &#x27;episode_return_max&#x27;: -173.29817059173007, &#x27;num_episodes_lifetime&#x27;: 8.0, &#x27;env_to_module_sum_episodes_length_in&#x27;: np.float64(891.0423566894266), &#x27;num_module_steps_sampled_lifetime&#x27;: {&#x27;pursuer_0&#x27;: 4000.0, &#x27;pursuer_1&#x27;: 4008.0}, &#x27;num_env_steps_sampled_lifetime&#x27;: 8000.0, &#x27;module_episode_returns_mean&#x27;: {&#x27;pursuer_0&#x27;: -85.1942479220129, &#x27;pursuer_1&#x27;: -176.6992179190044}, &#x27;env_to_module_sum_episodes_length_out&#x27;: np.float64(891.0423566894266), &#x27;num_agent_steps_sampled_lifetime&#x27;: {&#x27;pursuer_0&#x27;: 4000.0, &#x27;pursuer_1&#x27;: 4008.0}, &#x27;episode_len_max&#x27;: 1000, &#x27;weights_seq_no&#x27;: 1.0, &#x27;episode_len_min&#x27;: 1000, &#x27;num_module_steps_sampled&#x27;: {&#x27;pursuer_0&#x27;: 2000.0, &#x27;pursuer_1&#x27;: 2004.0}, &#x27;episode_duration_sec_mean&#x27;: 0.975714623253225, &#x27;agent_episode_returns_mean&#x27;: {&#x27;pursuer_0&#x27;: -85.1942479220129, &#x27;pursuer_1&#x27;: -176.6992179190044}, &#x27;agent_steps&#x27;: {&#x27;pursuer_0&#x27;: 500.0, &#x27;pursuer_1&#x27;: 500.0}, &#x27;num_env_steps_sampled&#x27;: 4000.0, &#x27;time_between_sampling&#x27;: np.float64(2.769492552499287), &#x27;num_env_steps_sampled_lifetime_throughput&#x27;: np.float64(1117.7133516949234)}</td><td>{&#x27;num_healthy_workers&#x27;: 2, &#x27;num_remote_worker_restarts&#x27;: 0}</td><td>{&#x27;pursuer_1&#x27;: {&#x27;num_trainable_parameters&#x27;: 129285, &#x27;curr_kl_coeff&#x27;: 0.45000001788139343, &#x27;vf_explained_var&#x27;: np.float32(0.004059255), &#x27;mean_kl_loss&#x27;: np.float32(0.026672682), &#x27;vf_loss&#x27;: np.float32(9.922472), &#x27;policy_loss&#x27;: np.float32(-0.0292667), &#x27;weights_seq_no&#x27;: 2.0, &#x27;default_optimizer_learning_rate&#x27;: 5e-05, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;entropy&#x27;: np.float32(2.9062617), &#x27;vf_loss_unclipped&#x27;: np.float32(917.9136), &#x27;num_module_steps_trained&#x27;: 60160, &#x27;total_loss&#x27;: np.float32(0.02834747), &#x27;gradients_default_optimizer_global_norm&#x27;: np.float32(1.0176111), &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: np.float32(1.0), &#x27;module_train_batch_size_mean&#x27;: 128.0, &#x27;num_module_steps_trained_lifetime&#x27;: 120320, &#x27;num_module_steps_trained_lifetime_throughput&#x27;: 25579.022279586174}, &#x27;__all_modules__&#x27;: {&#x27;learner_connector&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;numpy_to_tensor&#x27;: 0.001024238253448857, &#x27;general_advantage_estimation&#x27;: 0.053273552757600554, &#x27;add_columns_from_episodes_to_train_batch&#x27;: 0.03427698813378811, &#x27;add_one_ts_to_episodes_and_truncate&#x27;: 0.0030727839632891116, &#x27;add_states_from_episodes_to_batch&#x27;: 5.546252650674433e-06, &#x27;add_time_dim_to_batch_and_zero_pad&#x27;: 1.616818684851751e-05, &#x27;agent_to_module_mapping&#x27;: 0.00172503845373285, &#x27;add_observations_from_episodes_to_batch&#x27;: 6.537346242112107e-05, &#x27;batch_individual_items&#x27;: 0.02083398467162624}}, &#x27;connector_pipeline_timer&#x27;: 0.11462194546664249}, &#x27;num_env_steps_trained_lifetime&#x27;: 3760000, &#x27;num_module_steps_trained&#x27;: 120320, &#x27;num_env_steps_trained&#x27;: 1880000, &#x27;num_module_steps_trained_lifetime&#x27;: 240640, &#x27;num_trainable_parameters&#x27;: 258570, &#x27;learner_connector_sum_episodes_length_in&#x27;: 4000.0, &#x27;learner_connector_sum_episodes_length_out&#x27;: 4000.0, &#x27;num_non_trainable_parameters&#x27;: 0, &#x27;num_env_steps_trained_lifetime_throughput&#x27;: 798948.7245117108, &#x27;num_module_steps_trained_throughput&#x27;: 1319983.4197107716, &#x27;num_module_steps_trained_lifetime_throughput&#x27;: 1358390.6097280646}, &#x27;pursuer_0&#x27;: {&#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: np.float32(1.0), &#x27;module_train_batch_size_mean&#x27;: 128.0, &#x27;num_module_steps_trained_lifetime&#x27;: 120320, &#x27;policy_loss&#x27;: np.float32(-0.0038190074), &#x27;num_trainable_parameters&#x27;: 129285, &#x27;curr_kl_coeff&#x27;: 0.10000000149011612, &#x27;vf_explained_var&#x27;: np.float32(0.0007920265), &#x27;mean_kl_loss&#x27;: np.float32(0.009720638), &#x27;vf_loss&#x27;: np.float32(8.967316), &#x27;weights_seq_no&#x27;: 2.0, &#x27;default_optimizer_learning_rate&#x27;: 5e-05, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;entropy&#x27;: np.float32(2.852007), &#x27;vf_loss_unclipped&#x27;: np.float32(166.34991), &#x27;num_module_steps_trained&#x27;: 60160, &#x27;total_loss&#x27;: np.float32(0.04198965), &#x27;gradients_default_optimizer_global_norm&#x27;: np.float32(1.2207496), &#x27;num_module_steps_trained_lifetime_throughput&#x27;: 25595.89892801944}}</td><td style=\"text-align: right;\">                            8000</td><td style=\"text-align: right;\">                                      1</td><td>{&#x27;cpu_util_percent&#x27;: np.float64(6.683333333333334), &#x27;ram_util_percent&#x27;: np.float64(15.6)}</td><td>{&#x27;training_iteration&#x27;: 4.722338342497969, &#x27;restore_env_runners&#x27;: 1.442615714040585e-05, &#x27;training_step&#x27;: 4.722201133700292, &#x27;env_runner_sampling_timer&#x27;: 1.9861834006509163, &#x27;learner_update_timer&#x27;: 2.733198854412622, &#x27;synch_weights&#x27;: 0.0025978927277901676, &#x27;synch_env_connectors&#x27;: 0.0013619780074805021}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-06-24 15:17:52,504 E 553928 553958] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-06-24_15-17-42_041583_547330 is over 95% full, available space: 5.49271 GB; capacity: 456.175 GB. Object creation will fail if spilling is required.\n",
      "\u001b[36m(PPO pid=555405)\u001b[0m 2025-06-24 15:17:46,362\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-24 15:17:53 (running for 00:00:10.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-06-24_15-17-42_041583_547330/artifacts/2025-06-24_15-17-43/PPO_2025-06-24_15-17-43/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=555405)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-06-24_15-17-43/PPO_env_f0114_00000_0_2025-06-24_15-17-43/checkpoint_000000)\n",
      "2025-06-24 15:17:56,182\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/qrbao/ray_results/PPO_2025-06-24_15-17-43' in 0.0131s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-24 15:17:56 (running for 00:00:13.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-06-24_15-17-42_041583_547330/artifacts/2025-06-24_15-17-43/PPO_2025-06-24_15-17-43/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "+---------------------+------------+---------------------+--------+------------------+------+-------------------+--------------------+--------------------+\n",
      "| Trial name          | status     | loc                 |   iter |   total time (s) |   ts |   combined return |   return pursuer_0 |   return pursuer_1 |\n",
      "|---------------------+------------+---------------------+--------+------------------+------+-------------------+--------------------+--------------------|\n",
      "| PPO_env_f0114_00000 | TERMINATED | 192.168.0.25:555405 |      2 |          9.11264 | 8000 |          -261.893 |           -85.1942 |           -176.699 |\n",
      "+---------------------+------------+---------------------+--------+------------------+------+-------------------+--------------------+--------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:17:56,862\tINFO tune.py:1041 -- Total run time: 13.86 seconds (13.16 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "环境注册和配置完成\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2025-06-24 15:18:02,511 E 553928 553958] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-06-24_15-17-42_041583_547330 is over 95% full, available space: 5.48897 GB; capacity: 456.175 GB. Object creation will fail if spilling is required.\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-24 15:18:12,521 E 553928 553958] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-06-24_15-17-42_041583_547330 is over 95% full, available space: 5.48897 GB; capacity: 456.175 GB. Object creation will fail if spilling is required.\n",
      "\u001b[36m(PPO pid=555728)\u001b[0m 2025-06-24 15:18:20,652\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(MultiAgentEnvRunner pid=555800)\u001b[0m 2025-06-24 15:18:22,379\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[33m(raylet)\u001b[0m [2025-06-24 15:18:22,528 E 553928 553958] (raylet) file_system_monitor.cc:116: /tmp/ray/session_2025-06-24_15-17-42_041583_547330 is over 95% full, available space: 5.48876 GB; capacity: 456.175 GB. Object creation will fail if spilling is required.\n",
      "\u001b[36m(PPO pid=555728)\u001b[0m Install gputil for GPU system monitoring.\n",
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=555728)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-06-24_15-18-19/PPO_env_059c4_00000_0_2025-06-24_15-18-19/checkpoint_000000)\n",
      "\u001b[36m(PPO pid=555728)\u001b[0m 2025-06-24 15:18:22,449\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: 注册环境和配置算法\n",
    "# Here, we use the \"Agent Environment Cycle\" (AEC) PettingZoo environment type.\n",
    "# For a \"Parallel\" environment example, see the rock paper scissors examples\n",
    "# in this same repository folder.\n",
    "# Here, we use the \"Agent Environment Cycle\" (AEC) PettingZoo environment type.\n",
    "# For a \"Parallel\" environment example, see the rock paper scissors examples\n",
    "# in this same repository folder.\n",
    "register_env(\"env\", lambda _: PettingZooEnv(waterworld_v4.env()))\n",
    "\n",
    "# Policies are called just like the agents (exact 1:1 mapping).\n",
    "policies = {f\"pursuer_{i}\" for i in range(args.num_agents)}\n",
    "\n",
    "base_config = (\n",
    "    get_trainable_cls(args.algo)\n",
    "    .get_default_config()\n",
    "    .environment(\"env\")\n",
    "    .multi_agent(\n",
    "        policies=policies,\n",
    "        # Exact 1:1 mapping from AgentID to ModuleID.\n",
    "        policy_mapping_fn=(lambda aid, *args, **kwargs: aid),\n",
    "    )\n",
    "    .training(\n",
    "        vf_loss_coeff=0.005,\n",
    "    )\n",
    "    .rl_module(\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={p: RLModuleSpec() for p in policies},\n",
    "        ),\n",
    "        model_config=DefaultModelConfig(vf_share_layers=True),\n",
    "    )\n",
    ")\n",
    "\n",
    "# run_rllib_example_script_experiment(base_config, args)\n",
    "results = run_rllib_example_script_experiment(base_config, args, keep_ray_up=True)\n",
    "print(\"环境注册和配置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab01ac35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab165c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51748bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1de195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77b4eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LoadP0OnAlgoInitCallback(DefaultCallbacks):\n",
    "    def on_algorithm_init(self, *, algorithm, **kwargs):\n",
    "        module_p0 = algorithm.get_module(\"pursuer_0\")\n",
    "        weight_before = convert_to_numpy(next(iter(module_p0.parameters())))\n",
    "        algorithm.restore_from_path(\n",
    "            p_0_module_state_path,\n",
    "            component=(\n",
    "                COMPONENT_LEARNER_GROUP\n",
    "                + \"/\"\n",
    "                + COMPONENT_LEARNER\n",
    "                + \"/\"\n",
    "                + COMPONENT_RL_MODULE\n",
    "                + \"/pursuer_0\"\n",
    "            ),\n",
    "        )\n",
    "        # Make sure weights were updated.\n",
    "        weight_after = convert_to_numpy(next(iter(module_p0.parameters())))\n",
    "        check(weight_before, weight_after, false=True)\n",
    "\n",
    "base_config.callbacks(LoadP0OnAlgoInitCallback)\n",
    "\n",
    "# Define stopping criteria.\n",
    "stop = {\n",
    "    f\"{ENV_RUNNER_RESULTS}/{EPISODE_RETURN_MEAN}\": -800.0,\n",
    "    f\"{ENV_RUNNER_RESULTS}/{NUM_ENV_STEPS_SAMPLED_LIFETIME}\": 100000,\n",
    "    TRAINING_ITERATION: 100,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8bad6a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始第二次训练（使用预训练权重）...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:10:13,376\tINFO worker.py:1917 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-24 15:10:13 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-06-24_15-10-12_812887_547330/artifacts/2025-06-24_15-10-13/PPO_2025-06-24_15-10-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=550911)\u001b[0m 2025-06-24 15:10:15,320\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "2025-06-24 15:10:15,418\tERROR tune_controller.py:1331 -- Trial task failed for trial PPO_env_e44ce_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/worker.py\", line 2849, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/worker.py\", line 939, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::PPO.__init__()\u001b[39m (pid=550911, ip=192.168.0.25, actor_id=60c5265840a514c2efab336801000000, repr=PPO(env=env; env-runners=0; learners=0; multi-agent=True))\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 687, in make\n",
      "    env_spec = _find_spec(id)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 531, in _find_spec\n",
      "    _check_version_exists(ns, name, version)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 397, in _check_version_exists\n",
      "    _check_name_exists(ns, name)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 374, in _check_name_exists\n",
      "    raise error.NameNotFound(\n",
      "gymnasium.error.NameNotFound: Environment `env` doesn't exist.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[36mray::PPO.__init__()\u001b[39m (pid=550911, ip=192.168.0.25, actor_id=60c5265840a514c2efab336801000000, repr=PPO(env=env; env-runners=0; learners=0; multi-agent=True))\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 536, in __init__\n",
      "    super().__init__(\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 157, in __init__\n",
      "    self.setup(copy.deepcopy(self.config))\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 644, in setup\n",
      "    self.env_runner_group = EnvRunnerGroup(\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 198, in __init__\n",
      "    self._setup(\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 292, in _setup\n",
      "    self._local_env_runner = self._make_worker(\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/env_runner_group.py\", line 1294, in _make_worker\n",
      "    return self.env_runner_cls(**kwargs)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 115, in __init__\n",
      "    self.make_env()\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/multi_agent_env_runner.py\", line 815, in make_env\n",
      "    self.env = make_vec(\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 69, in make_vec\n",
      "    env = SyncVectorMultiAgentEnv(\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 29, in __init__\n",
      "    self.envs = [env_fn() for env_fn in self.env_fns]\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/vector/sync_vector_multi_agent_env.py\", line 29, in <listcomp>\n",
      "    self.envs = [env_fn() for env_fn in self.env_fns]\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/vector/registration.py\", line 62, in create_single_env\n",
      "    single_env = gym.make(env_spec, **env_spec_kwargs.copy())\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/envs/registration.py\", line 740, in make\n",
      "    env = env_creator(**env_spec_kwargs)\n",
      "  File \"/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/env/utils/__init__.py\", line 122, in _gym_env_creator\n",
      "    raise EnvError(ERR_MSG_INVALID_ENV_DESCRIPTOR.format(env_descriptor))\n",
      "ray.rllib.utils.error.EnvError: The env string you provided ('env') is:\n",
      "a) Not a supported or -installed environment.\n",
      "b) Not a tune-registered environment creator.\n",
      "c) Not a valid env class string.\n",
      "\n",
      "Try one of the following:\n",
      "a) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\n",
      "   For PyBullet support: `pip install pybullet`.\n",
      "b) To register your custom env, do `from ray import tune;\n",
      "   tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\n",
      "   Then in your config, do `config.environment(env='[name]').\n",
      "c) Make sure you provide a fully qualified classpath, e.g.:\n",
      "   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_e44ce_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 15:10:15,422\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/qrbao/ray_results/PPO_2025-06-24_15-10-13' in 0.0020s.\n",
      "2025-06-24 15:10:15,423\tERROR tune.py:1037 -- Trials did not complete: [PPO_env_e44ce_00000]\n",
      "2025-06-24 15:10:15,423\tINFO tune.py:1041 -- Total run time: 1.66 seconds (1.65 seconds for the tuning loop).\n",
      "2025-06-24 15:10:15,424\tWARNING experiment_analysis.py:180 -- Failed to fetch metrics for 1 trial(s):\n",
      "- PPO_env_e44ce_00000: FileNotFoundError('Could not fetch metrics for PPO_env_e44ce_00000: both result.json and progress.csv were not found at /home/qrbao/ray_results/PPO_2025-06-24_15-10-13/PPO_env_e44ce_00000_0_2025-06-24_15-10-13')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-06-24 15:10:15 (running for 00:00:01.66)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/24 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-06-24_15-10-12_812887_547330/artifacts/2025-06-24_15-10-13/PPO_2025-06-24_15-10-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "+---------------------+----------+-------+\n",
      "| Trial name          | status   | loc   |\n",
      "|---------------------+----------+-------|\n",
      "| PPO_env_e44ce_00000 | ERROR    |       |\n",
      "+---------------------+----------+-------+\n",
      "Number of errored trials: 1\n",
      "+---------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name          |   # failures | error file                                                                                                                                                                    |\n",
      "|---------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| PPO_env_e44ce_00000 |            1 | /tmp/ray/session_2025-06-24_15-10-12_812887_547330/artifacts/2025-06-24_15-10-13/PPO_2025-06-24_15-10-13/driver_artifacts/PPO_env_e44ce_00000_0_2025-06-24_15-10-13/error.txt |\n",
      "+---------------------+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Running the example script resulted in one or more errors! [EnvError(\"The env string you provided ('env') is:\\na) Not a supported or -installed environment.\\nb) Not a tune-registered environment creator.\\nc) Not a valid env class string.\\n\\nTry one of the following:\\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\\n   For PyBullet support: `pip install pybullet`.\\nb) To register your custom env, do `from ray import tune;\\n   tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\\n   Then in your config, do `config.environment(env='[name]').\\nc) Make sure you provide a fully qualified classpath, e.g.:\\n   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\\n\")]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/qrbao/Documents/code4/rllib/mycode/running.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Documents/code4/rllib/mycode/running.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m开始第二次训练（使用预训练权重）...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Documents/code4/rllib/mycode/running.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Run the experiment again with the restored MultiRLModule.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Documents/code4/rllib/mycode/running.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m final_results \u001b[39m=\u001b[39m run_rllib_example_script_experiment(base_config, args, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bngrok-tunnel/home/qrbao/Documents/code4/rllib/mycode/running.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m训练完成！\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/utils/test_utils.py:1352\u001b[0m, in \u001b[0;36mrun_rllib_example_script_experiment\u001b[0;34m(base_config, args, stop, success_metric, trainable, tune_callbacks, keep_config, keep_ray_up, scheduler, progress_reporter)\u001b[0m\n\u001b[1;32m   1348\u001b[0m \u001b[39m# Error out, if Tuner.fit() failed to run. Otherwise, erroneous examples might pass\u001b[39;00m\n\u001b[1;32m   1349\u001b[0m \u001b[39m# the CI tests w/o us knowing that they are broken (b/c some examples do not have\u001b[39;00m\n\u001b[1;32m   1350\u001b[0m \u001b[39m# a --as-test flag and/or any passing criteris).\u001b[39;00m\n\u001b[1;32m   1351\u001b[0m \u001b[39mif\u001b[39;00m results\u001b[39m.\u001b[39merrors:\n\u001b[0;32m-> 1352\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1353\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRunning the example script resulted in one or more errors! \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1354\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m[e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39me\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39mresults\u001b[39m.\u001b[39merrors]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1355\u001b[0m     )\n\u001b[1;32m   1357\u001b[0m \u001b[39m# If run as a test, check whether we reached the specified success criteria.\u001b[39;00m\n\u001b[1;32m   1358\u001b[0m test_passed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Running the example script resulted in one or more errors! [EnvError(\"The env string you provided ('env') is:\\na) Not a supported or -installed environment.\\nb) Not a tune-registered environment creator.\\nc) Not a valid env class string.\\n\\nTry one of the following:\\na) For Atari support: `pip install gym[atari] autorom[accept-rom-license]`.\\n   For PyBullet support: `pip install pybullet`.\\nb) To register your custom env, do `from ray import tune;\\n   tune.register_env('[name]', lambda cfg: [return env obj from here using cfg])`.\\n   Then in your config, do `config.environment(env='[name]').\\nc) Make sure you provide a fully qualified classpath, e.g.:\\n   `ray.rllib.examples.envs.classes.repeat_after_me_env.RepeatAfterMeEnv`\\n\")]"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"开始第二次训练（使用预训练权重）...\")\n",
    "# Run the experiment again with the restored MultiRLModule.\n",
    "final_results = run_rllib_example_script_experiment(base_config, args, stop=stop)\n",
    "print(\"训练完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cb0cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 7: 显示结果\n",
    "print(\"=\" * 50)\n",
    "print(\"训练结果:\")\n",
    "print(f\"最佳checkpoint路径: {final_results.get_best_result().checkpoint.path}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697db4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    ort_session = None\n",
    "    print(\" ok\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
