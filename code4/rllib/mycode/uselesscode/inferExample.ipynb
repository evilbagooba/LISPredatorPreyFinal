{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e5d9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example showing how one can implement a simple self-play training workflow.\n",
    "\n",
    "Uses the open spiel adapter of RLlib with the \"connect_four\" game and\n",
    "a multi-agent setup with a \"main\" policy and n \"main_v[x]\" policies\n",
    "(x=version number), which are all at-some-point-frozen copies of\n",
    "\"main\". At the very beginning, \"main\" plays against RandomPolicy.\n",
    "\n",
    "Checks for the training progress after each training update via a custom\n",
    "callback. We simply measure the win rate of \"main\" vs the opponent\n",
    "(\"main_v[x]\" or RandomPolicy at the beginning) by looking through the\n",
    "achieved rewards in the episodes in the train batch. If this win rate\n",
    "reaches some configurable threshold, we add a new policy to\n",
    "the policy map (a frozen copy of the current \"main\" one) and change the\n",
    "policy_mapping_fn to make new matches of \"main\" vs any of the previous\n",
    "versions of \"main\" (including the just added one).\n",
    "\n",
    "After training for n iterations, a configurable number of episodes can\n",
    "be played by the user against the \"main\" agent on the command line.\n",
    "\"\"\"\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from ray.tune.result import TRAINING_ITERATION\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "from ray.rllib.env.utils import try_import_pyspiel, try_import_open_spiel\n",
    "from ray.rllib.env.wrappers.open_spiel import OpenSpielEnv\n",
    "from ray.rllib.examples.rl_modules.classes.random_rlm import RandomRLModule\n",
    "from ray.rllib.examples.multi_agent.utils import (\n",
    "    ask_user_for_action,\n",
    "    SelfPlayCallback,\n",
    "    SelfPlayCallbackOldAPIStack,\n",
    ")\n",
    "from ray.rllib.examples._old_api_stack.policy.random_policy import RandomPolicy\n",
    "from ray.rllib.policy.policy import PolicySpec\n",
    "from ray.rllib.utils.metrics import NUM_ENV_STEPS_SAMPLED_LIFETIME\n",
    "from ray.rllib.utils.test_utils import (\n",
    "    add_rllib_example_script_args,\n",
    "    run_rllib_example_script_experiment,\n",
    ")\n",
    "from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "open_spiel = try_import_open_spiel(error=True)\n",
    "pyspiel = try_import_pyspiel(error=True)\n",
    "\n",
    "# Import after try_import_open_spiel, so we can error out with hints.\n",
    "from open_spiel.python.rl_environment import Environment  # noqa: E402\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ebe2934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--from-checkpoint'], dest='from_checkpoint', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, required=False, help='Full path to a checkpoint file for restoring a previously saved Algorithm state.', metavar=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "parser = add_rllib_example_script_args(default_timesteps=2000000)\n",
    "parser.set_defaults(\n",
    "    env=\"connect_four\",\n",
    "    checkpoint_freq=1,\n",
    "    checkpoint_at_end=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--win-rate-threshold\",\n",
    "    type=float,\n",
    "    default=0.95,\n",
    "    help=\"Win-rate at which we setup another opponent by freezing the \"\n",
    "    \"current main policy and playing against a uniform distribution \"\n",
    "    \"of previously frozen 'main's from here on.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--min-league-size\",\n",
    "    type=float,\n",
    "    default=3,\n",
    "    help=\"Minimum number of policies/RLModules to consider the test passed. \"\n",
    "    \"The initial league size is 2: `main` and `random`. \"\n",
    "    \"`--min-league-size=3` thus means that one new policy/RLModule has been \"\n",
    "    \"added so far (b/c the `main` one has reached the `--win-rate-threshold \"\n",
    "    \"against the `random` Policy/RLModule).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num-episodes-human-play\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"How many episodes to play against the user on the command \"\n",
    "    \"line after training has finished.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--from-checkpoint\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    help=\"Full path to a checkpoint file for restoring a previously saved \"\n",
    "    \"Algorithm state.\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "887ab46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数设置完成\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = [\n",
    "    'notebook_script.py',\n",
    "\n",
    "\n",
    "]\n",
    "print(\"参数设置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eab917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67cbab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d80142a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "register_env(\"open_spiel_env\", lambda _: OpenSpielEnv(pyspiel.load_game(args.env)))\n",
    "\n",
    "def agent_to_module_mapping_fn(agent_id, episode, **kwargs):\n",
    "    # agent_id = [0|1] -> module depends on episode ID\n",
    "    # This way, we make sure that both modules sometimes play agent0\n",
    "    # (start player) and sometimes agent1 (player to move 2nd).\n",
    "    return \"main\" if hash(episode.id_) % 2 == agent_id else \"random\"\n",
    "\n",
    "def policy_mapping_fn(agent_id, episode, worker, **kwargs):\n",
    "    return \"main\" if episode.episode_id % 2 == agent_id else \"random\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da865f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a0802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = (\n",
    "    get_trainable_cls(args.algo)\n",
    "    .get_default_config()\n",
    "    .environment(\"open_spiel_env\")\n",
    "    # Set up the main piece in this experiment: The league-bases self-play\n",
    "    # callback, which controls adding new policies/Modules to the league and\n",
    "    # properly matching the different policies in the league with each other.\n",
    "    .callbacks(\n",
    "        functools.partial(\n",
    "            (\n",
    "                SelfPlayCallback\n",
    "                if args.enable_new_api_stack\n",
    "                else SelfPlayCallbackOldAPIStack\n",
    "            ),\n",
    "            win_rate_threshold=args.win_rate_threshold,\n",
    "        )\n",
    "    )\n",
    "    .env_runners(\n",
    "        num_env_runners=(args.num_env_runners or 2),\n",
    "        num_envs_per_env_runner=1 if args.enable_new_api_stack else 5,\n",
    "    )\n",
    "    .multi_agent(\n",
    "        # Initial policy map: Random and default algo one. This will be expanded\n",
    "        # to more policy snapshots taken from \"main\" against which \"main\"\n",
    "        # will then play (instead of \"random\"). This is done in the\n",
    "        # custom callback defined above (`SelfPlayCallback`).\n",
    "        policies=(\n",
    "            {\n",
    "                # Our main policy, we'd like to optimize.\n",
    "                \"main\": PolicySpec(),\n",
    "                # An initial random opponent to play against.\n",
    "                \"random\": PolicySpec(policy_class=RandomPolicy),\n",
    "            }\n",
    "            if not args.enable_new_api_stack\n",
    "            else {\"main\", \"random\"}\n",
    "        ),\n",
    "        # Assign agent 0 and 1 randomly to the \"main\" policy or\n",
    "        # to the opponent (\"random\" at first). Make sure (via episode_id)\n",
    "        # that \"main\" always plays against \"random\" (and not against\n",
    "        # another \"main\").\n",
    "        policy_mapping_fn=(\n",
    "            agent_to_module_mapping_fn\n",
    "            if args.enable_new_api_stack\n",
    "            else policy_mapping_fn\n",
    "        ),\n",
    "        # Always just train the \"main\" policy.\n",
    "        policies_to_train=[\"main\"],\n",
    "    )\n",
    "    .rl_module(\n",
    "        model_config=DefaultModelConfig(fcnet_hiddens=[512, 512]),\n",
    "        rl_module_spec=MultiRLModuleSpec(\n",
    "            rl_module_specs={\n",
    "                \"main\": RLModuleSpec(),\n",
    "                \"random\": RLModuleSpec(module_class=RandomRLModule),\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Only for PPO, change the `num_epochs` setting.\n",
    "if args.algo == \"PPO\":\n",
    "    config.training(num_epochs=20)\n",
    "\n",
    "stop = {\n",
    "    NUM_ENV_STEPS_SAMPLED_LIFETIME: args.stop_timesteps,\n",
    "    TRAINING_ITERATION: args.stop_iters,\n",
    "    \"league_size\": args.min_league_size,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b93c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 23:01:16,887\tINFO worker.py:1747 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-06-24 23:02:27</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:11.00        </td></tr>\n",
       "<tr><td>Memory:      </td><td>23.8/125.5 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 3.0/24 CPUs, 0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  actor_manager_num_ou\n",
       "tstanding_async_reqs</th><th style=\"text-align: right;\">  num_remote_worker_re\n",
       "starts</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_open_spiel_env_b2703_00000</td><td>TERMINATED</td><td>192.168.0.25:667699</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         64.6118</td><td style=\"text-align: right;\">140000</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">0</td><td style=\"text-align: right;\">0</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 23:01:16,907\tWARNING algorithm_config.py:4984 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2025-06-24 23:01:16,911\tWARNING algorithm_config.py:4984 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  actor_manager_num_outstanding_async_reqs</th><th style=\"text-align: right;\">  agent_timesteps_total</th><th>counters                                                                                                                                </th><th>custom_metrics  </th><th>env_runners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </th><th>episode_media  </th><th>info                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              </th><th style=\"text-align: right;\">  league_size</th><th style=\"text-align: right;\">  num_agent_steps_sampled</th><th style=\"text-align: right;\">  num_agent_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_agent_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_sampled</th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_env_steps_sampled_this_iter</th><th style=\"text-align: right;\">  num_env_steps_sampled_throughput_per_sec</th><th style=\"text-align: right;\">  num_env_steps_trained</th><th style=\"text-align: right;\">  num_env_steps_trained_this_iter</th><th style=\"text-align: right;\">  num_env_steps_trained_throughput_per_sec</th><th style=\"text-align: right;\">  num_healthy_workers</th><th style=\"text-align: right;\">  num_remote_worker_restarts</th><th style=\"text-align: right;\">  num_steps_trained_this_iter</th><th>perf                                                                        </th><th>timers                                                                                                                                                                                                                          </th><th style=\"text-align: right;\">  win_rate</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_open_spiel_env_b2703_00000</td><td style=\"text-align: right;\">                                         0</td><td style=\"text-align: right;\">                 139992</td><td>{&#x27;num_env_steps_sampled&#x27;: 140000, &#x27;num_env_steps_trained&#x27;: 140000, &#x27;num_agent_steps_sampled&#x27;: 139992, &#x27;num_agent_steps_trained&#x27;: 139992}</td><td>{}              </td><td>{&#x27;episode_reward_max&#x27;: 0.0, &#x27;episode_reward_min&#x27;: -0.8999999999999998, &#x27;episode_reward_mean&#x27;: np.float64(-0.011873350923482852), &#x27;episode_len_mean&#x27;: np.float64(10.62269129287599), &#x27;episode_media&#x27;: {}, &#x27;episodes_timesteps_total&#x27;: 4026, &#x27;policy_reward_min&#x27;: {&#x27;random&#x27;: np.float64(-1.4000000000000001), &#x27;main&#x27;: np.float64(-1.2000000000000002)}, &#x27;policy_reward_max&#x27;: {&#x27;random&#x27;: np.float64(1.0), &#x27;main&#x27;: np.float64(1.0)}, &#x27;policy_reward_mean&#x27;: {&#x27;random&#x27;: np.float64(-0.9097625329815304), &#x27;main&#x27;: np.float64(0.8978891820580474)}, &#x27;custom_metrics&#x27;: {}, &#x27;hist_stats&#x27;: {&#x27;episode_reward&#x27;: [0.0, -0.09999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.4999999999999999, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10000000000000009, 0.0, -0.8999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20000000000000007, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.30000000000000004, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09999999999999998, 0.0, 0.0, -0.10000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.8000000000000002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10000000000000009, -0.09999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.10000000000000009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.09999999999999998, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.20000000000000018], &#x27;episode_lengths&#x27;: [12, 25, 8, 8, 24, 21, 7, 8, 7, 9, 8, 13, 17, 8, 7, 22, 17, 8, 7, 7, 7, 22, 8, 8, 13, 7, 7, 8, 8, 13, 16, 9, 7, 8, 7, 9, 8, 20, 15, 8, 11, 20, 10, 20, 9, 9, 9, 8, 8, 7, 19, 17, 8, 8, 20, 8, 8, 8, 7, 7, 7, 15, 15, 8, 8, 10, 9, 7, 7, 7, 7, 7, 11, 7, 7, 9, 9, 22, 8, 7, 25, 12, 7, 7, 18, 7, 8, 18, 19, 7, 8, 7, 8, 22, 8, 24, 8, 7, 8, 12, 8, 21, 8, 18, 22, 8, 7, 19, 7, 7, 7, 8, 8, 7, 7, 18, 7, 8, 7, 19, 7, 8, 7, 8, 7, 8, 13, 7, 8, 8, 7, 10, 7, 7, 9, 9, 7, 16, 8, 14, 8, 8, 8, 19, 8, 7, 16, 7, 8, 7, 8, 20, 7, 7, 8, 7, 7, 8, 12, 7, 7, 17, 8, 7, 7, 8, 10, 21, 7, 15, 10, 7, 18, 12, 7, 13, 7, 8, 8, 8, 7, 8, 7, 12, 18, 14, 8, 8, 7, 9, 7, 10, 29, 10, 8, 8, 14, 14, 9, 8, 8, 10, 8, 8, 15, 9, 19, 8, 27, 7, 8, 8, 18, 10, 16, 15, 7, 16, 14, 18, 8, 19, 18, 8, 8, 7, 7, 25, 8, 10, 13, 7, 7, 7, 8, 12, 13, 15, 9, 15, 8, 17, 21, 34, 8, 8, 8, 8, 7, 7, 7, 7, 8, 21, 15, 9, 8, 8, 7, 8, 10, 8, 9, 7, 7, 8, 7, 10, 7, 7, 7, 17, 7, 8, 8, 9, 10, 7, 8, 7, 12, 8, 8, 15, 20, 10, 7, 8, 23, 9, 8, 14, 8, 8, 8, 8, 16, 15, 12, 7, 8, 7, 21, 7, 7, 8, 7, 8, 7, 11, 9, 26, 7, 9, 9, 20, 7, 7, 15, 14, 15, 7, 9, 14, 7, 8, 9, 15, 21, 26, 16, 18, 8, 20, 8, 12, 8, 9, 7, 8, 11, 8, 13, 7, 15, 8, 7, 8, 8, 8, 8, 8, 7, 7, 19, 7, 17, 10, 8, 7, 8, 8, 8, 7, 14, 7, 7, 24, 7, 8, 13, 8, 7, 8, 7, 7, 7, 8, 22], &#x27;policy_random_reward&#x27;: [-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.1, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.1, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.2, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.1, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.2000000000000002, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0.9, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.4000000000000001, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.1, -1.1, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.1, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.1, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0.9, -1.0, -1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.1]}, &#x27;sampler_perf&#x27;: {&#x27;mean_raw_obs_processing_ms&#x27;: np.float64(0.8892811469404559), &#x27;mean_inference_ms&#x27;: np.float64(0.5795703321644742), &#x27;mean_action_processing_ms&#x27;: np.float64(0.1543484845921387), &#x27;mean_env_wait_ms&#x27;: np.float64(0.13519458640054344), &#x27;mean_env_render_ms&#x27;: np.float64(0.0)}, &#x27;num_faulty_episodes&#x27;: 0, &#x27;connector_metrics&#x27;: {&#x27;ObsPreprocessorConnector_ms&#x27;: np.float64(0.0019858254606302307), &#x27;StateBufferConnector_ms&#x27;: np.float64(0.003366891815668675), &#x27;ViewRequirementAgentConnector_ms&#x27;: np.float64(0.06566296152200422)}, &#x27;num_episodes&#x27;: 379, &#x27;episode_return_max&#x27;: 0.0, &#x27;episode_return_min&#x27;: -0.8999999999999998, &#x27;episode_return_mean&#x27;: np.float64(-0.011873350923482852), &#x27;episodes_this_iter&#x27;: 379}</td><td>{}             </td><td>{&#x27;learner&#x27;: {&#x27;main&#x27;: {&#x27;learner_stats&#x27;: {&#x27;allreduce_latency&#x27;: np.float64(0.0), &#x27;grad_gnorm&#x27;: np.float32(0.9769495), &#x27;cur_kl_coeff&#x27;: np.float64(0.01875), &#x27;cur_lr&#x27;: np.float64(5e-05), &#x27;total_loss&#x27;: np.float64(0.2106280327652177), &#x27;policy_loss&#x27;: np.float64(-0.01755137841203524), &#x27;vf_loss&#x27;: np.float64(0.22808257865116877), &#x27;vf_explained_var&#x27;: np.float64(0.1395652818329194), &#x27;kl&#x27;: np.float64(0.005164267819673726), &#x27;entropy&#x27;: np.float64(0.45911177037393347), &#x27;entropy_coeff&#x27;: np.float64(0.0)}, &#x27;model&#x27;: {}, &#x27;custom_metrics&#x27;: {}, &#x27;num_agent_steps_trained&#x27;: np.float64(122.88235294117646), &#x27;num_grad_updates_lifetime&#x27;: np.float64(11450.5), &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: np.float64(169.5)}}, &#x27;num_env_steps_sampled&#x27;: 140000, &#x27;num_env_steps_trained&#x27;: 140000, &#x27;num_agent_steps_sampled&#x27;: 139992, &#x27;num_agent_steps_trained&#x27;: 139992}</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">                   139992</td><td style=\"text-align: right;\">                            139992</td><td style=\"text-align: right;\">                   139992</td><td style=\"text-align: right;\">                 140000</td><td style=\"text-align: right;\">                          140000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   2110.04</td><td style=\"text-align: right;\">                 140000</td><td style=\"text-align: right;\">                             4000</td><td style=\"text-align: right;\">                                   2110.04</td><td style=\"text-align: right;\">                    2</td><td style=\"text-align: right;\">                           0</td><td style=\"text-align: right;\">                         4000</td><td>{&#x27;cpu_util_percent&#x27;: np.float64(9.15), &#x27;ram_util_percent&#x27;: np.float64(19.0)}</td><td>{&#x27;training_iteration_time_ms&#x27;: 1876.217, &#x27;restore_workers_time_ms&#x27;: 0.013, &#x27;training_step_time_ms&#x27;: 1876.18, &#x27;sample_time_ms&#x27;: 734.809, &#x27;learn_time_ms&#x27;: 1138.578, &#x27;learn_throughput&#x27;: 3513.153, &#x27;synch_weights_time_ms&#x27;: 2.591}</td><td style=\"text-align: right;\">  0.952507</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 23:02:27,893\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/qrbao/ray_results/PPO_2025-06-24_23-01-16' in 0.0398s.\n",
      "2025-06-24 23:02:28,479\tINFO tune.py:1041 -- Total run time: 71.59 seconds (70.96 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the \"main\" policy to play really well using self-play.\n",
    "results = None\n",
    "if not args.from_checkpoint:\n",
    "    results = run_rllib_example_script_experiment(\n",
    "        config, args, stop=stop, keep_ray_up=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8b59d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5694e026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数设置完成\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.argv = [\n",
    "    'notebook_script.py',\n",
    "\n",
    "]\n",
    "print(\"参数设置完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f70a5c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-24 23:04:51,962\tWARNING algorithm_config.py:4984 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:520: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2025-06-24 23:04:53,932\tWARNING algorithm_config.py:4984 -- You have setup a RLModuleSpec (via calling `config.rl_module(...)`), but have not enabled the new API stack. To enable it, call `config.api_stack(enable_rl_module_and_learner=True)`.\n",
      "2025-06-24 23:04:53,935\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2025-06-24 23:04:53,992\tINFO trainable.py:577 -- Restored on 192.168.0.25 from checkpoint: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-06-24_23-01-16/PPO_open_spiel_env_b2703_00000_0_2025-06-24_23-01-16/checkpoint_000034)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You play as o\n",
      "\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      ".......\n",
      "...x...\n",
      "\n",
      "Choose an action from [0, 1, 2, 3, 4, 5, 6]:\n",
      "Choose an action from [0, 1, 2, 3, 4, 5, 6]:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m player_id \u001b[38;5;241m=\u001b[39m time_step\u001b[38;5;241m.\u001b[39mobservations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurrent_player\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m player_id \u001b[38;5;241m==\u001b[39m human_player:\n\u001b[0;32m---> 30\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mask_user_for_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(time_step\u001b[38;5;241m.\u001b[39mobservations[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_state\u001b[39m\u001b[38;5;124m\"\u001b[39m][player_id])\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/examples/multi_agent/utils/__init__.py:29\u001b[0m, in \u001b[0;36mask_user_for_action\u001b[0;34m(time_step)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChoose an action from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(legal_moves))\n\u001b[1;32m     28\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m---> 29\u001b[0m choice_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     choice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(choice_str)\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Restore trained Algorithm (set to non-explore behavior) and play against\n",
    "# # human on command line.\n",
    "# if args.num_episodes_human_play > 0:\n",
    "#     num_episodes = 0\n",
    "#     config.explore = False\n",
    "#     algo = config.build()\n",
    "#     if args.from_checkpoint:\n",
    "#         algo.restore(args.from_checkpoint)\n",
    "#     else:\n",
    "#         checkpoint = results.get_best_result().checkpoint\n",
    "#         if not checkpoint:\n",
    "#             raise ValueError(\"No last checkpoint found in results!\")\n",
    "#         algo.restore(checkpoint)\n",
    "\n",
    "#     if args.enable_new_api_stack:\n",
    "#         rl_module = algo.get_module(\"main\")\n",
    "\n",
    "#     # Play from the command line against the trained agent\n",
    "#     # in an actual (non-RLlib-wrapped) open-spiel env.\n",
    "#     human_player = 1\n",
    "#     env = Environment(args.env)\n",
    "\n",
    "#     while num_episodes < args.num_episodes_human_play:\n",
    "#         print(\"You play as {}\".format(\"o\" if human_player else \"x\"))\n",
    "#         time_step = env.reset()\n",
    "#         while not time_step.last():\n",
    "#             player_id = time_step.observations[\"current_player\"]\n",
    "#             if player_id == human_player:\n",
    "#                 action = ask_user_for_action(time_step)\n",
    "#             else:\n",
    "#                 obs = np.array(time_step.observations[\"info_state\"][player_id])\n",
    "#                 if args.enable_new_api_stack:\n",
    "#                     action = np.argmax(\n",
    "#                         rl_module.forward_inference(\n",
    "#                             {\"obs\": torch.from_numpy(obs).unsqueeze(0).float()}\n",
    "#                         )[\"action_dist_inputs\"][0].numpy()\n",
    "#                     )\n",
    "#                 else:\n",
    "#                     action = algo.compute_single_action(obs, policy_id=\"main\")\n",
    "#                 # In case computer chooses an invalid action, pick a\n",
    "#                 # random one.\n",
    "#                 legal = time_step.observations[\"legal_actions\"][player_id]\n",
    "#                 if action not in legal:\n",
    "#                     action = np.random.choice(legal)\n",
    "#             time_step = env.step([action])\n",
    "#             print(f\"\\n{env.get_state}\")\n",
    "\n",
    "#         print(f\"\\n{env.get_state}\")\n",
    "\n",
    "#         print(\"End of game!\")\n",
    "#         if time_step.rewards[human_player] > 0:\n",
    "#             print(\"You win\")\n",
    "#         elif time_step.rewards[human_player] < 0:\n",
    "#             print(\"You lose\")\n",
    "#         else:\n",
    "#             print(\"Draw\")\n",
    "#         # Switch order of players.\n",
    "#         human_player = 1 - human_player\n",
    "\n",
    "#         num_episodes += 1\n",
    "\n",
    "#     algo.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22680aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d29610",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = waterworld_v4.env(render_mode=\"human\",n_predators=2,n_preys=2,n_evaders=5,n_obstacles=1,n_poisons=1)\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    print({agent: reward})\n",
    "\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
