"""
åŠ è½½è®­ç»ƒå¥½çš„PettingZoo Waterworldæ¨¡å‹å¹¶è¿›è¡Œå¯è§†åŒ–è¯„ä¼°
"""

import os
import numpy as np
import matplotlib.pyplot as plt
from pettingzoo.sisl import waterworld_v4
from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv
from ray.rllib.algorithms.ppo import PPO
from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec
from ray.rllib.core.rl_module.rl_module import RLModuleSpec
from ray.tune.registry import register_env
import time


def load_and_evaluate_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶è¿›è¡Œè¯„ä¼°"""
    
    # 1. è®¾ç½®checkpointè·¯å¾„ï¼ˆæ ¹æ®æ‚¨çš„è®­ç»ƒæ—¥å¿—ä¿®æ”¹ï¼‰
    # ä»æ—¥å¿—ä¸­çœ‹åˆ°ç»“æœä¿å­˜åœ¨: '/home/qrbao/ray_results/PPO_2025-06-23_19-21-45'
    checkpoint_dir = "/home/qrbao/ray_results/PPO_2025-06-23_19-21-45"
    
    # æŸ¥æ‰¾æœ€æ–°çš„checkpoint
    checkpoint_path = None
    if os.path.exists(checkpoint_dir):
        # æŸ¥æ‰¾checkpointæ–‡ä»¶å¤¹
        for item in os.listdir(checkpoint_dir):
            if item.startswith("checkpoint_"):
                checkpoint_path = os.path.join(checkpoint_dir, item)
                break
    
    if checkpoint_path is None:
        print("âŒ æœªæ‰¾åˆ°checkpointæ–‡ä»¶ï¼")
        print(f"è¯·æ£€æŸ¥è·¯å¾„: {checkpoint_dir}")
        return
    
    print(f"âœ… æ‰¾åˆ°checkpoint: {checkpoint_path}")
    
    # 2. é‡æ–°æ³¨å†Œç¯å¢ƒï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    register_env("env", lambda _: PettingZooEnv(waterworld_v4.env()))
    
    # 3. é‡å»ºé…ç½®ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    config = (
        PPO.get_default_config()
        .environment("env")
        .multi_agent(
            policies={"p0"},
            policy_mapping_fn=(lambda aid, *args, **kwargs: "p0"),
        )
        .training(
            model={"vf_share_layers": True},
            vf_loss_coeff=0.005,
        )
        .rl_module(
            rl_module_spec=MultiRLModuleSpec(
                rl_module_specs={"p0": RLModuleSpec()},
            ),
        )
        # è¯„ä¼°æ—¶çš„ç‰¹æ®Šè®¾ç½®
        .env_runners(
            num_env_runners=0,  # è¯„ä¼°æ—¶ä¸éœ€è¦å¤šä¸ªworker
            create_local_env_runner=True,
        )
        .debugging(
            log_level="INFO"
        )
    )
    
    # 4. åŠ è½½è®­ç»ƒå¥½çš„ç®—æ³•
    print("ğŸ”„ æ­£åœ¨åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
    try:
        algorithm = PPO(config=config)
        algorithm.restore(checkpoint_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    return algorithm


def visualize_trained_agents(algorithm, num_episodes=3, render_mode="human"):
    """å¯è§†åŒ–è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“è¡¨ç°"""
    
    print(f"\nğŸ® å¼€å§‹å¯è§†åŒ–è¯„ä¼° ({num_episodes} é›†)...")
    
    # åˆ›å»ºç¯å¢ƒç”¨äºå¯è§†åŒ–
    env = waterworld_v4.env(render_mode=render_mode)
    
    episode_rewards = []
    episode_lengths = []
    
    for episode in range(num_episodes):
        print(f"\nğŸ“Š Episode {episode + 1}/{num_episodes}")
        
        # é‡ç½®ç¯å¢ƒ
        env.reset()
        
        episode_reward = {agent: 0 for agent in env.possible_agents}
        episode_length = 0
        
        # è¿è¡Œä¸€é›†
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            
            # ç´¯è®¡å¥–åŠ±
            if agent in episode_reward:
                episode_reward[agent] += reward
            
            if termination or truncation:
                # æ™ºèƒ½ä½“ç»“æŸï¼Œé€‰æ‹©NoneåŠ¨ä½œ
                action = None
            else:
                # ä½¿ç”¨è®­ç»ƒå¥½çš„ç­–ç•¥é€‰æ‹©åŠ¨ä½œ
                try:
                    # è·å–åŠ¨ä½œ
                    action_dict = algorithm.compute_single_action(
                        observation=obs,
                        policy_id="p0"  # ä½¿ç”¨è®­ç»ƒæ—¶çš„ç­–ç•¥ID
                    )
                    action = action_dict
                except Exception as e:
                    print(f"âš ï¸ åŠ¨ä½œè®¡ç®—é”™è¯¯: {e}")
                    # å¦‚æœå‡ºé”™ï¼Œä½¿ç”¨éšæœºåŠ¨ä½œ
                    action = env.action_space(agent).sample()
            
            # æ‰§è¡ŒåŠ¨ä½œ
            env.step(action)
            episode_length += 1
            
            # å¯è§†åŒ–æ¸²æŸ“
            if render_mode == "human":
                env.render()
                time.sleep(0.05)  # æ§åˆ¶æ’­æ”¾é€Ÿåº¦
        
        # è®°å½•æœ¬é›†ç»“æœ
        total_reward = sum(episode_reward.values())
        episode_rewards.append(total_reward)
        episode_lengths.append(episode_length)
        
        print(f"  ğŸ“ˆ æ€»å¥–åŠ±: {total_reward:.2f}")
        print(f"  ğŸ“Š æ™ºèƒ½ä½“å¥–åŠ±: {episode_reward}")
        print(f"  â±ï¸ é›†é•¿åº¦: {episode_length}")
    
    env.close()
    
    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“Š è¯„ä¼°ç»Ÿè®¡ç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"  å¹³å‡é›†é•¿åº¦: {np.mean(episode_lengths):.2f} Â± {np.std(episode_lengths):.2f}")
    print(f"  æœ€é«˜å¥–åŠ±: {np.max(episode_rewards):.2f}")
    print(f"  æœ€ä½å¥–åŠ±: {np.min(episode_rewards):.2f}")
    
    return episode_rewards, episode_lengths


def compare_with_random_policy(algorithm, num_episodes=5):
    """ä¸éšæœºç­–ç•¥è¿›è¡Œå¯¹æ¯”è¯„ä¼°"""
    
    print("\nğŸ”„ æ­£åœ¨è¿›è¡Œå¯¹æ¯”è¯„ä¼°ï¼ˆè®­ç»ƒç­–ç•¥ vs éšæœºç­–ç•¥ï¼‰...")
    
    env = waterworld_v4.env(render_mode=None)  # ä¸å¯è§†åŒ–ä»¥åŠ å¿«é€Ÿåº¦
    
    # è¯„ä¼°è®­ç»ƒå¥½çš„ç­–ç•¥
    trained_rewards = []
    for episode in range(num_episodes):
        env.reset()
        total_reward = 0
        
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            total_reward += reward
            
            if termination or truncation:
                action = None
            else:
                try:
                    action = algorithm.compute_single_action(obs, policy_id="p0")
                except:
                    action = env.action_space(agent).sample()
            
            env.step(action)
        
        trained_rewards.append(total_reward)
    
    # è¯„ä¼°éšæœºç­–ç•¥
    random_rewards = []
    for episode in range(num_episodes):
        env.reset()
        total_reward = 0
        
        for agent in env.agent_iter():
            obs, reward, termination, truncation, info = env.last()
            total_reward += reward
            
            if termination or truncation:
                action = None
            else:
                action = env.action_space(agent).sample()  # éšæœºåŠ¨ä½œ
            
            env.step(action)
        
        random_rewards.append(total_reward)
    
    env.close()
    
    # æ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š å¯¹æ¯”ç»“æœ:")
    print(f"  è®­ç»ƒç­–ç•¥å¹³å‡å¥–åŠ±: {np.mean(trained_rewards):.2f} Â± {np.std(trained_rewards):.2f}")
    print(f"  éšæœºç­–ç•¥å¹³å‡å¥–åŠ±: {np.mean(random_rewards):.2f} Â± {np.std(random_rewards):.2f}")
    improvement = np.mean(trained_rewards) - np.mean(random_rewards)
    print(f"  ğŸš€ æ€§èƒ½æå‡: {improvement:.2f}")
    
    return trained_rewards, random_rewards


def plot_training_progress(checkpoint_dir):
    """ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ€§èƒ½å˜åŒ–"""
    
    # è¿™ä¸ªå‡½æ•°éœ€è¦ä»TensorBoardæˆ–å…¶ä»–æ—¥å¿—ä¸­è¯»å–è®­ç»ƒæ•°æ®
    # å¦‚æœæ‚¨æœ‰è®­ç»ƒæ—¥å¿—ï¼Œå¯ä»¥åœ¨è¿™é‡Œè§£æå¹¶ç»˜åˆ¶
    print("ğŸ“ˆ è®­ç»ƒè¿›åº¦å›¾è¡¨åŠŸèƒ½å¾…å®ç°...")
    print("ğŸ’¡ å»ºè®®ä½¿ç”¨TensorBoardæŸ¥çœ‹è¯¦ç»†è®­ç»ƒè¿›åº¦:")
    print(f"   tensorboard --logdir {checkpoint_dir}")


def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ¯ PettingZoo Waterworld è®­ç»ƒæ¨¡å‹è¯„ä¼°")
    print("=" * 50)
    
    # 1. åŠ è½½æ¨¡å‹
    algorithm = load_and_evaluate_model()
    if algorithm is None:
        return
    
    # 2. å¯è§†åŒ–è¯„ä¼°
    print("\n" + "=" * 50)
    episode_rewards, episode_lengths = visualize_trained_agents(
        algorithm, 
        num_episodes=3,
        render_mode="human"  # è®¾ç½®ä¸º"human"ä»¥æ˜¾ç¤ºå¯è§†åŒ–
    )
    
    # 3. å¯¹æ¯”è¯„ä¼°
    print("\n" + "=" * 50)
    trained_rewards, random_rewards = compare_with_random_policy(
        algorithm, 
        num_episodes=10
    )
    
    # 4. æä¾›å»ºè®®
    print("\n" + "=" * 50)
    print("ğŸ’¡ è¿›ä¸€æ­¥åˆ†æå»ºè®®:")
    print("1. ä½¿ç”¨ render_mode='human' è§‚çœ‹æ™ºèƒ½ä½“å®æ—¶è¡¨ç°")
    print("2. è°ƒæ•´ç¯å¢ƒå‚æ•°æµ‹è¯•æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›")
    print("3. å°è¯•ä¸åŒçš„æ™ºèƒ½ä½“æ•°é‡")
    print("4. åˆ†ææ™ºèƒ½ä½“çš„åä½œè¡Œä¸ºæ¨¡å¼")
    
    # æ¸…ç†èµ„æº
    algorithm.stop()
    print("\nâœ… è¯„ä¼°å®Œæˆï¼")


if __name__ == "__main__":
    main()