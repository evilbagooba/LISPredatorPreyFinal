['predator_0', 'predator_1', 'prey_0', 'prey_1']
ÂàõÂª∫ÁöÑpolicies: ['predator_0', 'predator_1', 'prey_0', 'prey_1']
ÂàõÂª∫ÁöÑRL module specs: ['predator_0', 'predator_1', 'prey_0', 'prey_1']
/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: [33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64[0m
  gym.logger.warn(
/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: [33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64[0m
  gym.logger.warn(
/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64[0m
  logger.warn(
/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.[0m
  logger.warn(f"{pre} is not within the observation space.")
ÂºÄÂßãËÆ≠ÁªÉ...
2025-07-01 17:27:36,372	INFO worker.py:1917 -- Started a local Ray instance.
2025-07-01 17:27:36,864	WARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html
== Status ==
Current time: 2025-07-01 17:27:36 (running for 00:00:00.13)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-01_17-27-35_814200_2760726/artifacts/2025-07-01_17-27-36/PPO_2025-07-01_17-27-36/driver_artifacts
Number of trials: 1/1 (1 PENDING)
+---------------------+----------+-------+
| Trial name          | status   | loc   |
|---------------------+----------+-------|
| PPO_env_3e74a_00000 | PENDING  |       |
+---------------------+----------+-------+
[36m(PPO pid=2762593)[0m 2025-07-01 17:27:39,082	WARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html


[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values
[36m(MultiAgentEnvRunner pid=2762726)[0m 2025-07-01 17:27:41,465	WARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!
== Status ==
Current time: 2025-07-01 17:27:42 (running for 00:00:05.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-01_17-27-35_814200_2760726/artifacts/2025-07-01_17-27-36/PPO_2025-07-01_17-27-36/driver_artifacts
Number of trials: 1/1 (1 PENDING)
+---------------------+----------+-------+
| Trial name          | status   | loc   |
|---------------------+----------+-------|
| PPO_env_3e74a_00000 | PENDING  |       |
+---------------------+----------+-------+
[36m(PPO pid=2762593)[0m Install gputil for GPU system monitoring.


[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity v
[36m(MultiAgentEnvRunner pid=2762726)[0m alues
[36m(_WandbLoggingActor pid=2763079)[0m wandb: WARNING `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.
[36m(_WandbLoggingActor pid=2763079)[0m wandb: Currently logged in as: bqr010817 (bqr010817-kyushu-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(_WandbLoggingActor pid=2763079)[0m wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
[36m(_WandbLoggingActor pid=2763079)[0m wandb: Tracking run with wandb version 0.20.1
[36m(_WandbLoggingActor pid=2763079)[0m wandb: Run data is saved locally in /tmp/ray/session_2025-07-01_17-27-35_814200_2760726/artifacts/2025-07-01_17-27-36/PPO_2025-07-01_17-27-36/driver_artifacts/PPO_env_3e74a_00000_0_2025-07-01_17-27-36/wandb/run-20250701_172744-3e74a_00000
[36m(_WandbLoggingActor pid=2763079)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(_WandbLoggingActor pid=2763079)[0m wandb: Syncing run PPO_env_3e74a_00000
[36m(_WandbLoggingActor pid=2763079)[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/bqr010817-kyushu-university/waterworld-v4
[36m(_WandbLoggingActor pid=2763079)[0m wandb: üöÄ View run at https://wandb.ai/bqr010817-kyushu-university/waterworld-v4/runs/3e74a_00000
== Status ==
Current time: 2025-07-01 17:27:47 (running for 00:00:10.15)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-01_17-27-35_814200_2760726/artifacts/2025-07-01_17-27-36/PPO_2025-07-01_17-27-36/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+
| Trial name          | status   | loc                  |
|---------------------+----------+----------------------|
| PPO_env_3e74a_00000 | RUNNING  | 192.168.0.25:2762593 |
+---------------------+----------+----------------------+


Trial PPO_env_3e74a_00000 reported env_runners/episode_return_mean=-437.5023532617924,num_env_steps_sampled_lifetime=4000.0,env_runners/episode_len_mean=2000.0 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_init
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2762593)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-01_17-27-36/PPO_env_3e74a_00000_0_2025-07-01_17-27-36/checkpoint_000000)
[36m(PPO pid=2762593)[0m 2025-07-01 17:27:41,560	WARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future![32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values[32m [repeated 12057x across cluster][0m
[36m(_WandbLoggingActor pid=2763079)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-01_17-27-36/PPO_env_3e74a_00000_0_2025-07-01_17-27-36/checkpoint_000000)... Done. 0.0s
== Status ==
Current time: 2025-07-01 17:27:52 (running for 00:00:15.21)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-01_17-27-35_814200_2760726/artifacts/2025-07-01_17-27-36/PPO_2025-07-01_17-27-36/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+------+-------------------+---------------------+---------------------+-----------------+-----------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |   ts |   combined return |   return predator_0 |   return predator_1 |   return prey_0 |   return prey_1 |
|---------------------+----------+----------------------+--------+------------------+------+-------------------+---------------------+---------------------+-----------------+-----------------|
| PPO_env_3e74a_00000 | RUNNING  | 192.168.0.25:2762593 |      1 |           5.5334 | 4000 |          -437.502 |            -111.963 |            -59.5171 |        -60.6052 |        -205.417 |
+---------------------+----------+----------------------+--------+------------------+------+-------------------+---------------------+---------------------+-----------------+-----------------+


Trial PPO_env_3e74a_00000 reported env_runners/episode_return_mean=-461.0301688448726,num_env_steps_sampled_lifetime=8000.0,env_runners/episode_len_mean=2000.0 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_init
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2762593)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-01_17-27-36/PPO_env_3e74a_00000_0_2025-07-01_17-27-36/checkpoint_000001)
[36m(MultiAgentEnvRunner pid=2762726)[0m warning: velocites provided but types or ids not provided, returning only distance and velocity values[32m [repeated 12091x across cluster][0m
[36m(_WandbLoggingActor pid=2763079)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-01_17-27-36/PPO_env_3e74a_00000_0_2025-07-01_17-27-36/checkpoint_000001)... Done. 0.0s
