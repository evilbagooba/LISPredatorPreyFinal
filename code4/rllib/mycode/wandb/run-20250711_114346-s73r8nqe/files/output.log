['predator_0', 'predator_1', 'prey_0', 'prey_1']
ÂàõÂª∫ÁöÑpolicies: ['predator_0', 'predator_1', 'prey_0', 'prey_1']
ÂàõÂª∫ÁöÑRL module specs: ['predator_0', 'predator_1', 'prey_0', 'prey_1']
/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: [33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64[0m
  gym.logger.warn(
/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: [33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64[0m
  gym.logger.warn(
/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: [33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64[0m
  logger.warn(
/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: [33mWARN: The obs returned by the `reset()` method is not within the observation space.[0m
  logger.warn(f"{pre} is not within the observation space.")
ÂºÄÂßãËÆ≠ÁªÉ...
2025-07-11 11:43:48,224	INFO worker.py:1917 -- Started a local Ray instance.
2025-07-11 11:43:48,739	WARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html
== Status ==
Current time: 2025-07-11 11:43:48 (running for 00:00:00.12)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 PENDING)
+---------------------+----------+-------+
| Trial name          | status   | loc   |
|---------------------+----------+-------|
| PPO_env_df440_00000 | PENDING  |       |
+---------------------+----------+-------+
[36m(PPO pid=2143020)[0m 2025-07-11 11:43:51,002	WARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html


[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1']
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1']
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1']
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1']
[36m(MultiAgentEnvRunner pid=2143130)[0m 2025-07-11 11:43:53,391	WARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!
== Status ==
Current time: 2025-07-11 11:43:53 (running for 00:00:05.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 PENDING)
+---------------------+----------+-------+
| Trial name          | status   | loc   |
|---------------------+----------+-------|
| PPO_env_df440_00000 | PENDING  |       |
+---------------------+----------+-------+
[36m(PPO pid=2143020)[0m Install gputil for GPU system monitoring.


[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: WARNING `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Currently logged in as: bqr010817 (bqr010817-kyushu-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(_WandbLoggingActor pid=2143344)[0m wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Tracking run with wandb version 0.20.1
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Run data is saved locally in /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts/PPO_env_df440_00000_0_2025-07-11_11-43-48/wandb/run-20250711_114356-df440_00000
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Syncing run PPO_env_df440_00000
[36m(_WandbLoggingActor pid=2143344)[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/bqr010817-kyushu-university/waterworld-v4
[36m(_WandbLoggingActor pid=2143344)[0m wandb: üöÄ View run at https://wandb.ai/bqr010817-kyushu-university/waterworld-v4/runs/df440_00000
== Status ==
Current time: 2025-07-11 11:43:58 (running for 00:00:10.15)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+
| Trial name          | status   | loc                  |
|---------------------+----------+----------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |
+---------------------+----------+----------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 6x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 6x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 6x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1542.0,num_env_steps_sampled_lifetime=4000.0,env_runners/episode_return_mean=-346.935021100471 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_initi
== Status ==
Current time: 2025-07-11 11:44:04 (running for 00:00:15.31)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |   ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      1 |          8.17265 | 4000 |          -346.935 |        -17.4988 |            -68.2214 |        -187.213 |            -74.0019 |
+---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------+
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000000)
[36m(PPO pid=2143020)[0m 2025-07-11 11:43:53,467	WARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future![32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000000)... Done. 0.0s


== Status ==
Current time: 2025-07-11 11:44:09 (running for 00:00:20.38)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |   ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      1 |          8.17265 | 4000 |          -346.935 |        -17.4988 |            -68.2214 |        -187.213 |            -74.0019 |
+---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1670.0,num_env_steps_sampled_lifetime=8000.0,env_runners/episode_return_mean=-380.93223223924133 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000001)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000001)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:44:14 (running for 00:00:25.43)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |   ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      2 |          14.8889 | 8000 |          -380.932 |        -63.0645 |             -72.335 |        -164.873 |              -80.66 |
+---------------------+----------+----------------------+--------+------------------+------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1606.3333333333333,num_env_steps_sampled_lifetime=12000.0,env_runners/episode_return_mean=-372.01094190082085 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000002)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000002)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:44:19 (running for 00:00:30.50)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      3 |          23.1215 | 12000 |          -372.011 |        -58.9623 |             -77.688 |         -149.78 |            -85.5808 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:44:24 (running for 00:00:35.52)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      3 |          23.1215 | 12000 |          -372.011 |        -58.9623 |             -77.688 |         -149.78 |            -85.5808 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1550.5555555555557,num_env_steps_sampled_lifetime=16000.0,env_runners/episode_return_mean=-354.49285014682937 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000003)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000003)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:44:29 (running for 00:00:40.55)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      4 |           30.786 | 16000 |          -354.493 |        -54.4951 |            -78.8735 |        -140.836 |            -80.2883 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:44:34 (running for 00:00:45.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      4 |           30.786 | 16000 |          -354.493 |        -54.4951 |            -78.8735 |        -140.836 |            -80.2883 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1537.8333333333333,num_env_steps_sampled_lifetime=20000.0,env_runners/episode_return_mean=-354.93747779209957 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000004)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000004)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:44:39 (running for 00:00:50.60)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      5 |          39.2691 | 20000 |          -354.937 |        -59.2186 |            -79.6924 |        -126.869 |            -89.1574 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1555.0714285714287,num_env_steps_sampled_lifetime=24000.0,env_runners/episode_return_mean=-356.00004333665305 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000005)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000005)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:44:44 (running for 00:00:55.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      6 |          46.2952 | 24000 |              -356 |        -58.4709 |            -79.0502 |        -130.341 |            -88.1383 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:44:49 (running for 00:01:00.69)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      6 |          46.2952 | 24000 |              -356 |        -58.4709 |            -79.0502 |        -130.341 |            -88.1383 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1578.5,num_env_steps_sampled_lifetime=28000.0,env_runners/episode_return_mean=-362.9381092075656 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000006)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000006)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:44:54 (running for 00:01:05.75)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      7 |          53.5271 | 28000 |          -362.938 |        -62.5709 |            -80.4063 |         -130.84 |            -89.1206 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1590.85,num_env_steps_sampled_lifetime=32000.0,env_runners/episode_return_mean=-363.3787910311685 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000007)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000007)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:44:59 (running for 00:01:10.76)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      8 |          60.5621 | 32000 |          -363.379 |        -66.6048 |             -81.497 |        -126.982 |            -88.2945 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1593.5454545454545,num_env_steps_sampled_lifetime=36000.0,env_runners/episode_return_mean=-364.0186732490398 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, '

[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000008)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000008)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:45:04 (running for 00:01:15.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      9 |           67.669 | 36000 |          -364.019 |        -64.3588 |            -80.9497 |        -132.061 |            -86.6494 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:45:09 (running for 00:01:20.83)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |      9 |           67.669 | 36000 |          -364.019 |        -64.3588 |            -80.9497 |        -132.061 |            -86.6494 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.6666666666667,num_env_steps_sampled_lifetime=40000.0,env_runners/episode_return_mean=-363.10080756383854 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000009)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000009)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:45:14 (running for 00:01:25.93)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     10 |          75.5437 | 40000 |          -363.101 |         -62.589 |            -80.5775 |        -127.972 |            -91.9624 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1577.1153846153845,num_env_steps_sampled_lifetime=44000.0,env_runners/episode_return_mean=-358.8387248077787 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000010)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000010)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:45:19 (running for 00:01:31.02)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     11 |          81.7492 | 44000 |          -358.839 |         -60.966 |            -78.0248 |        -130.077 |            -89.7707 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:45:24 (running for 00:01:36.03)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     11 |          81.7492 | 44000 |          -358.839 |         -60.966 |            -78.0248 |        -130.077 |            -89.7707 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1609.4827586206898,num_env_steps_sampled_lifetime=48000.0,env_runners/episode_return_mean=-347.5809474671088 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000011)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000011)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:45:29 (running for 00:01:41.04)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     12 |          88.5122 | 48000 |          -347.581 |        -65.6866 |            -72.0273 |        -124.645 |            -85.2221 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1622.032258064516,num_env_steps_sampled_lifetime=52000.0,env_runners/episode_return_mean=-346.1893756306207 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000012)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000012)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:45:34 (running for 00:01:46.07)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     13 |          95.7123 | 52000 |          -346.189 |        -68.5081 |            -70.3973 |        -122.799 |            -84.4851 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1613.6470588235295,num_env_steps_sampled_lifetime=56000.0,env_runners/episode_return_mean=-343.13033680723265 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000013)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000013)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:45:39 (running for 00:01:51.13)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     14 |          103.033 | 56000 |           -343.13 |        -68.2448 |             -69.138 |        -121.843 |            -83.9042 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:45:44 (running for 00:01:56.15)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     14 |          103.033 | 56000 |           -343.13 |        -68.2448 |             -69.138 |        -121.843 |            -83.9042 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1604.388888888889,num_env_steps_sampled_lifetime=60000.0,env_runners/episode_return_mean=-338.29111819914283 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000014)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000014)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:45:49 (running for 00:02:01.20)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     15 |          110.013 | 60000 |          -338.291 |         -66.303 |            -67.5673 |         -119.97 |            -84.4511 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1610.157894736842,num_env_steps_sampled_lifetime=64000.0,env_runners/episode_return_mean=-340.40428182433004 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000015)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000015)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:45:54 (running for 00:02:06.26)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     16 |          116.951 | 64000 |          -340.404 |        -69.9252 |            -67.7768 |         -117.89 |            -84.8123 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:46:00 (running for 00:02:11.30)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     16 |          116.951 | 64000 |          -340.404 |        -69.9252 |            -67.7768 |         -117.89 |            -84.8123 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1601.4761904761904,num_env_steps_sampled_lifetime=68000.0,env_runners/episode_return_mean=-328.02877285529263 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000016)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000016)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:46:05 (running for 00:02:16.37)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     17 |          124.306 | 68000 |          -328.029 |        -72.0762 |            -62.4741 |        -111.678 |            -81.8003 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1606.3863636363637,num_env_steps_sampled_lifetime=72000.0,env_runners/episode_return_mean=-322.89395543052257 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000017)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000017)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:46:10 (running for 00:02:21.42)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     18 |          130.963 | 72000 |          -322.894 |        -72.8052 |            -59.1965 |        -113.059 |            -77.8331 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1604.1739130434783,num_env_steps_sampled_lifetime=76000.0,env_runners/episode_return_mean=-313.7792466210259 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000018)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000018)...
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:46:15 (running for 00:02:26.47)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     19 |          137.597 | 76000 |          -313.779 |         -72.623 |            -56.1232 |        -108.159 |            -76.8741 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:46:20 (running for 00:02:31.51)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     19 |          137.597 | 76000 |          -313.779 |         -72.623 |            -56.1232 |        -108.159 |            -76.8741 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1605.8333333333333,num_env_steps_sampled_lifetime=80000.0,env_runners/episode_return_mean=-314.44209251947274 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lst
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000019)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000019)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:46:25 (running for 00:02:36.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     20 |          144.625 | 80000 |          -314.442 |        -74.1703 |            -56.1513 |         -106.92 |            -77.2003 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1615.02,num_env_steps_sampled_lifetime=84000.0,env_runners/episode_return_mean=-309.4408739157685 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000020)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000020)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:46:30 (running for 00:02:41.62)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     21 |          152.106 | 84000 |          -309.441 |        -76.5231 |            -52.5636 |        -107.325 |            -73.0296 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:46:35 (running for 00:02:46.64)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     21 |          152.106 | 84000 |          -309.441 |        -76.5231 |            -52.5636 |        -107.325 |            -73.0296 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1617.88,num_env_steps_sampled_lifetime=88000.0,env_runners/episode_return_mean=-307.2150166742874 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000021)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000021)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:46:40 (running for 00:02:51.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     22 |          159.973 | 88000 |          -307.215 |        -77.5757 |            -50.7391 |         -105.78 |            -73.1208 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1617.28,num_env_steps_sampled_lifetime=92000.0,env_runners/episode_return_mean=-296.02234248188296 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000022)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000022)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:46:45 (running for 00:02:56.69)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     23 |          167.888 | 92000 |          -296.022 |        -76.5945 |            -49.3488 |        -98.2772 |            -71.8018 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:46:50 (running for 00:03:01.73)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     23 |          167.888 | 92000 |          -296.022 |        -76.5945 |            -49.3488 |        -98.2772 |            -71.8018 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1640.14,num_env_steps_sampled_lifetime=96000.0,env_runners/episode_return_mean=-295.2064040494434 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000023)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000023)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:46:55 (running for 00:03:06.73)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     24 |          174.768 | 96000 |          -295.206 |        -78.3088 |            -47.1175 |        -101.874 |            -67.9066 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1647.3,num_env_steps_sampled_lifetime=100000.0,env_runners/episode_return_mean=-286.6249072340878 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000024)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000024)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:47:00 (running for 00:03:11.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     25 |          181.445 | 100000 |          -286.625 |        -80.1062 |            -43.3949 |        -98.2546 |            -64.8693 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1655.42,num_env_steps_sampled_lifetime=104000.0,env_runners/episode_return_mean=-278.9212969625844 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000025)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000025)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:47:05 (running for 00:03:16.80)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     26 |          187.835 | 104000 |          -278.921 |        -78.4902 |            -39.0002 |        -102.973 |            -58.4574 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:47:10 (running for 00:03:21.86)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     26 |          187.835 | 104000 |          -278.921 |        -78.4902 |            -39.0002 |        -102.973 |            -58.4574 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1662.46,num_env_steps_sampled_lifetime=108000.0,env_runners/episode_return_mean=-274.9733413434058 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000026)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000026)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:47:15 (running for 00:03:26.88)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     27 |          194.748 | 108000 |          -274.973 |        -81.3727 |            -36.2287 |        -102.168 |            -55.2044 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1679.2,num_env_steps_sampled_lifetime=112000.0,env_runners/episode_return_mean=-261.36360424395525 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000027)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000027)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:47:20 (running for 00:03:31.96)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     28 |          202.564 | 112000 |          -261.364 |        -84.8567 |            -29.6777 |        -100.463 |            -46.3666 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:47:25 (running for 00:03:36.98)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     28 |          202.564 | 112000 |          -261.364 |        -84.8567 |            -29.6777 |        -100.463 |            -46.3666 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1677.7,num_env_steps_sampled_lifetime=116000.0,env_runners/episode_return_mean=-257.00626636153555 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000028)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000028)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:47:30 (running for 00:03:42.06)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     29 |          210.706 | 116000 |          -257.006 |        -84.9673 |            -26.8106 |        -101.422 |            -43.8066 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1684.64,num_env_steps_sampled_lifetime=120000.0,env_runners/episode_return_mean=-246.87615419732444 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000029)
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000029)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:47:35 (running for 00:03:47.13)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     30 |          217.444 | 120000 |          -246.876 |        -87.9986 |            -22.1184 |        -98.0831 |             -38.676 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1701.52,num_env_steps_sampled_lifetime=124000.0,env_runners/episode_return_mean=-243.73323075739268 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000030)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000030)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:47:40 (running for 00:03:52.15)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     31 |          223.219 | 124000 |          -243.733 |        -91.8103 |            -20.2491 |        -95.8069 |            -35.8669 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:47:45 (running for 00:03:57.17)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     31 |          223.219 | 124000 |          -243.733 |        -91.8103 |            -20.2491 |        -95.8069 |            -35.8669 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1714.96,num_env_steps_sampled_lifetime=128000.0,env_runners/episode_return_mean=-239.89766546675935 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000031)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000031)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:47:51 (running for 00:04:02.27)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     32 |          229.845 | 128000 |          -239.898 |        -93.8504 |            -18.5204 |        -95.7619 |             -31.765 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1703.62,num_env_steps_sampled_lifetime=132000.0,env_runners/episode_return_mean=-238.15387006245416 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000032)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000032)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:47:56 (running for 00:04:07.32)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     33 |           237.67 | 132000 |          -238.154 |        -91.7008 |            -17.5142 |         -99.462 |            -29.4769 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:48:01 (running for 00:04:12.36)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     33 |           237.67 | 132000 |          -238.154 |        -91.7008 |            -17.5142 |         -99.462 |            -29.4769 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1696.54,num_env_steps_sampled_lifetime=136000.0,env_runners/episode_return_mean=-227.28586773550515 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000033)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000033)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:48:06 (running for 00:04:17.43)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     34 |          245.925 | 136000 |          -227.286 |         -89.864 |            -12.4003 |        -101.711 |            -23.3102 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:48:11 (running for 00:04:22.48)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     34 |          245.925 | 136000 |          -227.286 |         -89.864 |            -12.4003 |        -101.711 |            -23.3102 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1697.8,num_env_steps_sampled_lifetime=140000.0,env_runners/episode_return_mean=-225.56000132430123 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000034)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000034)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:48:16 (running for 00:04:27.51)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     35 |          254.329 | 140000 |           -225.56 |        -90.3013 |            -12.2168 |        -101.563 |            -21.4785 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1710.34,num_env_steps_sampled_lifetime=144000.0,env_runners/episode_return_mean=-221.16711757721816 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000035)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000035)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:48:21 (running for 00:04:32.59)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     36 |          261.713 | 144000 |          -221.167 |         -90.757 |            -9.94636 |        -102.156 |            -18.3081 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:48:26 (running for 00:04:37.61)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     36 |          261.713 | 144000 |          -221.167 |         -90.757 |            -9.94636 |        -102.156 |            -18.3081 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1717.38,num_env_steps_sampled_lifetime=148000.0,env_runners/episode_return_mean=-212.06518369717492 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000036)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000036)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:48:31 (running for 00:04:42.67)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     37 |          269.348 | 148000 |          -212.065 |         -90.208 |            -5.31041 |        -104.701 |            -11.8461 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1713.04,num_env_steps_sampled_lifetime=152000.0,env_runners/episode_return_mean=-207.51165076885272 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000037)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000037)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:48:36 (running for 00:04:47.70)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     38 |          276.352 | 152000 |          -207.512 |        -86.9113 |            -2.31804 |        -108.604 |            -9.67798 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:48:41 (running for 00:04:52.74)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     38 |          276.352 | 152000 |          -207.512 |        -86.9113 |            -2.31804 |        -108.604 |            -9.67798 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1721.2,num_env_steps_sampled_lifetime=156000.0,env_runners/episode_return_mean=-207.99732750335548 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000038)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000038)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:48:46 (running for 00:04:57.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     39 |          283.681 | 156000 |          -207.997 |        -85.6926 |            -1.71048 |        -112.099 |             -8.4956 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1731.86,num_env_steps_sampled_lifetime=160000.0,env_runners/episode_return_mean=-207.175812651633 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000039)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000039)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:48:51 (running for 00:05:02.82)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     40 |          292.146 | 160000 |          -207.176 |        -85.7336 |            -1.18729 |         -114.96 |            -5.29445 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:48:56 (running for 00:05:07.84)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     40 |          292.146 | 160000 |          -207.176 |        -85.7336 |            -1.18729 |         -114.96 |            -5.29445 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1719.56,num_env_steps_sampled_lifetime=164000.0,env_runners/episode_return_mean=-206.059633800183 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000040)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000040)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:49:01 (running for 00:05:12.88)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     41 |          299.637 | 164000 |           -206.06 |        -82.6179 |           0.0220527 |        -119.115 |            -4.34896 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1724.58,num_env_steps_sampled_lifetime=168000.0,env_runners/episode_return_mean=-204.6590966658206 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000041)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000041)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:49:06 (running for 00:05:17.93)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     42 |          306.282 | 168000 |          -204.659 |         -82.525 |             2.52886 |         -121.67 |            -2.99297 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:49:11 (running for 00:05:22.98)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     42 |          306.282 | 168000 |          -204.659 |         -82.525 |             2.52886 |         -121.67 |            -2.99297 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1718.36,num_env_steps_sampled_lifetime=172000.0,env_runners/episode_return_mean=-193.0525425143829 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000042)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000042)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:49:16 (running for 00:05:28.00)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     43 |          313.591 | 172000 |          -193.053 |          -80.54 |             8.40124 |        -123.735 |             2.82153 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1710.4,num_env_steps_sampled_lifetime=176000.0,env_runners/episode_return_mean=-190.2286547971824 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000043)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000043)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:49:21 (running for 00:05:33.04)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     44 |          320.846 | 176000 |          -190.229 |         -78.051 |             12.3304 |        -130.889 |             6.38054 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1702.42,num_env_steps_sampled_lifetime=180000.0,env_runners/episode_return_mean=-179.31673231020144 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000044)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000044)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:49:26 (running for 00:05:38.12)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     45 |          327.508 | 180000 |          -179.317 |        -75.5959 |             17.9863 |        -132.617 |             10.9098 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:49:31 (running for 00:05:43.17)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     45 |          327.508 | 180000 |          -179.317 |        -75.5959 |             17.9863 |        -132.617 |             10.9098 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1697.28,num_env_steps_sampled_lifetime=184000.0,env_runners/episode_return_mean=-181.36941056655098 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000045)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000045)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:49:36 (running for 00:05:48.24)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     46 |          334.562 | 184000 |          -181.369 |        -75.2785 |              16.758 |        -134.467 |             11.6181 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1680.36,num_env_steps_sampled_lifetime=188000.0,env_runners/episode_return_mean=-182.4100201697035 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000046)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000046)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:49:42 (running for 00:05:53.32)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     47 |          342.859 | 188000 |           -182.41 |        -72.8323 |             14.0498 |        -131.652 |             8.02426 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:49:47 (running for 00:05:58.33)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     47 |          342.859 | 188000 |           -182.41 |        -72.8323 |             14.0498 |        -131.652 |             8.02426 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1676.04,num_env_steps_sampled_lifetime=192000.0,env_runners/episode_return_mean=-181.38758461609228 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000047)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000047)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:49:52 (running for 00:06:03.36)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     48 |          350.929 | 192000 |          -181.388 |        -73.3716 |             15.5023 |        -133.071 |             9.55254 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:49:57 (running for 00:06:08.37)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     48 |          350.929 | 192000 |          -181.388 |        -73.3716 |             15.5023 |        -133.071 |             9.55254 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1670.2,num_env_steps_sampled_lifetime=196000.0,env_runners/episode_return_mean=-169.94751830480075 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000048)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000048)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:50:02 (running for 00:06:13.40)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     49 |          359.121 | 196000 |          -169.948 |        -69.0506 |             20.8702 |        -133.587 |               11.82 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1668.56,num_env_steps_sampled_lifetime=200000.0,env_runners/episode_return_mean=-172.39509186828383 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000049)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000049)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:50:07 (running for 00:06:18.46)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     50 |          365.868 | 200000 |          -172.395 |         -68.562 |              20.776 |        -134.079 |             9.47018 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1661.24,num_env_steps_sampled_lifetime=204000.0,env_runners/episode_return_mean=-167.3968872405319 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000050)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000050)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:50:12 (running for 00:06:23.47)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     51 |          372.796 | 204000 |          -167.397 |        -66.1847 |             22.9648 |        -136.145 |              11.968 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:50:17 (running for 00:06:28.48)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     51 |          372.796 | 204000 |          -167.397 |        -66.1847 |             22.9648 |        -136.145 |              11.968 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1651.9,num_env_steps_sampled_lifetime=208000.0,env_runners/episode_return_mean=-160.06866909869862 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000051)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000051)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:50:22 (running for 00:06:33.48)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     52 |          380.544 | 208000 |          -160.069 |        -66.2506 |             26.1888 |        -135.389 |             15.3825 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:50:27 (running for 00:06:38.50)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     52 |          380.544 | 208000 |          -160.069 |        -66.2506 |             26.1888 |        -135.389 |             15.3825 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1638.4,num_env_steps_sampled_lifetime=212000.0,env_runners/episode_return_mean=-154.6277494683319 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 6x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000052)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000052)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:50:32 (running for 00:06:43.55)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     53 |          389.951 | 212000 |          -154.628 |        -65.4798 |             27.5566 |        -133.512 |             16.8079 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:50:37 (running for 00:06:48.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     53 |          389.951 | 212000 |          -154.628 |        -65.4798 |             27.5566 |        -133.512 |             16.8079 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1627.72,num_env_steps_sampled_lifetime=216000.0,env_runners/episode_return_mean=-155.07981301232863 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000053)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000053)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:50:42 (running for 00:06:53.63)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     54 |          399.776 | 216000 |           -155.08 |        -65.9495 |             26.4328 |        -130.316 |             14.7532 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:50:47 (running for 00:06:58.65)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     54 |          399.776 | 216000 |           -155.08 |        -65.9495 |             26.4328 |        -130.316 |             14.7532 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1612.1,num_env_steps_sampled_lifetime=220000.0,env_runners/episode_return_mean=-146.9803545354143 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000054)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000054)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:50:52 (running for 00:07:03.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     55 |          409.336 | 220000 |           -146.98 |        -64.1192 |             29.9201 |        -127.808 |              15.027 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1603.64,num_env_steps_sampled_lifetime=224000.0,env_runners/episode_return_mean=-141.193866623392 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000055)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000055)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:50:57 (running for 00:07:08.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     56 |          417.631 | 224000 |          -141.194 |        -61.7809 |             33.4562 |         -130.28 |             17.4106 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:51:02 (running for 00:07:13.71)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     56 |          417.631 | 224000 |          -141.194 |        -61.7809 |             33.4562 |         -130.28 |             17.4106 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1588.98,num_env_steps_sampled_lifetime=228000.0,env_runners/episode_return_mean=-139.00112988678424 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000056)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000056)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:51:07 (running for 00:07:18.78)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     57 |          425.397 | 228000 |          -139.001 |        -60.0929 |             33.2197 |        -127.778 |             15.6497 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1595.84,num_env_steps_sampled_lifetime=232000.0,env_runners/episode_return_mean=-136.21496242264257 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000057)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000057)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:51:12 (running for 00:07:23.88)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     58 |          432.208 | 232000 |          -136.215 |        -59.7542 |             34.6231 |        -129.928 |             18.8441 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:51:17 (running for 00:07:28.89)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     58 |          432.208 | 232000 |          -136.215 |        -59.7542 |             34.6231 |        -129.928 |             18.8441 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1591.78,num_env_steps_sampled_lifetime=236000.0,env_runners/episode_return_mean=-130.33971060829035 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000058)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000058)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:51:22 (running for 00:07:33.94)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     59 |            439.5 | 236000 |           -130.34 |        -62.2691 |             37.6296 |        -125.992 |             20.2914 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1588.6,num_env_steps_sampled_lifetime=240000.0,env_runners/episode_return_mean=-125.23630232738523 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000059)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000059)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:51:27 (running for 00:07:39.01)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     60 |          448.363 | 240000 |          -125.236 |        -61.1639 |             38.9072 |        -125.319 |             22.3394 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:51:32 (running for 00:07:44.06)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     60 |          448.363 | 240000 |          -125.236 |        -61.1639 |             38.9072 |        -125.319 |             22.3394 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1575.76,num_env_steps_sampled_lifetime=244000.0,env_runners/episode_return_mean=-127.31057109951703 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000060)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000060)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:51:37 (running for 00:07:49.13)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     61 |          457.418 | 244000 |          -127.311 |        -59.3458 |             37.0812 |        -123.207 |             18.1613 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:51:42 (running for 00:07:54.18)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     61 |          457.418 | 244000 |          -127.311 |        -59.3458 |             37.0812 |        -123.207 |             18.1613 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1575.16,num_env_steps_sampled_lifetime=248000.0,env_runners/episode_return_mean=-120.98131014537907 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000061)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000061)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:51:47 (running for 00:07:59.19)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     62 |          466.114 | 248000 |          -120.981 |        -59.0855 |             38.2185 |        -123.224 |             23.1096 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1576.04,num_env_steps_sampled_lifetime=252000.0,env_runners/episode_return_mean=-127.2536389384479 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
== Status ==
Current time: 2025-07-11 11:51:52 (running for 00:08:04.20)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     63 |          473.579 | 252000 |          -127.254 |        -58.9888 |             34.1907 |        -121.654 |              19.198 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000062)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000062)... Done. 0.0s


== Status ==
Current time: 2025-07-11 11:51:57 (running for 00:08:09.21)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     63 |          473.579 | 252000 |          -127.254 |        -58.9888 |             34.1907 |        -121.654 |              19.198 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.54,num_env_steps_sampled_lifetime=256000.0,env_runners/episode_return_mean=-133.76157806667155 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000063)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000063)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:52:03 (running for 00:08:14.27)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     64 |          481.054 | 256000 |          -133.762 |        -61.7783 |             30.8861 |        -117.894 |              15.025 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:52:08 (running for 00:08:19.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     64 |          481.054 | 256000 |          -133.762 |        -61.7783 |             30.8861 |        -117.894 |              15.025 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1562.92,num_env_steps_sampled_lifetime=260000.0,env_runners/episode_return_mean=-130.8423130794082 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_le

[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000064)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000064)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:52:13 (running for 00:08:24.31)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     65 |          488.737 | 260000 |          -130.842 |         -60.951 |             32.2183 |        -114.786 |             12.6763 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1561.7,num_env_steps_sampled_lifetime=264000.0,env_runners/episode_return_mean=-122.57507451440335 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000065)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000065)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:52:18 (running for 00:08:29.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     66 |          496.445 | 264000 |          -122.575 |         -59.759 |             35.7361 |        -113.557 |             15.0051 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:52:23 (running for 00:08:34.40)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     66 |          496.445 | 264000 |          -122.575 |         -59.759 |             35.7361 |        -113.557 |             15.0051 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1577.38,num_env_steps_sampled_lifetime=268000.0,env_runners/episode_return_mean=-115.32282837954546 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000066)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000066)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:52:28 (running for 00:08:39.44)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     67 |          505.172 | 268000 |          -115.323 |        -59.1854 |             38.7095 |        -114.046 |             19.1986 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1571.72,num_env_steps_sampled_lifetime=272000.0,env_runners/episode_return_mean=-114.89292033861574 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000067)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000067)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:52:33 (running for 00:08:44.49)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     68 |          512.733 | 272000 |          -114.893 |        -58.0346 |             38.9323 |        -114.814 |             19.0238 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:52:38 (running for 00:08:49.51)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     68 |          512.733 | 272000 |          -114.893 |        -58.0346 |             38.9323 |        -114.814 |             19.0238 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1576.28,num_env_steps_sampled_lifetime=276000.0,env_runners/episode_return_mean=-111.95703729334925 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000068)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000068)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:52:43 (running for 00:08:54.54)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     69 |            518.7 | 276000 |          -111.957 |        -57.8928 |              38.285 |        -112.285 |             19.9353 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.64,num_env_steps_sampled_lifetime=280000.0,env_runners/episode_return_mean=-103.51608623682492 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000069)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000069)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:52:48 (running for 00:08:59.57)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     70 |          526.637 | 280000 |          -103.516 |          -56.94 |              41.896 |        -110.076 |             21.6036 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:52:53 (running for 00:09:04.63)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     70 |          526.637 | 280000 |          -103.516 |          -56.94 |              41.896 |        -110.076 |             21.6036 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1564.0,num_env_steps_sampled_lifetime=284000.0,env_runners/episode_return_mean=-97.0421878869215 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000070)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000070)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:52:58 (running for 00:09:09.72)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     71 |          535.824 | 284000 |          -97.0422 |        -52.1917 |             43.8383 |        -109.506 |              20.817 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:53:03 (running for 00:09:14.78)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     71 |          535.824 | 284000 |          -97.0422 |        -52.1917 |             43.8383 |        -109.506 |              20.817 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1560.22,num_env_steps_sampled_lifetime=288000.0,env_runners/episode_return_mean=-87.96416231549588 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000071)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000071)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:53:08 (running for 00:09:19.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     72 |          545.235 | 288000 |          -87.9642 |        -51.3038 |             49.1128 |        -108.801 |             23.0279 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:53:13 (running for 00:09:24.85)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     72 |          545.235 | 288000 |          -87.9642 |        -51.3038 |             49.1128 |        -108.801 |             23.0279 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1568.26,num_env_steps_sampled_lifetime=292000.0,env_runners/episode_return_mean=-87.67094570581355 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000072)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000072)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:53:18 (running for 00:09:29.90)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     73 |          553.976 | 292000 |          -87.6709 |        -49.7369 |             48.5476 |        -109.894 |             23.4125 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1562.94,num_env_steps_sampled_lifetime=296000.0,env_runners/episode_return_mean=-80.98896886535425 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000073)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000073)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:53:23 (running for 00:09:34.93)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     74 |          563.298 | 296000 |           -80.989 |        -48.9676 |             49.2406 |        -104.349 |             23.0873 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:53:28 (running for 00:09:39.94)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     74 |          563.298 | 296000 |           -80.989 |        -48.9676 |             49.2406 |        -104.349 |             23.0873 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1579.9,num_env_steps_sampled_lifetime=300000.0,env_runners/episode_return_mean=-75.89703493522832 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000074)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000074)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:53:33 (running for 00:09:44.99)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     75 |          571.293 | 300000 |           -75.897 |        -50.2106 |             51.6522 |        -104.621 |             27.2823 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:53:38 (running for 00:09:50.00)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     75 |          571.293 | 300000 |           -75.897 |        -50.2106 |             51.6522 |        -104.621 |             27.2823 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1590.22,num_env_steps_sampled_lifetime=304000.0,env_runners/episode_return_mean=-79.36166431816035 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000075)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000075)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:53:43 (running for 00:09:55.07)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     76 |          580.922 | 304000 |          -79.3617 |        -50.7076 |             50.7099 |        -106.868 |             27.5042 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:53:48 (running for 00:10:00.09)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     76 |          580.922 | 304000 |          -79.3617 |        -50.7076 |             50.7099 |        -106.868 |             27.5042 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1587.32,num_env_steps_sampled_lifetime=308000.0,env_runners/episode_return_mean=-77.07155834602943 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000076)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000076)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:53:53 (running for 00:10:05.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     77 |           589.13 | 308000 |          -77.0716 |         -50.003 |             51.9835 |        -107.315 |              28.263 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1586.58,num_env_steps_sampled_lifetime=312000.0,env_runners/episode_return_mean=-72.4623071220345 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000077)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000077)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:53:58 (running for 00:10:10.20)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     78 |           597.48 | 312000 |          -72.4623 |        -49.8132 |             52.8653 |        -105.341 |             29.8262 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:54:04 (running for 00:10:15.26)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     78 |           597.48 | 312000 |          -72.4623 |        -49.8132 |             52.8653 |        -105.341 |             29.8262 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1587.3,num_env_steps_sampled_lifetime=316000.0,env_runners/episode_return_mean=-71.05333257453233 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000078)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000078)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:54:09 (running for 00:10:20.29)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     79 |          606.313 | 316000 |          -71.0533 |        -46.4419 |              52.682 |        -106.703 |             29.4093 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:54:14 (running for 00:10:25.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     79 |          606.313 | 316000 |          -71.0533 |        -46.4419 |              52.682 |        -106.703 |             29.4093 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1588.12,num_env_steps_sampled_lifetime=320000.0,env_runners/episode_return_mean=-70.6864468456517 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000079)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000079)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:54:19 (running for 00:10:30.37)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     80 |          614.936 | 320000 |          -70.6864 |        -46.0355 |             52.1947 |         -106.82 |             29.9744 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1594.94,num_env_steps_sampled_lifetime=324000.0,env_runners/episode_return_mean=-62.199315169129335 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000080)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000080)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:54:24 (running for 00:10:35.38)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     81 |          622.457 | 324000 |          -62.1993 |         -45.771 |             55.4695 |        -108.743 |             36.8454 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:54:29 (running for 00:10:40.40)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     81 |          622.457 | 324000 |          -62.1993 |         -45.771 |             55.4695 |        -108.743 |             36.8454 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1590.88,num_env_steps_sampled_lifetime=328000.0,env_runners/episode_return_mean=-61.1733490908889 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000081)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000081)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:54:34 (running for 00:10:45.49)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     82 |          629.804 | 328000 |          -61.1733 |        -44.2664 |             54.6412 |        -106.861 |             35.3127 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1597.72,num_env_steps_sampled_lifetime=332000.0,env_runners/episode_return_mean=-51.54044827834187 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i

[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000082)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000082)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:54:39 (running for 00:10:50.51)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     83 |          637.007 | 332000 |          -51.5404 |        -45.6706 |             59.4546 |        -104.749 |              39.425 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:54:44 (running for 00:10:55.57)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     83 |          637.007 | 332000 |          -51.5404 |        -45.6706 |             59.4546 |        -104.749 |              39.425 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1599.38,num_env_steps_sampled_lifetime=336000.0,env_runners/episode_return_mean=-44.00613636830838 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000083)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000083)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:54:49 (running for 00:11:00.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     84 |          645.497 | 336000 |          -44.0061 |        -43.3839 |             64.5154 |        -108.405 |              43.267 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:54:54 (running for 00:11:05.71)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     84 |          645.497 | 336000 |          -44.0061 |        -43.3839 |             64.5154 |        -108.405 |              43.267 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1610.16,num_env_steps_sampled_lifetime=340000.0,env_runners/episode_return_mean=-47.277382643339436 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000084)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000084)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:54:59 (running for 00:11:10.78)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     85 |          653.962 | 340000 |          -47.2774 |         -43.715 |             63.0278 |        -111.154 |              44.564 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1590.42,num_env_steps_sampled_lifetime=344000.0,env_runners/episode_return_mean=-45.46612117254297 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000085)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000085)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:55:04 (running for 00:11:15.82)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     86 |          661.703 | 344000 |          -45.4661 |        -41.2675 |             63.6107 |        -111.335 |             43.5258 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1605.0,num_env_steps_sampled_lifetime=348000.0,env_runners/episode_return_mean=-40.79930585919057 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000086)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000086)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:55:09 (running for 00:11:20.89)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     87 |          667.978 | 348000 |          -40.7993 |          -44.05 |             64.5145 |         -105.44 |             44.1764 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:55:14 (running for 00:11:25.96)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     87 |          667.978 | 348000 |          -40.7993 |          -44.05 |             64.5145 |         -105.44 |             44.1764 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1605.36,num_env_steps_sampled_lifetime=352000.0,env_runners/episode_return_mean=-41.70445484359874 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000087)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000087)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:55:19 (running for 00:11:31.01)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     88 |           674.13 | 352000 |          -41.7045 |        -44.6195 |             64.0826 |        -105.674 |             44.5061 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1602.16,num_env_steps_sampled_lifetime=356000.0,env_runners/episode_return_mean=-42.37549672268264 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i

[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000088)
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000088)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:55:24 (running for 00:11:36.09)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     89 |          681.416 | 356000 |          -42.3755 |        -42.8176 |             63.5049 |        -106.052 |             42.9895 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:55:29 (running for 00:11:41.11)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     89 |          681.416 | 356000 |          -42.3755 |        -42.8176 |             63.5049 |        -106.052 |             42.9895 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1607.72,num_env_steps_sampled_lifetime=360000.0,env_runners/episode_return_mean=-39.83755921029569 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000089)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000089)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:55:34 (running for 00:11:46.11)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     90 |          689.173 | 360000 |          -39.8376 |        -43.1283 |             63.3393 |        -103.171 |             43.1223 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1616.5,num_env_steps_sampled_lifetime=364000.0,env_runners/episode_return_mean=-33.08800195437475 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000090)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000090)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:55:39 (running for 00:11:51.20)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     91 |          697.359 | 364000 |           -33.088 |        -44.7126 |             65.9178 |        -99.5844 |             45.2913 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:55:44 (running for 00:11:56.26)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     91 |          697.359 | 364000 |           -33.088 |        -44.7126 |             65.9178 |        -99.5844 |             45.2913 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1627.54,num_env_steps_sampled_lifetime=368000.0,env_runners/episode_return_mean=-38.94041808572155 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000091)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000091)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:55:50 (running for 00:12:01.31)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     92 |          706.005 | 368000 |          -38.9404 |        -45.5365 |             64.0572 |        -103.013 |             45.5517 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1636.46,num_env_steps_sampled_lifetime=372000.0,env_runners/episode_return_mean=-45.562463558090705 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000092)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000092)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:55:55 (running for 00:12:06.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     93 |          713.648 | 372000 |          -45.5625 |        -46.1036 |             62.8023 |        -107.786 |             45.5252 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:56:00 (running for 00:12:11.37)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     93 |          713.648 | 372000 |          -45.5625 |        -46.1036 |             62.8023 |        -107.786 |             45.5252 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1628.86,num_env_steps_sampled_lifetime=376000.0,env_runners/episode_return_mean=-47.370118408592205 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000093)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000093)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:56:05 (running for 00:12:16.46)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     94 |          721.502 | 376000 |          -47.3701 |        -45.2728 |             63.6084 |        -111.279 |             45.5732 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1628.6,num_env_steps_sampled_lifetime=380000.0,env_runners/episode_return_mean=-47.13630034924052 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000094)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000094)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:56:10 (running for 00:12:21.53)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     95 |          727.872 | 380000 |          -47.1363 |        -47.5751 |             63.1676 |         -108.12 |              45.391 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:56:15 (running for 00:12:26.59)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     95 |          727.872 | 380000 |          -47.1363 |        -47.5751 |             63.1676 |         -108.12 |              45.391 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1631.1,num_env_steps_sampled_lifetime=384000.0,env_runners/episode_return_mean=-46.98606042185152 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000095)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000095)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:56:20 (running for 00:12:31.67)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     96 |          735.151 | 384000 |          -46.9861 |        -46.6132 |             62.6695 |        -108.871 |             45.8287 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1635.8,num_env_steps_sampled_lifetime=388000.0,env_runners/episode_return_mean=-36.94241438652156 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000096)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000096)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:56:25 (running for 00:12:36.69)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     97 |          742.051 | 388000 |          -36.9424 |        -45.8287 |             66.1407 |        -105.537 |             48.2822 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:56:30 (running for 00:12:41.71)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     97 |          742.051 | 388000 |          -36.9424 |        -45.8287 |             66.1407 |        -105.537 |             48.2822 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1643.28,num_env_steps_sampled_lifetime=392000.0,env_runners/episode_return_mean=-32.36291603913323 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000097)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000097)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:56:35 (running for 00:12:46.71)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     98 |           750.43 | 392000 |          -32.3629 |        -46.3623 |             67.3234 |        -102.809 |             49.4849 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1646.24,num_env_steps_sampled_lifetime=396000.0,env_runners/episode_return_mean=-38.615807102588235 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000098)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000098)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:56:40 (running for 00:12:51.77)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     99 |          757.884 | 396000 |          -38.6158 |        -47.5824 |             64.7121 |        -103.941 |             48.1957 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:56:45 (running for 00:12:56.83)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |     99 |          757.884 | 396000 |          -38.6158 |        -47.5824 |             64.7121 |        -103.941 |             48.1957 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1653.26,num_env_steps_sampled_lifetime=400000.0,env_runners/episode_return_mean=-33.09046516966782 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000099)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000099)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:56:50 (running for 00:13:01.88)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    100 |          764.228 | 400000 |          -33.0905 |        -47.5787 |             67.0366 |         -102.71 |              50.162 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1646.3,num_env_steps_sampled_lifetime=404000.0,env_runners/episode_return_mean=-27.74279425358874 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000100)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000100)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:56:55 (running for 00:13:06.97)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    101 |          772.015 | 404000 |          -27.7428 |        -48.4209 |             70.1173 |        -98.4985 |             49.0594 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:57:00 (running for 00:13:11.98)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    101 |          772.015 | 404000 |          -27.7428 |        -48.4209 |             70.1173 |        -98.4985 |             49.0594 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1649.04,num_env_steps_sampled_lifetime=408000.0,env_runners/episode_return_mean=-34.69533441272384 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000101)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000101)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:57:05 (running for 00:13:17.06)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    102 |          779.229 | 408000 |          -34.6953 |        -49.7428 |             67.2824 |        -97.6696 |             45.4347 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1659.34,num_env_steps_sampled_lifetime=412000.0,env_runners/episode_return_mean=-36.18829295024016 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000102)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000102)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:57:10 (running for 00:13:22.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    103 |          785.518 | 412000 |          -36.1883 |        -51.4818 |             66.5684 |         -95.761 |             44.4861 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1651.96,num_env_steps_sampled_lifetime=416000.0,env_runners/episode_return_mean=-40.11260066245079 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000103)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000103)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:57:15 (running for 00:13:27.20)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    104 |          792.974 | 416000 |          -40.1126 |        -50.0953 |             64.2098 |        -97.6442 |             43.4171 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:57:20 (running for 00:13:32.21)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    104 |          792.974 | 416000 |          -40.1126 |        -50.0953 |             64.2098 |        -97.6442 |             43.4171 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1652.86,num_env_steps_sampled_lifetime=420000.0,env_runners/episode_return_mean=-38.66093556423504 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000104)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000104)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:57:26 (running for 00:13:37.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    105 |          800.804 | 420000 |          -38.6609 |        -50.3052 |             64.7297 |        -97.0997 |             44.0142 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1650.82,num_env_steps_sampled_lifetime=424000.0,env_runners/episode_return_mean=-33.597446742182505 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000105)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000105)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:57:31 (running for 00:13:42.32)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    106 |          808.946 | 424000 |          -33.5974 |        -49.4626 |             66.8136 |        -96.6999 |             45.7514 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:57:36 (running for 00:13:47.39)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    106 |          808.946 | 424000 |          -33.5974 |        -49.4626 |             66.8136 |        -96.6999 |             45.7514 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1651.04,num_env_steps_sampled_lifetime=428000.0,env_runners/episode_return_mean=-43.96035083871471 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000106)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000106)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:57:41 (running for 00:13:52.48)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    107 |           817.03 | 428000 |          -43.9604 |        -47.3265 |             62.8203 |        -102.206 |             42.7517 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:57:46 (running for 00:13:57.54)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    107 |           817.03 | 428000 |          -43.9604 |        -47.3265 |             62.8203 |        -102.206 |             42.7517 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1641.56,num_env_steps_sampled_lifetime=432000.0,env_runners/episode_return_mean=-50.600801180007956 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000107)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000107)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 11:57:51 (running for 00:14:02.58)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    108 |           825.78 | 432000 |          -50.6008 |        -45.8136 |              60.482 |        -106.454 |              41.185 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1632.9,num_env_steps_sampled_lifetime=436000.0,env_runners/episode_return_mean=-50.868741030694395 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
== Status ==
Current time: 2025-07-11 11:57:56 (running for 00:14:07.60)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    109 |          834.386 | 436000 |          -50.8687 |        -44.1621 |             61.4578 |        -109.683 |              41.519 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000108)


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000108)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:58:01 (running for 00:14:12.65)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    109 |          834.386 | 436000 |          -50.8687 |        -44.1621 |             61.4578 |        -109.683 |              41.519 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1626.64,num_env_steps_sampled_lifetime=440000.0,env_runners/episode_return_mean=-53.06984917075829 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000109)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000109)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:58:06 (running for 00:14:17.67)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    110 |          842.211 | 440000 |          -53.0698 |        -42.5221 |             61.0551 |        -112.895 |             41.2919 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:58:11 (running for 00:14:22.69)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    110 |          842.211 | 440000 |          -53.0698 |        -42.5221 |             61.0551 |        -112.895 |             41.2919 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1608.5,num_env_steps_sampled_lifetime=444000.0,env_runners/episode_return_mean=-52.45776706048084 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000110)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000110)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:58:16 (running for 00:14:27.80)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    111 |           851.08 | 444000 |          -52.4578 |        -42.0866 |             62.2025 |        -114.206 |             41.6323 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:58:21 (running for 00:14:32.81)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    111 |           851.08 | 444000 |          -52.4578 |        -42.0866 |             62.2025 |        -114.206 |             41.6323 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1606.5,num_env_steps_sampled_lifetime=448000.0,env_runners/episode_return_mean=-48.73688365487441 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000111)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000111)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:58:26 (running for 00:14:37.89)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    112 |          859.612 | 448000 |          -48.7369 |        -41.4478 |             63.9137 |        -114.561 |             43.3584 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1607.0,num_env_steps_sampled_lifetime=452000.0,env_runners/episode_return_mean=-52.439123666799645 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000112)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000112)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:58:31 (running for 00:14:42.92)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    113 |          866.553 | 452000 |          -52.4391 |        -41.3136 |             63.0796 |        -116.068 |             41.8626 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1613.28,num_env_steps_sampled_lifetime=456000.0,env_runners/episode_return_mean=-46.847699223279285 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000113)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000113)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:58:36 (running for 00:14:48.02)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    114 |          873.041 | 456000 |          -46.8477 |        -41.5524 |             63.3173 |        -112.651 |             44.0389 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:58:41 (running for 00:14:53.03)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    114 |          873.041 | 456000 |          -46.8477 |        -41.5524 |             63.3173 |        -112.651 |             44.0389 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1626.18,num_env_steps_sampled_lifetime=460000.0,env_runners/episode_return_mean=-49.8726188114695 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000114)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000114)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:58:46 (running for 00:14:58.16)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    115 |           879.73 | 460000 |          -49.8726 |        -40.1195 |             62.7827 |        -117.475 |             44.9394 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1615.08,num_env_steps_sampled_lifetime=464000.0,env_runners/episode_return_mean=-46.97518822688364 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000115)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000115)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:58:51 (running for 00:15:03.24)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    116 |          887.221 | 464000 |          -46.9752 |        -38.4229 |             65.7377 |        -119.574 |             45.2835 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:58:57 (running for 00:15:08.31)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    116 |          887.221 | 464000 |          -46.9752 |        -38.4229 |             65.7377 |        -119.574 |             45.2835 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1606.78,num_env_steps_sampled_lifetime=468000.0,env_runners/episode_return_mean=-48.605936144953546 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000116)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000116)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:59:02 (running for 00:15:13.39)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    117 |          894.807 | 468000 |          -48.6059 |        -37.4597 |             66.2498 |        -122.805 |             45.4089 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1607.34,num_env_steps_sampled_lifetime=472000.0,env_runners/episode_return_mean=-52.11007894394954 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000117)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000117)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 11:59:07 (running for 00:15:18.42)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    118 |          901.825 | 472000 |          -52.1101 |        -38.5378 |             66.0125 |        -124.231 |             44.6457 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:59:12 (running for 00:15:23.43)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    118 |          901.825 | 472000 |          -52.1101 |        -38.5378 |             66.0125 |        -124.231 |             44.6457 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1596.92,num_env_steps_sampled_lifetime=476000.0,env_runners/episode_return_mean=-47.99247003090652 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000118)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000118)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:59:17 (running for 00:15:28.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    119 |          910.707 | 476000 |          -47.9925 |        -37.5441 |             66.6468 |        -121.091 |             43.9956 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1582.3,num_env_steps_sampled_lifetime=480000.0,env_runners/episode_return_mean=-51.692138504614384 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000119)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000119)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:59:22 (running for 00:15:33.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    120 |          918.815 | 480000 |          -51.6921 |        -38.8986 |             66.1736 |         -121.98 |              43.013 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:59:27 (running for 00:15:38.63)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    120 |          918.815 | 480000 |          -51.6921 |        -38.8986 |             66.1736 |         -121.98 |              43.013 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1596.42,num_env_steps_sampled_lifetime=484000.0,env_runners/episode_return_mean=-50.497326765658435 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000120)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000120)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:59:32 (running for 00:15:43.73)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    121 |          927.118 | 484000 |          -50.4973 |        -39.3928 |             66.5736 |        -122.248 |             44.5695 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:59:37 (running for 00:15:48.81)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    121 |          927.118 | 484000 |          -50.4973 |        -39.3928 |             66.5736 |        -122.248 |             44.5695 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1595.12,num_env_steps_sampled_lifetime=488000.0,env_runners/episode_return_mean=-44.896928114631464 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000121)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000121)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:59:42 (running for 00:15:53.85)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    122 |          935.378 | 488000 |          -44.8969 |        -38.4394 |              68.738 |        -121.868 |             46.6723 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1584.32,num_env_steps_sampled_lifetime=492000.0,env_runners/episode_return_mean=-39.61356552781288 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000122)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000122)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:59:47 (running for 00:15:58.94)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    123 |          943.177 | 492000 |          -39.6136 |        -36.8712 |             71.4646 |        -122.945 |              48.738 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 11:59:52 (running for 00:16:03.96)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    123 |          943.177 | 492000 |          -39.6136 |        -36.8712 |             71.4646 |        -122.945 |              48.738 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1583.68,num_env_steps_sampled_lifetime=496000.0,env_runners/episode_return_mean=-40.62417849436579 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000123)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000123)... Done. 0.0s
== Status ==
Current time: 2025-07-11 11:59:57 (running for 00:16:09.00)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    124 |          951.151 | 496000 |          -40.6242 |        -36.3181 |             71.1183 |        -123.205 |             47.7805 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1582.8,num_env_steps_sampled_lifetime=500000.0,env_runners/episode_return_mean=-36.712524648947294 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000124)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000124)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:00:02 (running for 00:16:14.09)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    125 |          959.184 | 500000 |          -36.7125 |        -35.8881 |             73.9382 |        -125.984 |             51.2211 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:00:07 (running for 00:16:19.18)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    125 |          959.184 | 500000 |          -36.7125 |        -35.8881 |             73.9382 |        -125.984 |             51.2211 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1583.1,num_env_steps_sampled_lifetime=504000.0,env_runners/episode_return_mean=-35.81313897927078 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000125)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000125)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:00:12 (running for 00:16:24.25)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    126 |          966.372 | 504000 |          -35.8131 |         -34.892 |             74.7359 |        -126.812 |             51.1546 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1581.0,num_env_steps_sampled_lifetime=508000.0,env_runners/episode_return_mean=-28.365556486998475 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000126)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000126)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:00:18 (running for 00:16:29.32)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    127 |          974.237 | 508000 |          -28.3656 |        -33.7482 |             78.3988 |        -127.272 |             54.2557 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:00:23 (running for 00:16:34.33)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    127 |          974.237 | 508000 |          -28.3656 |        -33.7482 |             78.3988 |        -127.272 |             54.2557 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.8,num_env_steps_sampled_lifetime=512000.0,env_runners/episode_return_mean=-26.226191645298094 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000127)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000127)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:00:28 (running for 00:16:39.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    128 |          981.518 | 512000 |          -26.2262 |        -36.0236 |             80.7863 |        -123.935 |             52.9462 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1575.52,num_env_steps_sampled_lifetime=516000.0,env_runners/episode_return_mean=-22.17751836673011 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000128)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000128)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:00:33 (running for 00:16:44.40)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    129 |          988.564 | 516000 |          -22.1775 |         -36.483 |             83.4059 |        -123.381 |             54.2807 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:00:38 (running for 00:16:49.48)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    129 |          988.564 | 516000 |          -22.1775 |         -36.483 |             83.4059 |        -123.381 |             54.2807 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.0,num_env_steps_sampled_lifetime=520000.0,env_runners/episode_return_mean=-17.6864169356565 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000129)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000129)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:00:43 (running for 00:16:54.55)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    130 |          996.049 | 520000 |          -17.6864 |        -36.8581 |             86.9602 |        -124.418 |             56.6293 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1584.74,num_env_steps_sampled_lifetime=524000.0,env_runners/episode_return_mean=-18.84575155960394 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000130)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000130)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:00:48 (running for 00:16:59.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    131 |          1003.65 | 524000 |          -18.8458 |        -35.9158 |             87.2855 |        -127.227 |              57.011 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:00:53 (running for 00:17:04.58)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    131 |          1003.65 | 524000 |          -18.8458 |        -35.9158 |             87.2855 |        -127.227 |              57.011 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1576.42,num_env_steps_sampled_lifetime=528000.0,env_runners/episode_return_mean=-18.1138580285416 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000131)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000131)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:00:58 (running for 00:17:09.63)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    132 |          1012.05 | 528000 |          -18.1139 |        -35.4779 |             87.3307 |        -123.153 |             53.1864 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.56,num_env_steps_sampled_lifetime=532000.0,env_runners/episode_return_mean=-15.209318482527296 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000132)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000132)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:01:03 (running for 00:17:14.71)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    133 |          1019.65 | 532000 |          -15.2093 |        -34.8232 |             88.7884 |        -124.132 |             54.9575 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:01:08 (running for 00:17:19.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    133 |          1019.65 | 532000 |          -15.2093 |        -34.8232 |             88.7884 |        -124.132 |             54.9575 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1565.16,num_env_steps_sampled_lifetime=536000.0,env_runners/episode_return_mean=-11.305951564874697 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000133)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000133)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:01:13 (running for 00:17:24.87)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    134 |          1028.35 | 536000 |           -11.306 |        -34.8048 |             90.2085 |        -120.484 |              53.774 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:01:18 (running for 00:17:29.96)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    134 |          1028.35 | 536000 |           -11.306 |        -34.8048 |             90.2085 |        -120.484 |              53.774 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1561.62,num_env_steps_sampled_lifetime=540000.0,env_runners/episode_return_mean=-5.846467618945763 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000134)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000134)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:01:23 (running for 00:17:34.98)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    135 |           1036.4 | 540000 |          -5.84647 |        -33.9217 |             92.2071 |        -120.908 |             56.7765 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.92,num_env_steps_sampled_lifetime=544000.0,env_runners/episode_return_mean=-5.333250209243617 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000135)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000135)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:01:28 (running for 00:17:40.05)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    136 |          1044.06 | 544000 |          -5.33325 |        -34.8995 |             92.3537 |        -120.374 |             57.5866 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:01:33 (running for 00:17:45.07)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    136 |          1044.06 | 544000 |          -5.33325 |        -34.8995 |             92.3537 |        -120.374 |             57.5866 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1561.42,num_env_steps_sampled_lifetime=548000.0,env_runners/episode_return_mean=-3.9746570589918533 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000136)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000136)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:01:38 (running for 00:17:50.13)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    137 |          1052.48 | 548000 |          -3.97466 |        -33.6195 |              92.533 |        -121.796 |             58.9083 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:01:43 (running for 00:17:55.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    137 |          1052.48 | 548000 |          -3.97466 |        -33.6195 |              92.533 |        -121.796 |             58.9083 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1550.14,num_env_steps_sampled_lifetime=552000.0,env_runners/episode_return_mean=-1.0222623464942133 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000137)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000137)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:01:48 (running for 00:18:00.23)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    138 |          1061.83 | 552000 |          -1.02226 |        -32.2152 |             95.0161 |        -124.791 |             60.9678 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.2,num_env_steps_sampled_lifetime=556000.0,env_runners/episode_return_mean=5.661570538488937 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000138)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000138)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:01:54 (running for 00:18:05.29)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    139 |             1070 | 556000 |           5.66157 |        -32.6687 |             97.4654 |        -121.976 |             62.8413 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:01:59 (running for 00:18:10.37)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    139 |             1070 | 556000 |           5.66157 |        -32.6687 |             97.4654 |        -121.976 |             62.8413 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1554.4,num_env_steps_sampled_lifetime=560000.0,env_runners/episode_return_mean=5.052508439477633 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000139)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000139)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:02:04 (running for 00:18:15.46)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    140 |          1078.41 | 560000 |           5.05251 |        -31.3767 |             99.0287 |        -126.906 |              64.307 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:02:09 (running for 00:18:20.54)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    140 |          1078.41 | 560000 |           5.05251 |        -31.3767 |             99.0287 |        -126.906 |              64.307 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1553.52,num_env_steps_sampled_lifetime=564000.0,env_runners/episode_return_mean=3.637177315166266 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000140)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000140)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:02:14 (running for 00:18:25.57)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    141 |          1086.76 | 564000 |           3.63718 |        -30.8974 |              98.252 |        -127.854 |             64.1367 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1552.64,num_env_steps_sampled_lifetime=568000.0,env_runners/episode_return_mean=6.484533146877594 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000141)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000141)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:02:19 (running for 00:18:30.61)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    142 |           1094.6 | 568000 |           6.48453 |        -31.5283 |             99.1349 |         -126.86 |             65.7381 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:02:24 (running for 00:18:35.63)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    142 |           1094.6 | 568000 |           6.48453 |        -31.5283 |             99.1349 |         -126.86 |             65.7381 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1554.1,num_env_steps_sampled_lifetime=572000.0,env_runners/episode_return_mean=10.86599558899385 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len'

[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000142)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000142)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:02:29 (running for 00:18:40.68)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    143 |           1101.3 | 572000 |            10.866 |         -30.962 |              102.63 |        -128.783 |             67.9807 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1546.4,num_env_steps_sampled_lifetime=576000.0,env_runners/episode_return_mean=12.72721122121304 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000143)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000143)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:02:34 (running for 00:18:45.75)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    144 |          1110.18 | 576000 |           12.7272 |        -31.8558 |             102.151 |        -124.099 |             66.5306 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:02:39 (running for 00:18:50.83)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    144 |          1110.18 | 576000 |           12.7272 |        -31.8558 |             102.151 |        -124.099 |             66.5306 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1536.9,num_env_steps_sampled_lifetime=580000.0,env_runners/episode_return_mean=13.728669844218391 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000144)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000144)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:02:44 (running for 00:18:55.90)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    145 |          1117.46 | 580000 |           13.7287 |        -32.4674 |             102.299 |        -121.247 |              65.144 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1526.86,num_env_steps_sampled_lifetime=584000.0,env_runners/episode_return_mean=15.660578972772017 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000145)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000145)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:02:49 (running for 00:19:00.92)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    146 |          1124.35 | 584000 |           15.6606 |        -33.6333 |             104.043 |        -119.434 |             64.6842 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:02:54 (running for 00:19:05.93)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    146 |          1124.35 | 584000 |           15.6606 |        -33.6333 |             104.043 |        -119.434 |             64.6842 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1544.14,num_env_steps_sampled_lifetime=588000.0,env_runners/episode_return_mean=17.06482735246842 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000146)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000146)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:02:59 (running for 00:19:11.02)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    147 |          1131.39 | 588000 |           17.0648 |        -35.0222 |             105.462 |        -119.957 |             66.5829 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1550.56,num_env_steps_sampled_lifetime=592000.0,env_runners/episode_return_mean=18.678758825097724 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000147)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000147)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:03:04 (running for 00:19:16.04)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    148 |          1139.43 | 592000 |           18.6788 |        -34.0393 |             104.197 |        -117.392 |             65.9132 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:03:09 (running for 00:19:21.12)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    148 |          1139.43 | 592000 |           18.6788 |        -34.0393 |             104.197 |        -117.392 |             65.9132 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1558.88,num_env_steps_sampled_lifetime=596000.0,env_runners/episode_return_mean=24.679574002363367 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000148)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000148)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:03:14 (running for 00:19:26.18)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    149 |          1147.51 | 596000 |           24.6796 |        -35.2185 |              106.71 |         -114.82 |             68.0082 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:03:19 (running for 00:19:31.26)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    149 |          1147.51 | 596000 |           24.6796 |        -35.2185 |              106.71 |         -114.82 |             68.0082 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1559.18,num_env_steps_sampled_lifetime=600000.0,env_runners/episode_return_mean=23.27221482540694 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000149)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000149)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:03:25 (running for 00:19:36.33)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    150 |             1156 | 600000 |           23.2722 |        -35.7499 |             106.489 |        -115.617 |             68.1495 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1557.9,num_env_steps_sampled_lifetime=604000.0,env_runners/episode_return_mean=24.384182623646534 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000150)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000150)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:03:30 (running for 00:19:41.42)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    151 |          1163.27 | 604000 |           24.3842 |         -36.311 |             106.324 |        -114.895 |             69.2664 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1567.56,num_env_steps_sampled_lifetime=608000.0,env_runners/episode_return_mean=27.526319087936276 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000151)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000151)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:03:35 (running for 00:19:46.50)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    152 |          1170.67 | 608000 |           27.5263 |         -37.595 |             108.702 |        -117.126 |             73.5451 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:03:40 (running for 00:19:51.59)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    152 |          1170.67 | 608000 |           27.5263 |         -37.595 |             108.702 |        -117.126 |             73.5451 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1571.04,num_env_steps_sampled_lifetime=612000.0,env_runners/episode_return_mean=26.70171758098971 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000152)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000152)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:03:45 (running for 00:19:56.59)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    153 |           1178.6 | 612000 |           26.7017 |        -39.0368 |             108.219 |        -116.379 |             73.8982 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1571.0,num_env_steps_sampled_lifetime=616000.0,env_runners/episode_return_mean=25.696685636674893 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000153)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000153)...
== Status ==
Current time: 2025-07-11 12:03:50 (running for 00:20:01.68)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    154 |          1185.84 | 616000 |           25.6967 |        -39.2823 |             107.382 |        -114.914 |             72.5117 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s


== Status ==
Current time: 2025-07-11 12:03:55 (running for 00:20:06.70)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    154 |          1185.84 | 616000 |           25.6967 |        -39.2823 |             107.382 |        -114.914 |             72.5117 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1573.4,num_env_steps_sampled_lifetime=620000.0,env_runners/episode_return_mean=18.516354646347924 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000154)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000154)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:04:00 (running for 00:20:11.72)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    155 |          1193.46 | 620000 |           18.5164 |        -40.4555 |             103.737 |        -113.999 |             69.2338 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.64,num_env_steps_sampled_lifetime=624000.0,env_runners/episode_return_mean=20.033146723723608 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000155)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000155)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:04:05 (running for 00:20:16.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    156 |          1200.14 | 624000 |           20.0331 |        -40.1065 |             104.699 |        -112.724 |             68.1654 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:04:10 (running for 00:20:21.88)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    156 |          1200.14 | 624000 |           20.0331 |        -40.1065 |             104.699 |        -112.724 |             68.1654 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1572.46,num_env_steps_sampled_lifetime=628000.0,env_runners/episode_return_mean=24.305686895757773 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000156)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000156)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:04:15 (running for 00:20:26.98)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    157 |          1208.06 | 628000 |           24.3057 |        -42.1226 |             105.885 |        -106.954 |             67.4971 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1591.92,num_env_steps_sampled_lifetime=632000.0,env_runners/episode_return_mean=25.409593724026124 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000157)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000157)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:04:20 (running for 00:20:32.03)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    158 |          1215.58 | 632000 |           25.4096 |        -43.2104 |             107.666 |         -109.64 |             70.5931 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:04:25 (running for 00:20:37.05)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    158 |          1215.58 | 632000 |           25.4096 |        -43.2104 |             107.666 |         -109.64 |             70.5931 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1584.8,num_env_steps_sampled_lifetime=636000.0,env_runners/episode_return_mean=24.11466891004571 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000158)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000158)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:04:30 (running for 00:20:42.07)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    159 |          1224.27 | 636000 |           24.1147 |        -40.9568 |             107.579 |        -114.181 |             71.6732 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:04:35 (running for 00:20:47.09)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    159 |          1224.27 | 636000 |           24.1147 |        -40.9568 |             107.579 |        -114.181 |             71.6732 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1585.44,num_env_steps_sampled_lifetime=640000.0,env_runners/episode_return_mean=28.567102501391943 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000159)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000159)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:04:40 (running for 00:20:52.23)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    160 |          1232.47 | 640000 |           28.5671 |         -41.497 |             109.048 |        -112.175 |             73.1904 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:04:45 (running for 00:20:57.24)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    160 |          1232.47 | 640000 |           28.5671 |         -41.497 |             109.048 |        -112.175 |             73.1904 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1576.06,num_env_steps_sampled_lifetime=644000.0,env_runners/episode_return_mean=28.842393508176155 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000160)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000160)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:04:51 (running for 00:21:02.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    161 |          1241.45 | 644000 |           28.8424 |        -41.8955 |             109.496 |        -108.826 |              70.067 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1565.52,num_env_steps_sampled_lifetime=648000.0,env_runners/episode_return_mean=31.250299601798602 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000161)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000161)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:04:56 (running for 00:21:07.38)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    162 |           1250.3 | 648000 |           31.2503 |        -41.5114 |             110.499 |        -107.514 |             69.7768 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:05:01 (running for 00:21:12.46)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    162 |           1250.3 | 648000 |           31.2503 |        -41.5114 |             110.499 |        -107.514 |             69.7768 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1572.52,num_env_steps_sampled_lifetime=652000.0,env_runners/episode_return_mean=31.802744261221243 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000162)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000162)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:05:06 (running for 00:21:17.50)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    163 |          1257.95 | 652000 |           31.8027 |        -39.8752 |             111.896 |        -111.969 |             71.7515 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1582.16,num_env_steps_sampled_lifetime=656000.0,env_runners/episode_return_mean=31.119730762798234 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000163)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000163)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:05:11 (running for 00:21:22.62)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    164 |          1265.45 | 656000 |           31.1197 |        -40.1942 |             113.199 |        -116.473 |             74.5879 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:05:16 (running for 00:21:27.64)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    164 |          1265.45 | 656000 |           31.1197 |        -40.1942 |             113.199 |        -116.473 |             74.5879 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1580.56,num_env_steps_sampled_lifetime=660000.0,env_runners/episode_return_mean=31.920747518433863 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000164)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000164)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:05:21 (running for 00:21:32.72)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    165 |          1272.39 | 660000 |           31.9207 |          -39.38 |              113.16 |        -117.255 |             75.3959 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.22,num_env_steps_sampled_lifetime=664000.0,env_runners/episode_return_mean=29.282965622819592 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000165)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000165)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:05:26 (running for 00:21:37.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    166 |          1280.68 | 664000 |            29.283 |        -37.1246 |             111.973 |        -120.122 |             74.5564 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:05:31 (running for 00:21:42.87)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    166 |          1280.68 | 664000 |            29.283 |        -37.1246 |             111.973 |        -120.122 |             74.5564 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.28,num_env_steps_sampled_lifetime=668000.0,env_runners/episode_return_mean=30.09274223511863 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000166)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000166)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:05:36 (running for 00:21:47.87)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    167 |             1287 | 668000 |           30.0927 |         -35.693 |             112.794 |        -124.374 |             77.3664 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1572.2,num_env_steps_sampled_lifetime=672000.0,env_runners/episode_return_mean=36.10320548786527 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000167)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000167)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:05:41 (running for 00:21:53.06)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    168 |          1294.67 | 672000 |           36.1032 |        -35.7189 |             113.765 |        -119.569 |             77.6263 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:05:46 (running for 00:21:58.07)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    168 |          1294.67 | 672000 |           36.1032 |        -35.7189 |             113.765 |        -119.569 |             77.6263 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.42,num_env_steps_sampled_lifetime=676000.0,env_runners/episode_return_mean=34.04513288996932 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000168)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000168)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:05:51 (running for 00:22:03.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    169 |          1302.77 | 676000 |           34.0451 |        -34.1046 |             113.043 |        -122.825 |             77.9317 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:05:56 (running for 00:22:08.16)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    169 |          1302.77 | 676000 |           34.0451 |        -34.1046 |             113.043 |        -122.825 |             77.9317 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1565.88,num_env_steps_sampled_lifetime=680000.0,env_runners/episode_return_mean=31.31522314136147 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000169)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000169)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:06:02 (running for 00:22:13.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    170 |          1311.86 | 680000 |           31.3152 |          -34.65 |             111.519 |        -123.338 |             77.7843 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1564.24,num_env_steps_sampled_lifetime=684000.0,env_runners/episode_return_mean=31.639393663040224 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000170)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000170)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:06:07 (running for 00:22:18.31)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    171 |          1320.22 | 684000 |           31.6394 |        -33.9899 |             112.907 |        -125.361 |             78.0839 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:06:12 (running for 00:22:23.40)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    171 |          1320.22 | 684000 |           31.6394 |        -33.9899 |             112.907 |        -125.361 |             78.0839 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.94,num_env_steps_sampled_lifetime=688000.0,env_runners/episode_return_mean=31.72771103928665 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000171)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000171)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:06:17 (running for 00:22:28.43)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    172 |          1327.78 | 688000 |           31.7277 |        -30.4725 |             113.754 |         -131.79 |              80.237 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1554.96,num_env_steps_sampled_lifetime=692000.0,env_runners/episode_return_mean=27.160990131786708 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000172)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000172)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:06:22 (running for 00:22:33.57)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    173 |          1335.69 | 692000 |            27.161 |        -30.0153 |             111.616 |        -130.768 |             76.3284 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:06:27 (running for 00:22:38.58)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    173 |          1335.69 | 692000 |            27.161 |        -30.0153 |             111.616 |        -130.768 |             76.3284 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1556.44,num_env_steps_sampled_lifetime=696000.0,env_runners/episode_return_mean=26.2601456311739 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000173)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000173)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:06:32 (running for 00:22:43.68)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    174 |          1342.35 | 696000 |           26.2601 |        -29.9222 |             110.789 |        -130.105 |             75.4984 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1556.74,num_env_steps_sampled_lifetime=700000.0,env_runners/episode_return_mean=33.74177096691873 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000174)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000174)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:06:37 (running for 00:22:48.72)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    175 |          1350.11 | 700000 |           33.7418 |        -28.7387 |             114.282 |         -130.58 |             78.7782 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:06:42 (running for 00:22:53.82)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    175 |          1350.11 | 700000 |           33.7418 |        -28.7387 |             114.282 |         -130.58 |             78.7782 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1547.18,num_env_steps_sampled_lifetime=704000.0,env_runners/episode_return_mean=31.931113679785447 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000175)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000175)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:06:47 (running for 00:22:58.88)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    176 |          1357.61 | 704000 |           31.9311 |        -25.4583 |             115.455 |        -137.344 |             79.2782 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1555.62,num_env_steps_sampled_lifetime=708000.0,env_runners/episode_return_mean=33.883859419601265 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000176)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000176)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:06:52 (running for 00:23:03.96)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    177 |          1365.55 | 708000 |           33.8839 |         -25.797 |             114.955 |        -135.049 |             79.7749 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:06:57 (running for 00:23:08.97)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    177 |          1365.55 | 708000 |           33.8839 |         -25.797 |             114.955 |        -135.049 |             79.7749 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1545.62,num_env_steps_sampled_lifetime=712000.0,env_runners/episode_return_mean=27.384002444048434 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000177)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000177)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:07:02 (running for 00:23:14.05)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    178 |          1374.88 | 712000 |            27.384 |        -23.6716 |              111.95 |        -138.959 |             78.0646 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:07:07 (running for 00:23:19.07)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    178 |          1374.88 | 712000 |            27.384 |        -23.6716 |              111.95 |        -138.959 |             78.0646 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1546.88,num_env_steps_sampled_lifetime=716000.0,env_runners/episode_return_mean=20.01030382362095 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000178)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000178)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:07:12 (running for 00:23:24.19)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    179 |          1383.16 | 716000 |           20.0103 |        -22.4585 |              110.74 |        -143.983 |             75.7113 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:07:17 (running for 00:23:29.20)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    179 |          1383.16 | 716000 |           20.0103 |        -22.4585 |              110.74 |        -143.983 |             75.7113 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1545.82,num_env_steps_sampled_lifetime=720000.0,env_runners/episode_return_mean=28.39943092604367 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000179)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000179)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:07:23 (running for 00:23:34.34)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    180 |          1392.37 | 720000 |           28.3994 |        -23.1151 |             114.934 |        -143.862 |             80.4427 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1554.78,num_env_steps_sampled_lifetime=724000.0,env_runners/episode_return_mean=27.0799286147159 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000180)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000180)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:07:28 (running for 00:23:39.37)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    181 |          1400.02 | 724000 |           27.0799 |        -22.0626 |             115.311 |        -147.667 |             81.4976 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:07:33 (running for 00:23:44.47)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    181 |          1400.02 | 724000 |           27.0799 |        -22.0626 |             115.311 |        -147.667 |             81.4976 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1555.86,num_env_steps_sampled_lifetime=728000.0,env_runners/episode_return_mean=28.000594197202645 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000181)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000181)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:07:38 (running for 00:23:49.52)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    182 |           1407.5 | 728000 |           28.0006 |        -22.5263 |              115.93 |         -148.13 |             82.7272 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1554.36,num_env_steps_sampled_lifetime=732000.0,env_runners/episode_return_mean=27.96862154113931 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000182)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000182)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:07:43 (running for 00:23:54.64)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    183 |          1414.59 | 732000 |           27.9686 |        -22.1519 |             116.434 |        -150.197 |             83.8835 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1544.54,num_env_steps_sampled_lifetime=736000.0,env_runners/episode_return_mean=26.477687417585933 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
== Status ==
Current time: 2025-07-11 12:07:48 (running for 00:23:59.67)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    184 |          1422.29 | 736000 |           26.4777 |        -21.8709 |             115.593 |        -147.684 |               80.44 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000183)


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000183)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:07:53 (running for 00:24:04.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    184 |          1422.29 | 736000 |           26.4777 |        -21.8709 |             115.593 |        -147.684 |               80.44 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1551.88,num_env_steps_sampled_lifetime=740000.0,env_runners/episode_return_mean=27.732701749741103 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000184)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000184)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:07:58 (running for 00:24:09.89)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    185 |          1429.92 | 740000 |           27.7327 |        -22.2412 |             117.566 |         -152.14 |             84.5483 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:08:03 (running for 00:24:14.98)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    185 |          1429.92 | 740000 |           27.7327 |        -22.2412 |             117.566 |         -152.14 |             84.5483 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1545.92,num_env_steps_sampled_lifetime=744000.0,env_runners/episode_return_mean=31.886916720770014 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000185)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000185)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:08:08 (running for 00:24:20.04)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    186 |          1438.71 | 744000 |           31.8869 |        -22.4339 |             119.047 |        -148.874 |             84.1478 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1536.34,num_env_steps_sampled_lifetime=748000.0,env_runners/episode_return_mean=24.741022788576885 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
== Status ==
Current time: 2025-07-11 12:08:13 (running for 00:24:25.06)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    187 |          1447.49 | 748000 |            24.741 |        -21.8897 |             117.799 |         -154.84 |             83.6719 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000186)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000186)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:08:18 (running for 00:24:30.11)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    187 |          1447.49 | 748000 |            24.741 |        -21.8897 |             117.799 |         -154.84 |             83.6719 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1536.64,num_env_steps_sampled_lifetime=752000.0,env_runners/episode_return_mean=24.78333727992763 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000187)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000187)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:08:23 (running for 00:24:35.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    188 |          1455.69 | 752000 |           24.7833 |        -21.7791 |             120.071 |        -158.924 |             85.4155 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:08:28 (running for 00:24:40.16)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    188 |          1455.69 | 752000 |           24.7833 |        -21.7791 |             120.071 |        -158.924 |             85.4155 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1534.16,num_env_steps_sampled_lifetime=756000.0,env_runners/episode_return_mean=31.713834850297932 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000188)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000188)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:08:34 (running for 00:24:45.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    189 |          1463.66 | 756000 |           31.7138 |        -20.7817 |             123.368 |        -158.206 |             87.3335 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1524.0,num_env_steps_sampled_lifetime=760000.0,env_runners/episode_return_mean=37.756745636867066 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000189)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000189)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:08:39 (running for 00:24:50.30)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    190 |          1471.73 | 760000 |           37.7567 |        -19.6717 |             123.352 |        -153.846 |             87.9227 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:08:44 (running for 00:24:55.39)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    190 |          1471.73 | 760000 |           37.7567 |        -19.6717 |             123.352 |        -153.846 |             87.9227 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1524.36,num_env_steps_sampled_lifetime=764000.0,env_runners/episode_return_mean=35.99374430302268 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000190)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000190)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:08:49 (running for 00:25:00.49)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    191 |          1479.16 | 764000 |           35.9937 |        -20.2145 |             124.084 |        -156.194 |             88.3187 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1521.96,num_env_steps_sampled_lifetime=768000.0,env_runners/episode_return_mean=41.571076546524154 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000191)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000191)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:08:54 (running for 00:25:05.51)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    192 |          1487.04 | 768000 |           41.5711 |        -19.8137 |             128.942 |        -159.317 |             91.7595 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:08:59 (running for 00:25:10.52)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    192 |          1487.04 | 768000 |           41.5711 |        -19.8137 |             128.942 |        -159.317 |             91.7595 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1520.82,num_env_steps_sampled_lifetime=772000.0,env_runners/episode_return_mean=41.75827602188139 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000192)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000192)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:09:04 (running for 00:25:15.60)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    193 |          1495.18 | 772000 |           41.7583 |        -19.8089 |             130.141 |        -162.094 |             93.5199 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:09:09 (running for 00:25:20.62)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    193 |          1495.18 | 772000 |           41.7583 |        -19.8089 |             130.141 |        -162.094 |             93.5199 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1529.02,num_env_steps_sampled_lifetime=776000.0,env_runners/episode_return_mean=38.40438122814413 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000193)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000193)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:09:14 (running for 00:25:25.69)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    194 |          1503.52 | 776000 |           38.4044 |        -19.8295 |             130.899 |        -168.391 |             95.7257 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1532.14,num_env_steps_sampled_lifetime=780000.0,env_runners/episode_return_mean=40.00430333109303 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000194)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000194)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:09:19 (running for 00:25:30.69)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    195 |          1511.56 | 780000 |           40.0043 |        -20.4107 |             130.526 |        -166.706 |             96.5944 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:09:24 (running for 00:25:35.80)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    195 |          1511.56 | 780000 |           40.0043 |        -20.4107 |             130.526 |        -166.706 |             96.5944 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1532.82,num_env_steps_sampled_lifetime=784000.0,env_runners/episode_return_mean=40.94861254637559 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000195)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000195)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:09:29 (running for 00:25:40.89)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    196 |          1518.89 | 784000 |           40.9486 |        -21.0204 |             131.168 |        -165.921 |             96.7225 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1532.32,num_env_steps_sampled_lifetime=788000.0,env_runners/episode_return_mean=42.72554322237683 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000196)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000196)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:09:34 (running for 00:25:45.96)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    197 |          1525.93 | 788000 |           42.7255 |        -21.2023 |             133.286 |        -167.474 |              98.116 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:09:39 (running for 00:25:50.98)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    197 |          1525.93 | 788000 |           42.7255 |        -21.2023 |             133.286 |        -167.474 |              98.116 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1530.92,num_env_steps_sampled_lifetime=792000.0,env_runners/episode_return_mean=44.759222037662056 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000197)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000197)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:09:44 (running for 00:25:56.05)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    198 |          1533.15 | 792000 |           44.7592 |        -21.8148 |              134.34 |        -166.473 |             98.7068 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1533.4,num_env_steps_sampled_lifetime=796000.0,env_runners/episode_return_mean=39.47943592512147 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000198)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000198)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:09:49 (running for 00:26:01.10)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    199 |          1541.02 | 796000 |           39.4794 |        -21.4756 |              133.04 |        -169.087 |             97.0016 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:09:54 (running for 00:26:06.21)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    199 |          1541.02 | 796000 |           39.4794 |        -21.4756 |              133.04 |        -169.087 |             97.0016 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1543.88,num_env_steps_sampled_lifetime=800000.0,env_runners/episode_return_mean=34.70427559508559 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000199)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000199)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:09:59 (running for 00:26:11.24)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    200 |           1550.1 | 800000 |           34.7043 |        -22.0637 |             132.191 |        -173.948 |             98.5248 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:10:05 (running for 00:26:16.34)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    200 |           1550.1 | 800000 |           34.7043 |        -22.0637 |             132.191 |        -173.948 |             98.5248 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1544.46,num_env_steps_sampled_lifetime=804000.0,env_runners/episode_return_mean=33.5001871866107 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000200)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000200)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:10:10 (running for 00:26:21.44)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    201 |          1558.33 | 804000 |           33.5002 |        -22.5875 |             132.698 |            -176 |             99.3888 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1553.28,num_env_steps_sampled_lifetime=808000.0,env_runners/episode_return_mean=33.96675787703901 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000201)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000201)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:10:15 (running for 00:26:26.47)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    202 |          1566.14 | 808000 |           33.9668 |        -22.7354 |             132.335 |        -174.108 |             98.4752 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1562.32,num_env_steps_sampled_lifetime=812000.0,env_runners/episode_return_mean=37.25898691053847 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000202)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000202)...
== Status ==
Current time: 2025-07-11 12:10:20 (running for 00:26:31.55)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    203 |          1572.95 | 812000 |            37.259 |         -22.665 |             135.041 |        -177.814 |             102.697 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s


[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:10:25 (running for 00:26:36.65)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    203 |          1572.95 | 812000 |            37.259 |         -22.665 |             135.041 |        -177.814 |             102.697 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1575.1,num_env_steps_sampled_lifetime=816000.0,env_runners/episode_return_mean=39.89003639195594 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000203)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000203)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:10:30 (running for 00:26:41.73)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    204 |          1579.86 | 816000 |             39.89 |        -24.2885 |             135.006 |         -173.87 |             103.043 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:10:35 (running for 00:26:46.83)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    204 |          1579.86 | 816000 |             39.89 |        -24.2885 |             135.006 |         -173.87 |             103.043 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1566.5,num_env_steps_sampled_lifetime=820000.0,env_runners/episode_return_mean=41.38331960609976 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000204)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000204)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:10:40 (running for 00:26:51.86)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    205 |          1589.04 | 820000 |           41.3833 |        -24.8063 |             135.169 |        -170.168 |             101.189 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.42,num_env_steps_sampled_lifetime=824000.0,env_runners/episode_return_mean=35.04038130814043 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000205)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000205)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:10:45 (running for 00:26:56.95)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    206 |          1598.07 | 824000 |           35.0404 |        -23.0627 |             132.694 |        -173.523 |             98.9321 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:10:50 (running for 00:27:01.97)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    206 |          1598.07 | 824000 |           35.0404 |        -23.0627 |             132.694 |        -173.523 |             98.9321 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1573.22,num_env_steps_sampled_lifetime=828000.0,env_runners/episode_return_mean=31.551818424578933 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000206)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000206)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:10:55 (running for 00:27:07.13)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    207 |          1606.38 | 828000 |           31.5518 |        -22.9962 |             131.695 |         -175.27 |             98.1227 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:11:00 (running for 00:27:12.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    207 |          1606.38 | 828000 |           31.5518 |        -22.9962 |             131.695 |         -175.27 |             98.1227 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1573.68,num_env_steps_sampled_lifetime=832000.0,env_runners/episode_return_mean=27.919987046732334 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000207)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000207)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:11:06 (running for 00:27:17.27)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    208 |          1615.57 | 832000 |             27.92 |        -23.5179 |             130.019 |        -174.879 |             96.2977 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1572.62,num_env_steps_sampled_lifetime=836000.0,env_runners/episode_return_mean=26.137454821783564 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000208)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000208)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:11:11 (running for 00:27:22.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    209 |          1623.34 | 836000 |           26.1375 |        -23.6213 |             131.074 |        -177.668 |             96.3533 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:11:16 (running for 00:27:27.39)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    209 |          1623.34 | 836000 |           26.1375 |        -23.6213 |             131.074 |        -177.668 |             96.3533 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1582.26,num_env_steps_sampled_lifetime=840000.0,env_runners/episode_return_mean=27.30157611291824 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000209)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000209)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:11:21 (running for 00:27:32.39)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    210 |          1630.19 | 840000 |           27.3016 |        -23.7413 |             130.819 |        -177.207 |             97.4304 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1583.88,num_env_steps_sampled_lifetime=844000.0,env_runners/episode_return_mean=27.802671083542393 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000210)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000210)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:11:26 (running for 00:27:37.49)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    211 |          1637.55 | 844000 |           27.8027 |        -24.3421 |             130.552 |        -175.454 |             97.0465 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:11:31 (running for 00:27:42.51)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    211 |          1637.55 | 844000 |           27.8027 |        -24.3421 |             130.552 |        -175.454 |             97.0465 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1587.98,num_env_steps_sampled_lifetime=848000.0,env_runners/episode_return_mean=25.04545587060157 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000211)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000211)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:11:36 (running for 00:27:47.69)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    212 |          1645.78 | 848000 |           25.0455 |        -24.8806 |             129.048 |         -174.48 |             95.3583 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:11:41 (running for 00:27:52.71)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    212 |          1645.78 | 848000 |           25.0455 |        -24.8806 |             129.048 |         -174.48 |             95.3583 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1578.14,num_env_steps_sampled_lifetime=852000.0,env_runners/episode_return_mean=25.72165973278406 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000212)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000212)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:11:46 (running for 00:27:57.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    213 |          1655.32 | 852000 |           25.7217 |        -25.0265 |             129.306 |        -171.205 |             92.6467 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:11:51 (running for 00:28:02.80)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    213 |          1655.32 | 852000 |           25.7217 |        -25.0265 |             129.306 |        -171.205 |             92.6467 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1575.32,num_env_steps_sampled_lifetime=856000.0,env_runners/episode_return_mean=30.07471548462875 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000213)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000213)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:11:56 (running for 00:28:07.93)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    214 |          1664.01 | 856000 |           30.0747 |        -24.3192 |             131.051 |        -171.094 |              94.437 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1564.22,num_env_steps_sampled_lifetime=860000.0,env_runners/episode_return_mean=21.296222253042643 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000214)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000214)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:01 (running for 00:28:12.98)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    215 |           1672.3 | 860000 |           21.2962 |         -22.575 |             129.267 |        -177.756 |             92.3601 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:12:06 (running for 00:28:18.08)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    215 |           1672.3 | 860000 |           21.2962 |         -22.575 |             129.267 |        -177.756 |             92.3601 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1562.84,num_env_steps_sampled_lifetime=864000.0,env_runners/episode_return_mean=23.753534129227774 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000215)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000215)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:11 (running for 00:28:23.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    216 |           1679.2 | 864000 |           23.7535 |        -21.4285 |             130.215 |        -178.592 |             93.5586 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1553.14,num_env_steps_sampled_lifetime=868000.0,env_runners/episode_return_mean=23.927946292352217 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000216)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000216)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:16 (running for 00:28:28.26)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    217 |          1687.77 | 868000 |           23.9279 |        -22.3371 |             130.845 |        -176.385 |             91.8055 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:12:22 (running for 00:28:33.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    217 |          1687.77 | 868000 |           23.9279 |        -22.3371 |             130.845 |        -176.385 |             91.8055 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1545.14,num_env_steps_sampled_lifetime=872000.0,env_runners/episode_return_mean=27.638104295140796 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000217)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000217)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:27 (running for 00:28:38.39)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    218 |          1695.75 | 872000 |           27.6381 |        -22.2103 |             131.282 |        -172.736 |              91.302 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:12:32 (running for 00:28:43.40)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    218 |          1695.75 | 872000 |           27.6381 |        -22.2103 |             131.282 |        -172.736 |              91.302 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1545.56,num_env_steps_sampled_lifetime=876000.0,env_runners/episode_return_mean=32.684702182535055 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000218)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000218)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:37 (running for 00:28:48.57)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    219 |          1704.37 | 876000 |           32.6847 |        -21.8736 |             133.186 |        -172.607 |             93.9803 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1544.86,num_env_steps_sampled_lifetime=880000.0,env_runners/episode_return_mean=35.9334526477094 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000219)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000219)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:42 (running for 00:28:53.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    220 |           1710.4 | 880000 |           35.9335 |        -21.4532 |             133.742 |        -170.949 |             94.5936 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1536.54,num_env_steps_sampled_lifetime=884000.0,env_runners/episode_return_mean=34.953401510331275 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000220)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000220)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:47 (running for 00:28:58.75)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    221 |          1715.95 | 884000 |           34.9534 |         -20.481 |             132.099 |        -170.498 |             93.8329 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1540.28,num_env_steps_sampled_lifetime=888000.0,env_runners/episode_return_mean=35.61033833522296 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000221)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000221)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:52 (running for 00:29:03.82)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    222 |          1721.56 | 888000 |           35.6103 |        -21.4938 |             132.621 |        -170.635 |             95.1183 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1538.62,num_env_steps_sampled_lifetime=892000.0,env_runners/episode_return_mean=37.32125003656972 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000222)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000222)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:12:57 (running for 00:29:08.93)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    223 |          1727.02 | 892000 |           37.3213 |        -21.4066 |             132.791 |        -170.288 |             96.2252 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1538.38,num_env_steps_sampled_lifetime=896000.0,env_runners/episode_return_mean=31.61209173752371 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000223)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000223)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:02 (running for 00:29:13.93)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    224 |          1732.77 | 896000 |           31.6121 |        -19.5295 |             132.209 |        -177.569 |             96.5024 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1537.86,num_env_steps_sampled_lifetime=900000.0,env_runners/episode_return_mean=38.153819858904384 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000224)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000224)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:07 (running for 00:29:19.03)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    225 |          1738.32 | 900000 |           38.1538 |        -20.9992 |             135.658 |        -176.726 |              100.22 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1537.74,num_env_steps_sampled_lifetime=904000.0,env_runners/episode_return_mean=43.50800886889364 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000225)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000225)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:12 (running for 00:29:24.10)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    226 |          1743.82 | 904000 |            43.508 |        -21.4644 |             138.293 |        -174.968 |             101.647 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:13:17 (running for 00:29:29.20)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    226 |          1743.82 | 904000 |            43.508 |        -21.4644 |             138.293 |        -174.968 |             101.647 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1538.38,num_env_steps_sampled_lifetime=908000.0,env_runners/episode_return_mean=48.23245414869809 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000226)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000226)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:23 (running for 00:29:34.27)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    227 |          1749.61 | 908000 |           48.2325 |        -21.5091 |             139.614 |        -173.245 |             103.373 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1531.36,num_env_steps_sampled_lifetime=912000.0,env_runners/episode_return_mean=47.38923907576158 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000227)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000227)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:28 (running for 00:29:39.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    228 |          1755.75 | 912000 |           47.3892 |        -23.5146 |             137.723 |        -168.473 |             101.655 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1529.9,num_env_steps_sampled_lifetime=916000.0,env_runners/episode_return_mean=48.05029817550878 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000228)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000228)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:33 (running for 00:29:44.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    229 |           1761.4 | 916000 |           48.0503 |        -22.1506 |             139.865 |        -172.026 |             102.362 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1530.12,num_env_steps_sampled_lifetime=920000.0,env_runners/episode_return_mean=43.13297709394988 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000229)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000229)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:38 (running for 00:29:49.53)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    230 |          1767.23 | 920000 |            43.133 |        -21.9843 |             138.435 |        -175.283 |             101.965 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1539.0,num_env_steps_sampled_lifetime=924000.0,env_runners/episode_return_mean=40.56963417036464 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000230)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000230)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:43 (running for 00:29:54.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    231 |           1772.8 | 924000 |           40.5696 |        -23.6339 |             138.402 |        -179.238 |              105.04 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1542.52,num_env_steps_sampled_lifetime=928000.0,env_runners/episode_return_mean=46.929096833847325 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000231)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000231)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:48 (running for 00:29:59.67)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    232 |          1778.56 | 928000 |           46.9291 |        -24.6371 |             139.597 |        -175.977 |             107.947 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1543.32,num_env_steps_sampled_lifetime=932000.0,env_runners/episode_return_mean=45.006722746330325 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000232)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000232)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:13:53 (running for 00:30:04.68)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    233 |          1784.22 | 932000 |           45.0067 |          -25.55 |             140.112 |        -176.976 |              107.42 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:13:58 (running for 00:30:09.78)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    233 |          1784.22 | 932000 |           45.0067 |          -25.55 |             140.112 |        -176.976 |              107.42 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1543.98,num_env_steps_sampled_lifetime=936000.0,env_runners/episode_return_mean=47.268678374453046 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000233)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000233)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:03 (running for 00:30:14.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    234 |          1789.86 | 936000 |           47.2687 |         -25.819 |             140.641 |        -175.342 |             107.788 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1545.72,num_env_steps_sampled_lifetime=940000.0,env_runners/episode_return_mean=44.39257679957956 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in

[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000234)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000234)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:08 (running for 00:30:19.94)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    235 |          1795.47 | 940000 |           44.3926 |        -27.7299 |             139.628 |        -172.167 |             104.662 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1537.12,num_env_steps_sampled_lifetime=944000.0,env_runners/episode_return_mean=47.1232250170737 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000235)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000235)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:13 (running for 00:30:25.00)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    236 |          1801.37 | 944000 |           47.1232 |        -28.1245 |             138.825 |        -166.568 |             102.991 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1565.12,num_env_steps_sampled_lifetime=948000.0,env_runners/episode_return_mean=50.71080440268257 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000236)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000236)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:18 (running for 00:30:30.11)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    237 |          1806.74 | 948000 |           50.7108 |        -29.5801 |              139.81 |        -167.156 |             107.637 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.4,num_env_steps_sampled_lifetime=952000.0,env_runners/episode_return_mean=49.89868919182376 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000237)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000237)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:23 (running for 00:30:35.13)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    238 |          1812.21 | 952000 |           49.8987 |        -29.6758 |             139.989 |        -167.313 |             106.899 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.88,num_env_steps_sampled_lifetime=956000.0,env_runners/episode_return_mean=55.62153337211739 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000238)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000238)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:29 (running for 00:30:40.30)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    239 |          1817.74 | 956000 |           55.6215 |        -30.0478 |             141.628 |        -164.016 |             108.058 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1564.62,num_env_steps_sampled_lifetime=960000.0,env_runners/episode_return_mean=47.15869504699273 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000239)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000239)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:34 (running for 00:30:45.38)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    240 |          1823.56 | 960000 |           47.1587 |        -30.5811 |             139.805 |        -167.613 |             105.548 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1554.24,num_env_steps_sampled_lifetime=964000.0,env_runners/episode_return_mean=55.79997381156519 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000240)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000240)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:39 (running for 00:30:50.48)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    241 |          1829.41 | 964000 |              55.8 |        -30.7267 |             141.509 |        -160.639 |             105.656 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:14:44 (running for 00:30:55.49)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    241 |          1829.41 | 964000 |              55.8 |        -30.7267 |             141.509 |        -160.639 |             105.656 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1551.6,num_env_steps_sampled_lifetime=968000.0,env_runners/episode_return_mean=56.289190250643735 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000241)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000241)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:49 (running for 00:31:00.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    242 |          1835.03 | 968000 |           56.2892 |        -30.9639 |             142.634 |        -160.942 |             105.561 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1557.78,num_env_steps_sampled_lifetime=972000.0,env_runners/episode_return_mean=61.33338698617088 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000242)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000242)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:54 (running for 00:31:05.76)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    243 |          1840.37 | 972000 |           61.3334 |         -31.952 |             144.907 |         -157.85 |             106.228 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1560.12,num_env_steps_sampled_lifetime=976000.0,env_runners/episode_return_mean=62.75560652930692 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000243)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000243)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:14:59 (running for 00:31:10.78)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    244 |          1845.75 | 976000 |           62.7556 |        -31.5733 |             144.267 |        -156.686 |             106.748 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.66,num_env_steps_sampled_lifetime=980000.0,env_runners/episode_return_mean=63.60981592440512 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000244)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000244)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:04 (running for 00:31:15.85)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    245 |          1851.17 | 980000 |           63.6098 |        -32.9872 |             143.344 |        -152.936 |             106.189 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1553.56,num_env_steps_sampled_lifetime=984000.0,env_runners/episode_return_mean=68.57455029292514 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000245)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000245)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:09 (running for 00:31:20.97)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    246 |          1857.24 | 984000 |           68.5746 |        -32.3541 |              144.86 |        -148.791 |              104.86 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1555.92,num_env_steps_sampled_lifetime=988000.0,env_runners/episode_return_mean=72.10087856911356 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000246)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000246)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:14 (running for 00:31:26.03)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    247 |          1862.74 | 988000 |           72.1009 |        -32.6587 |             146.267 |        -147.275 |             105.767 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1565.96,num_env_steps_sampled_lifetime=992000.0,env_runners/episode_return_mean=73.85765914362477 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000247)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000247)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:19 (running for 00:31:31.10)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    248 |          1868.26 | 992000 |           73.8577 |        -32.3821 |             148.039 |        -149.161 |             107.362 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1569.26,num_env_steps_sampled_lifetime=996000.0,env_runners/episode_return_mean=79.63288953120562 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000248)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000248)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:24 (running for 00:31:36.16)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |     ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    249 |          1873.78 | 996000 |           79.6329 |        -33.9737 |             150.059 |        -145.708 |             109.256 |
+---------------------+----------+----------------------+--------+------------------+--------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1569.32,num_env_steps_sampled_lifetime=1000000.0,env_runners/episode_return_mean=81.09893968802122 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000249)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000249)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:29 (running for 00:31:41.25)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    250 |          1879.32 | 1e+06 |           81.0989 |        -33.1725 |             150.268 |        -145.541 |             109.544 |
+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1565.46,num_env_steps_sampled_lifetime=1004000.0,env_runners/episode_return_mean=84.76390357409564 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000250)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000250)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:35 (running for 00:31:46.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    251 |           1884.9 | 1.004e+06 |           84.7639 |          -32.13 |             153.505 |         -146.24 |              109.63 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:15:40 (running for 00:31:51.40)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    251 |           1884.9 | 1.004e+06 |           84.7639 |          -32.13 |             153.505 |         -146.24 |              109.63 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1573.2,num_env_steps_sampled_lifetime=1008000.0,env_runners/episode_return_mean=76.10824136517164 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000251)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000251)...
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:45 (running for 00:31:56.40)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    252 |          1890.35 | 1.008e+06 |           76.1082 |        -32.1785 |             151.458 |        -151.506 |             108.335 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1580.8,num_env_steps_sampled_lifetime=1012000.0,env_runners/episode_return_mean=82.03407696683902 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000252)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000252)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:50 (running for 00:32:01.54)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    253 |          1895.85 | 1.012e+06 |           82.0341 |        -32.4988 |             152.379 |        -146.843 |             108.997 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1571.34,num_env_steps_sampled_lifetime=1016000.0,env_runners/episode_return_mean=83.18833482923549 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000253)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000253)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:15:55 (running for 00:32:06.62)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    254 |          1901.78 | 1.016e+06 |           83.1883 |        -32.6796 |             152.196 |        -143.066 |             106.737 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1574.04,num_env_steps_sampled_lifetime=1020000.0,env_runners/episode_return_mean=89.21239542124422 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000254)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000254)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:00 (running for 00:32:11.70)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    255 |          1907.31 | 1.02e+06 |           89.2124 |        -33.7826 |             153.839 |        -141.021 |             110.177 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1582.92,num_env_steps_sampled_lifetime=1024000.0,env_runners/episode_return_mean=87.97881361721109 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000255)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000255)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:05 (running for 00:32:16.77)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    256 |          1913.13 | 1.024e+06 |           87.9788 |        -32.9301 |             155.904 |        -146.733 |             111.738 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1571.7,num_env_steps_sampled_lifetime=1028000.0,env_runners/episode_return_mean=86.60528652512113 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000256)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000256)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:10 (running for 00:32:21.82)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    257 |          1918.62 | 1.028e+06 |           86.6053 |        -31.0154 |              156.22 |        -150.421 |             111.822 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1563.76,num_env_steps_sampled_lifetime=1032000.0,env_runners/episode_return_mean=91.3571852012242 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000257)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000257)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:15 (running for 00:32:26.84)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    258 |          1924.89 | 1.032e+06 |           91.3572 |        -29.9261 |             156.458 |         -146.38 |             111.205 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:16:20 (running for 00:32:31.96)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    258 |          1924.89 | 1.032e+06 |           91.3572 |        -29.9261 |             156.458 |         -146.38 |             111.205 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1553.62,num_env_steps_sampled_lifetime=1036000.0,env_runners/episode_return_mean=96.66277377062013 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000258)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000258)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:25 (running for 00:32:37.05)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    259 |          1931.04 | 1.036e+06 |           96.6628 |        -29.3006 |             157.266 |        -142.113 |              110.81 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1562.48,num_env_steps_sampled_lifetime=1040000.0,env_runners/episode_return_mean=91.60002151806093 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000259)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000259)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:30 (running for 00:32:42.13)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    260 |          1937.17 | 1.04e+06 |              91.6 |        -29.4443 |             156.507 |        -146.594 |             111.131 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1555.0,num_env_steps_sampled_lifetime=1044000.0,env_runners/episode_return_mean=84.01688534406588 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000260)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000260)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:35 (running for 00:32:47.18)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    261 |          1942.77 | 1.044e+06 |           84.0169 |        -29.6335 |             154.376 |        -146.941 |             106.215 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1554.68,num_env_steps_sampled_lifetime=1048000.0,env_runners/episode_return_mean=86.43335101687688 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000261)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000261)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:40 (running for 00:32:52.25)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    262 |          1948.89 | 1.048e+06 |           86.4334 |        -29.0941 |             153.791 |        -144.139 |             105.875 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1535.18,num_env_steps_sampled_lifetime=1052000.0,env_runners/episode_return_mean=84.9829383262084 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000262)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000262)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:46 (running for 00:32:57.33)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    263 |          1954.72 | 1.052e+06 |           84.9829 |        -27.8338 |             153.051 |        -144.621 |             104.386 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
== Status ==
Current time: 2025-07-11 12:16:51 (running for 00:33:02.45)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    263 |          1954.72 | 1.052e+06 |           84.9829 |        -27.8338 |             153.051 |        -144.621 |             104.386 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1523.7,num_env_steps_sampled_lifetime=1056000.0,env_runners/episode_return_mean=86.21406094694237 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000263)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000263)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:16:56 (running for 00:33:07.52)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    264 |          1960.66 | 1.056e+06 |           86.2141 |        -25.9617 |             153.447 |        -144.521 |              103.25 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1535.42,num_env_steps_sampled_lifetime=1060000.0,env_runners/episode_return_mean=78.62591153919244 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000264)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000264)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:17:01 (running for 00:33:12.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    265 |          1966.24 | 1.06e+06 |           78.6259 |        -26.6347 |             150.201 |        -149.125 |             104.184 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1533.74,num_env_steps_sampled_lifetime=1064000.0,env_runners/episode_return_mean=76.39377695568346 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000265)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000265)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:17:06 (running for 00:33:17.58)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    266 |          1971.72 | 1.064e+06 |           76.3938 |        -27.2176 |             150.493 |        -150.394 |             103.512 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1541.26,num_env_steps_sampled_lifetime=1068000.0,env_runners/episode_return_mean=84.55375799683814 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000266)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000266)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
== Status ==
Current time: 2025-07-11 12:17:11 (running for 00:33:22.68)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    267 |          1977.07 | 1.068e+06 |           84.5538 |        -28.5843 |             153.315 |         -146.16 |             105.983 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1537.56,num_env_steps_sampled_lifetime=1072000.0,env_runners/episode_return_mean=83.18690027558425 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000267)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000267)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:17:16 (running for 00:33:27.69)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    268 |          1982.73 | 1.072e+06 |           83.1869 |        -27.2144 |             152.465 |        -147.285 |             105.221 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1537.72,num_env_steps_sampled_lifetime=1076000.0,env_runners/episode_return_mean=80.74430525050589 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000268)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000268)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:17:21 (running for 00:33:32.80)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    269 |          1988.19 | 1.076e+06 |           80.7443 |        -26.9729 |             150.625 |        -146.496 |             103.588 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1538.34,num_env_steps_sampled_lifetime=1080000.0,env_runners/episode_return_mean=79.06574948349731 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000269)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000269)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:17:26 (running for 00:33:37.88)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    270 |          1994.33 | 1.08e+06 |           79.0657 |        -27.2507 |             150.186 |        -146.277 |             102.407 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1519.76,num_env_steps_sampled_lifetime=1084000.0,env_runners/episode_return_mean=83.51801956461102 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000270)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000270)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:17:31 (running for 00:33:42.93)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    271 |          2000.41 | 1.084e+06 |            83.518 |        -27.1075 |             149.511 |         -137.31 |              98.425 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:17:36 (running for 00:33:47.94)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    271 |          2000.41 | 1.084e+06 |            83.518 |        -27.1075 |             149.511 |         -137.31 |              98.425 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1511.4,num_env_steps_sampled_lifetime=1088000.0,env_runners/episode_return_mean=78.30414035697463 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000271)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000271)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:17:41 (running for 00:33:53.05)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    272 |          2006.08 | 1.088e+06 |           78.3041 |        -26.3391 |             148.971 |        -141.546 |             97.2187 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1520.32,num_env_steps_sampled_lifetime=1092000.0,env_runners/episode_return_mean=76.96325653454899 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000272)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000272)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:17:46 (running for 00:33:58.12)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    273 |           2012.3 | 1.092e+06 |           76.9633 |        -26.1332 |             148.993 |        -145.478 |             99.5822 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1510.76,num_env_steps_sampled_lifetime=1096000.0,env_runners/episode_return_mean=75.00037913345396 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000273)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000273)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:17:51 (running for 00:34:03.19)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    274 |          2017.71 | 1.096e+06 |           75.0004 |        -25.7935 |             148.713 |        -146.243 |             98.3246 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1509.32,num_env_steps_sampled_lifetime=1100000.0,env_runners/episode_return_mean=74.55117220188745 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000274)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000274)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:17:57 (running for 00:34:08.28)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |      ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    275 |          2023.25 | 1.1e+06 |           74.5512 |        -24.9699 |              147.45 |        -144.954 |             97.0254 |
+---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1511.42,num_env_steps_sampled_lifetime=1104000.0,env_runners/episode_return_mean=75.40150892741657 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000275)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000275)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:02 (running for 00:34:13.35)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    276 |          2028.76 | 1.104e+06 |           75.4015 |        -25.4912 |             147.603 |        -144.491 |             97.7807 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1509.98,num_env_steps_sampled_lifetime=1108000.0,env_runners/episode_return_mean=74.64103692399084 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_le

[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000276)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000276)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:07 (running for 00:34:18.42)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    277 |           2034.3 | 1.108e+06 |            74.641 |         -26.332 |             146.506 |        -143.033 |             97.5008 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1533.44,num_env_steps_sampled_lifetime=1112000.0,env_runners/episode_return_mean=68.81649840866262 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000277)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000277)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:12 (running for 00:34:23.55)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    278 |          2039.64 | 1.112e+06 |           68.8165 |        -28.7217 |             146.209 |        -147.557 |             98.8865 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1533.24,num_env_steps_sampled_lifetime=1116000.0,env_runners/episode_return_mean=72.83651413884179 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000278)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000278)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:17 (running for 00:34:28.57)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    279 |          2045.24 | 1.116e+06 |           72.8365 |        -29.1302 |             149.158 |        -147.741 |              100.55 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1544.68,num_env_steps_sampled_lifetime=1120000.0,env_runners/episode_return_mean=74.34267383062526 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000279)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000279)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:22 (running for 00:34:33.74)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    280 |          2050.83 | 1.12e+06 |           74.3427 |        -29.3391 |              149.86 |        -148.667 |             102.489 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:18:27 (running for 00:34:38.75)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    280 |          2050.83 | 1.12e+06 |           74.3427 |        -29.3391 |              149.86 |        -148.667 |             102.489 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1542.68,num_env_steps_sampled_lifetime=1124000.0,env_runners/episode_return_mean=73.97342222119526 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000280)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000280)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:32 (running for 00:34:43.92)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    281 |          2056.46 | 1.124e+06 |           73.9734 |        -32.4006 |             150.548 |        -147.104 |              102.93 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1556.5,num_env_steps_sampled_lifetime=1128000.0,env_runners/episode_return_mean=69.91419771862974 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000281)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000281)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:37 (running for 00:34:49.01)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    282 |          2062.01 | 1.128e+06 |           69.9142 |        -33.8666 |              151.18 |         -151.54 |             104.141 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1542.46,num_env_steps_sampled_lifetime=1132000.0,env_runners/episode_return_mean=66.67212497594436 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000282)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000282)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:42 (running for 00:34:54.03)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    283 |          2068.18 | 1.132e+06 |           66.6721 |         -33.373 |             151.799 |        -152.031 |             100.278 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1552.28,num_env_steps_sampled_lifetime=1136000.0,env_runners/episode_return_mean=65.30633308612443 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000283)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000283)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:47 (running for 00:34:59.07)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    284 |          2073.87 | 1.136e+06 |           65.3063 |         -34.009 |             152.502 |        -154.304 |             101.118 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1550.86,num_env_steps_sampled_lifetime=1140000.0,env_runners/episode_return_mean=68.73371606958969 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000284)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000284)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:52 (running for 00:35:04.20)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    285 |          2079.42 | 1.14e+06 |           68.7337 |        -33.5756 |             153.308 |        -153.825 |             102.827 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1543.16,num_env_steps_sampled_lifetime=1144000.0,env_runners/episode_return_mean=65.84135747504004 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000285)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000285)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:18:57 (running for 00:35:09.26)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    286 |          2085.15 | 1.144e+06 |           65.8414 |        -31.7164 |             152.018 |        -157.367 |             102.907 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1543.74,num_env_steps_sampled_lifetime=1148000.0,env_runners/episode_return_mean=61.36011140772537 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000286)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000286)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:03 (running for 00:35:14.37)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    287 |          2090.69 | 1.148e+06 |           61.3601 |         -31.733 |             149.984 |        -158.852 |             101.961 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:19:08 (running for 00:35:19.38)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    287 |          2090.69 | 1.148e+06 |           61.3601 |         -31.733 |             149.984 |        -158.852 |             101.961 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1543.46,num_env_steps_sampled_lifetime=1152000.0,env_runners/episode_return_mean=63.755058438544836 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000287)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000287)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:13 (running for 00:35:24.51)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    288 |          2096.57 | 1.152e+06 |           63.7551 |        -32.1239 |             152.748 |        -160.674 |             103.805 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1523.82,num_env_steps_sampled_lifetime=1156000.0,env_runners/episode_return_mean=72.74341630908233 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000288)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000288)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:18 (running for 00:35:29.59)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    289 |          2102.73 | 1.156e+06 |           72.7434 |        -31.6892 |              155.67 |        -154.604 |             103.367 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1547.62,num_env_steps_sampled_lifetime=1160000.0,env_runners/episode_return_mean=63.368619880341065 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000289)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000289)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:23 (running for 00:35:34.72)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    290 |          2108.88 | 1.16e+06 |           63.3686 |        -32.7914 |             153.177 |          -162.5 |             105.483 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1536.42,num_env_steps_sampled_lifetime=1164000.0,env_runners/episode_return_mean=70.38446966735702 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000290)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000290)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:28 (running for 00:35:39.74)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    291 |          2114.47 | 1.164e+06 |           70.3845 |        -32.2538 |             155.831 |         -159.91 |             106.718 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1537.34,num_env_steps_sampled_lifetime=1168000.0,env_runners/episode_return_mean=70.00974259033406 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000291)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000291)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:33 (running for 00:35:44.85)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    292 |          2120.52 | 1.168e+06 |           70.0097 |        -32.5882 |             156.319 |        -160.456 |             106.735 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1535.72,num_env_steps_sampled_lifetime=1172000.0,env_runners/episode_return_mean=70.48520433556907 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000292)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000292)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:38 (running for 00:35:49.92)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    293 |          2126.26 | 1.172e+06 |           70.4852 |        -31.7846 |             157.233 |        -162.119 |             107.156 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:19:43 (running for 00:35:55.05)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    293 |          2126.26 | 1.172e+06 |           70.4852 |        -31.7846 |             157.233 |        -162.119 |             107.156 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1534.88,num_env_steps_sampled_lifetime=1176000.0,env_runners/episode_return_mean=68.6796617113472 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000293)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000293)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:48 (running for 00:36:00.14)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    294 |          2131.71 | 1.176e+06 |           68.6797 |        -31.6678 |             156.023 |         -160.28 |             104.604 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1537.46,num_env_steps_sampled_lifetime=1180000.0,env_runners/episode_return_mean=66.03737201171127 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000294)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000294)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:53 (running for 00:36:05.21)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    295 |           2137.3 | 1.18e+06 |           66.0374 |        -31.3581 |             152.936 |        -156.272 |             100.731 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1536.88,num_env_steps_sampled_lifetime=1184000.0,env_runners/episode_return_mean=65.31286545541865 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000295)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000295)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:19:58 (running for 00:36:10.23)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    296 |           2142.9 | 1.184e+06 |           65.3129 |        -30.0018 |             153.342 |        -158.976 |             100.948 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1534.22,num_env_steps_sampled_lifetime=1188000.0,env_runners/episode_return_mean=63.75149013366028 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000296)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000296)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:04 (running for 00:36:15.39)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    297 |           2148.5 | 1.188e+06 |           63.7515 |        -28.8714 |             152.032 |        -160.481 |             101.071 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1536.28,num_env_steps_sampled_lifetime=1192000.0,env_runners/episode_return_mean=61.28276699095337 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000297)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000297)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:09 (running for 00:36:20.48)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    298 |          2154.05 | 1.192e+06 |           61.2828 |         -29.319 |             149.118 |        -158.246 |             99.7299 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1532.06,num_env_steps_sampled_lifetime=1196000.0,env_runners/episode_return_mean=59.59560802462369 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000298)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000298)... Done. 0.0s
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§
== Status ==
Current time: 2025-07-11 12:20:14 (running for 00:36:25.56)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    299 |          2159.56 | 1.196e+06 |           59.5956 |        -28.0644 |             149.514 |        -162.237 |             100.382 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1535.1,num_env_steps_sampled_lifetime=1200000.0,env_runners/episode_return_mean=55.58417795462827 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000299)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000299)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:19 (running for 00:36:30.59)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |      ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    300 |          2165.27 | 1.2e+06 |           55.5842 |         -25.388 |             148.222 |        -166.132 |             98.8826 |
+---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1533.68,num_env_steps_sampled_lifetime=1204000.0,env_runners/episode_return_mean=56.441819743703 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000300)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000300)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:24 (running for 00:36:35.74)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    301 |          2170.65 | 1.204e+06 |           56.4418 |        -24.7845 |             148.067 |        -167.538 |             100.698 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1532.9,num_env_steps_sampled_lifetime=1208000.0,env_runners/episode_return_mean=57.51788927691865 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000301)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000301)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:29 (running for 00:36:40.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    302 |          2176.57 | 1.208e+06 |           57.5179 |        -24.4766 |             147.924 |        -166.754 |             100.824 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:20:34 (running for 00:36:45.92)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    302 |          2176.57 | 1.208e+06 |           57.5179 |        -24.4766 |             147.924 |        -166.754 |             100.824 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1540.48,num_env_steps_sampled_lifetime=1212000.0,env_runners/episode_return_mean=59.838166304706505 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000302)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000302)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:39 (running for 00:36:51.01)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    303 |          2182.13 | 1.212e+06 |           59.8382 |        -23.0612 |             150.633 |        -172.293 |             104.559 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1542.76,num_env_steps_sampled_lifetime=1216000.0,env_runners/episode_return_mean=57.08956654928859 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000303)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000303)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:44 (running for 00:36:56.04)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    304 |          2187.79 | 1.216e+06 |           57.0896 |          -23.28 |             150.036 |        -172.776 |             103.109 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1541.68,num_env_steps_sampled_lifetime=1220000.0,env_runners/episode_return_mean=54.07525678311551 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000304)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000304)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:49 (running for 00:37:01.09)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    305 |          2193.21 | 1.22e+06 |           54.0753 |        -23.9234 |             150.126 |        -174.329 |             102.202 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1544.24,num_env_steps_sampled_lifetime=1224000.0,env_runners/episode_return_mean=48.2957567979465 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000305)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000305)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:54 (running for 00:37:06.17)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    306 |          2198.61 | 1.224e+06 |           48.2958 |        -25.2012 |             147.959 |        -174.641 |             100.179 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1551.74,num_env_steps_sampled_lifetime=1228000.0,env_runners/episode_return_mean=54.72550435225208 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000306)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000306)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:20:59 (running for 00:37:11.19)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    307 |          2204.41 | 1.228e+06 |           54.7255 |        -25.8132 |             148.797 |        -169.268 |             101.009 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1552.2,num_env_steps_sampled_lifetime=1232000.0,env_runners/episode_return_mean=56.9099325350895 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_ini
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000307)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000307)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:05 (running for 00:37:16.33)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    308 |          2210.08 | 1.232e+06 |           56.9099 |        -24.9273 |              148.33 |        -166.953 |             100.461 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1561.58,num_env_steps_sampled_lifetime=1236000.0,env_runners/episode_return_mean=51.432491517884756 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000308)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000308)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:10 (running for 00:37:21.38)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    309 |           2215.5 | 1.236e+06 |           51.4325 |        -25.9708 |              148.01 |        -172.479 |             101.872 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1571.52,num_env_steps_sampled_lifetime=1240000.0,env_runners/episode_return_mean=50.539842848916265 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000309)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000309)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:15 (running for 00:37:26.45)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    310 |           2221.1 | 1.24e+06 |           50.5398 |        -25.9099 |             147.329 |        -174.817 |             103.938 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1571.84,num_env_steps_sampled_lifetime=1244000.0,env_runners/episode_return_mean=53.39974845804201 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000310)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000310)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:20 (running for 00:37:31.50)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    311 |          2226.92 | 1.244e+06 |           53.3997 |        -25.8727 |             148.248 |        -173.132 |             104.157 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:21:25 (running for 00:37:36.72)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    311 |          2226.92 | 1.244e+06 |           53.3997 |        -25.8727 |             148.248 |        -173.132 |             104.157 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1559.1,num_env_steps_sampled_lifetime=1248000.0,env_runners/episode_return_mean=61.222814280290876 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000311)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 6x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000311)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:30 (running for 00:37:41.79)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    312 |          2233.55 | 1.248e+06 |           61.2228 |        -24.6476 |             150.795 |         -169.64 |             104.716 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1561.24,num_env_steps_sampled_lifetime=1252000.0,env_runners/episode_return_mean=62.06315727559894 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000312)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000312)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:35 (running for 00:37:46.90)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    313 |          2239.69 | 1.252e+06 |           62.0632 |        -25.7578 |             150.621 |        -168.078 |             105.278 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1548.7,num_env_steps_sampled_lifetime=1256000.0,env_runners/episode_return_mean=67.82327507345495 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000313)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000313)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:40 (running for 00:37:51.92)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    314 |          2245.29 | 1.256e+06 |           67.8233 |         -25.935 |             154.983 |        -169.673 |             108.448 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1547.62,num_env_steps_sampled_lifetime=1260000.0,env_runners/episode_return_mean=66.43476424946454 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000314)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000314)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:45 (running for 00:37:57.08)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    315 |          2250.87 | 1.26e+06 |           66.4348 |        -25.9409 |             155.403 |         -171.86 |             108.833 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1549.58,num_env_steps_sampled_lifetime=1264000.0,env_runners/episode_return_mean=66.35023190999425 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000315)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000315)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:50 (running for 00:38:02.09)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    316 |          2256.34 | 1.264e+06 |           66.3502 |        -27.1274 |              156.78 |        -172.555 |             109.253 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1550.78,num_env_steps_sampled_lifetime=1268000.0,env_runners/episode_return_mean=67.70697896324364 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000316)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000316)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:21:55 (running for 00:38:07.24)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    317 |          2261.82 | 1.268e+06 |            67.707 |        -26.8749 |             157.182 |        -172.007 |             109.408 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1552.46,num_env_steps_sampled_lifetime=1272000.0,env_runners/episode_return_mean=69.12284327854515 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000317)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000317)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:01 (running for 00:38:12.30)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    318 |          2267.36 | 1.272e+06 |           69.1228 |        -29.4591 |              157.98 |        -169.921 |             110.523 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:22:06 (running for 00:38:17.44)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    318 |          2267.36 | 1.272e+06 |           69.1228 |        -29.4591 |              157.98 |        -169.921 |             110.523 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1548.56,num_env_steps_sampled_lifetime=1276000.0,env_runners/episode_return_mean=77.42770673511768 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000318)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000318)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:11 (running for 00:38:22.46)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    319 |          2272.96 | 1.276e+06 |           77.4277 |        -28.2917 |             161.569 |         -169.21 |              113.36 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1550.9,num_env_steps_sampled_lifetime=1280000.0,env_runners/episode_return_mean=79.52313219851969 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000319)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000319)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:16 (running for 00:38:27.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    320 |          2278.38 | 1.28e+06 |           79.5231 |        -29.9306 |             162.518 |        -167.542 |             114.478 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1558.22,num_env_steps_sampled_lifetime=1284000.0,env_runners/episode_return_mean=88.58569756814491 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000320)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000320)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:21 (running for 00:38:32.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    321 |          2283.85 | 1.284e+06 |           88.5857 |        -29.9322 |              162.97 |        -160.286 |             115.834 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1552.44,num_env_steps_sampled_lifetime=1288000.0,env_runners/episode_return_mean=86.95000004737089 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000321)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000321)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:26 (running for 00:38:37.84)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    322 |          2289.96 | 1.288e+06 |             86.95 |        -31.7878 |              158.03 |        -149.907 |             110.614 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1550.86,num_env_steps_sampled_lifetime=1292000.0,env_runners/episode_return_mean=87.8979081553237 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000322)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000322)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:31 (running for 00:38:42.92)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    323 |          2295.61 | 1.292e+06 |           87.8979 |        -31.8287 |             157.511 |        -149.179 |             111.394 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1552.98,num_env_steps_sampled_lifetime=1296000.0,env_runners/episode_return_mean=90.42066673172474 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000323)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000323)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:36 (running for 00:38:48.00)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    324 |          2301.21 | 1.296e+06 |           90.4207 |          -32.37 |                 157 |        -146.022 |             111.813 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1551.76,num_env_steps_sampled_lifetime=1300000.0,env_runners/episode_return_mean=97.10904630718458 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000324)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000324)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:41 (running for 00:38:53.07)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |      ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    325 |          2307.05 | 1.3e+06 |            97.109 |        -30.8883 |             158.504 |        -142.321 |             111.815 |
+---------------------+----------+----------------------+--------+------------------+---------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1543.68,num_env_steps_sampled_lifetime=1304000.0,env_runners/episode_return_mean=83.11386081005944 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000325)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000325)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:46 (running for 00:38:58.15)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    326 |          2312.57 | 1.304e+06 |           83.1139 |        -29.8798 |             153.034 |        -147.683 |             107.643 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:22:51 (running for 00:39:03.17)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    326 |          2312.57 | 1.304e+06 |           83.1139 |        -29.8798 |             153.034 |        -147.683 |             107.643 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1552.76,num_env_steps_sampled_lifetime=1308000.0,env_runners/episode_return_mean=81.58050418421504 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000326)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000326)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:22:57 (running for 00:39:08.32)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    327 |          2318.48 | 1.308e+06 |           81.5805 |        -30.3677 |             153.687 |        -152.053 |             110.314 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1542.58,num_env_steps_sampled_lifetime=1312000.0,env_runners/episode_return_mean=78.71361401167977 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000327)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000327)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:02 (running for 00:39:13.41)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    328 |          2324.22 | 1.312e+06 |           78.7136 |        -30.3004 |             151.589 |         -148.79 |             106.215 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1540.06,num_env_steps_sampled_lifetime=1316000.0,env_runners/episode_return_mean=80.10533303688048 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000328)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000328)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:07 (running for 00:39:18.48)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    329 |          2329.92 | 1.316e+06 |           80.1053 |        -29.4086 |             151.888 |        -148.776 |             106.402 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1530.18,num_env_steps_sampled_lifetime=1320000.0,env_runners/episode_return_mean=78.02377152977505 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000329)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 4x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000329)...
[36m(_WandbLoggingActor pid=2143344)[0m Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:12 (running for 00:39:23.58)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    330 |          2336.09 | 1.32e+06 |           78.0238 |        -28.7036 |             150.303 |        -146.267 |             102.692 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1540.74,num_env_steps_sampled_lifetime=1324000.0,env_runners/episode_return_mean=73.8562145722355 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000330)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000330)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:17 (running for 00:39:28.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    331 |          2341.69 | 1.324e+06 |           73.8562 |        -29.0938 |              148.48 |        -147.206 |             101.675 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1549.86,num_env_steps_sampled_lifetime=1328000.0,env_runners/episode_return_mean=69.87599181749867 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000331)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000331)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:22 (running for 00:39:33.70)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    332 |          2347.52 | 1.328e+06 |            69.876 |        -28.5976 |             146.833 |        -149.158 |             100.798 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1560.28,num_env_steps_sampled_lifetime=1332000.0,env_runners/episode_return_mean=66.78088636750154 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000332)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000332)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:27 (running for 00:39:38.85)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    333 |          2353.19 | 1.332e+06 |           66.7809 |        -29.2029 |               145.7 |        -151.066 |              101.35 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:23:32 (running for 00:39:43.87)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    333 |          2353.19 | 1.332e+06 |           66.7809 |        -29.2029 |               145.7 |        -151.066 |              101.35 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1559.84,num_env_steps_sampled_lifetime=1336000.0,env_runners/episode_return_mean=65.2428902767095 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000333)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000333)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:37 (running for 00:39:48.99)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    334 |          2358.67 | 1.336e+06 |           65.2429 |        -28.8209 |             143.954 |          -148.8 |             98.9091 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1558.94,num_env_steps_sampled_lifetime=1340000.0,env_runners/episode_return_mean=64.9895912571843 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_in
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000334)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000334)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:42 (running for 00:39:54.00)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    335 |          2364.46 | 1.34e+06 |           64.9896 |         -27.747 |             144.465 |        -151.765 |             100.036 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1540.02,num_env_steps_sampled_lifetime=1344000.0,env_runners/episode_return_mean=69.11806706963739 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 5x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000335)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000335)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:47 (running for 00:39:59.19)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    336 |          2370.86 | 1.344e+06 |           69.1181 |        -28.4587 |             143.888 |        -141.424 |             95.1128 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1518.26,num_env_steps_sampled_lifetime=1348000.0,env_runners/episode_return_mean=69.95468298665367 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 4x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000336)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000336)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:52 (running for 00:40:04.21)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    337 |          2377.36 | 1.348e+06 |           69.9547 |        -25.0303 |             141.397 |        -136.121 |             89.7098 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1520.26,num_env_steps_sampled_lifetime=1352000.0,env_runners/episode_return_mean=60.12709431129472 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 2 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000337)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000337)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:23:58 (running for 00:40:09.42)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    338 |          2382.99 | 1.352e+06 |           60.1271 |        -25.7879 |             136.912 |        -135.803 |             84.8055 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


== Status ==
Current time: 2025-07-11 12:24:03 (running for 00:40:14.44)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    338 |          2382.99 | 1.352e+06 |           60.1271 |        -25.7879 |             136.912 |        -135.803 |             84.8055 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1517.96,num_env_steps_sampled_lifetime=1356000.0,env_runners/episode_return_mean=55.14957340667285 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000338)
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143131)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000338)... Done. 0.0s
== Status ==
Current time: 2025-07-11 12:24:08 (running for 00:40:19.66)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |        ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    339 |          2389.06 | 1.356e+06 |           55.1496 |        -27.3868 |             133.398 |        -132.114 |              81.253 |
+---------------------+----------+----------------------+--------+------------------+-----------+-------------------+-----------------+---------------------+-----------------+---------------------+


Trial PPO_env_df440_00000 reported env_runners/episode_len_mean=1509.54,num_env_steps_sampled_lifetime=1360000.0,env_runners/episode_return_mean=56.64720351470581 with parameters={'exploration_config': {}, 'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'placement_strategy': 'PACK', 'num_gpus': 0, '_fake_gpus': False, 'num_cpus_for_main_process': 1, 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'torch_ddp_kwargs': {}, 'torch_skip_nan_gradients': False, 'env': 'env', 'env_config': {}, 'observation_space': None, 'action_space': None, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, '_is_atari': None, 'disable_env_checking': False, 'render_env': False, 'action_mask_key': 'action_mask', 'env_runner_cls': None, 'num_env_runners': 2, 'create_local_env_runner': True, 'num_envs_per_env_runner': 1, 'gym_env_vectorize_mode': 'SYNC', 'num_cpus_per_env_runner': 1, 'num_gpus_per_env_runner': 0, 'custom_resources_per_env_runner': {}, 'validate_env_runners_after_construction': True, 'episodes_to_numpy': True, 'max_requests_in_flight_per_env_runner': 1, 'sample_timeout_s': 60.0, '_env_to_module_connector': None, 'add_default_connectors_to_env_to_module_pipeline': True, '_module_to_env_connector': None, 'add_default_connectors_to_module_to_env_pipeline': True, 'merge_env_runner_states': 'training_only', 'broadcast_env_runner_states': True, 'episode_lookback_horizon': 1, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'compress_observations': False, 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'enable_tf1_exec_eagerly': False, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'sampler_perf_stats_ema_coef': None, '_is_online': True, 'num_learners': 0, 'num_gpus_per_learner': 1, 'num_cpus_per_learner': 'auto', 'num_aggregator_actors_per_learner': 0, 'max_requests_in_flight_per_aggregator_actor': 3, 'local_gpu_idx': 0, 'max_requests_in_flight_per_learner': 3, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', '_train_batch_size_per_learner': None, 'train_batch_size': 4000, 'num_epochs': 30, 'minibatch_size': 128, 'shuffle_batch_per_epoch': True, 'model': {'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'fcnet_weights_initializer': None, 'fcnet_weights_initializer_config': None, 'fcnet_bias_initializer': None, 'fcnet_bias_initializer_config': None, 'conv_filters': None, 'conv_activation': 'relu', 'conv_kernel_initializer': None, 'conv_kernel_initializer_config': None, 'conv_bias_initializer': None, 'conv_bias_initializer_config': None, 'conv_transpose_kernel_initializer': None, 'conv_transpose_kernel_initializer_config': None, 'conv_transpose_bias_initializer': None, 'conv_transpose_bias_initializer_config': None, 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'post_fcnet_weights_initializer': None, 'post_fcnet_weights_initializer_config': None, 'post_fcnet_bias_initializer': None, 'post_fcnet_bias_initializer_config': None, 'free_log_std': False, 'log_std_clip_param': 20.0, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, 'lstm_weights_i
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 3x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143131)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 2x across cluster][0m
[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000339)
[36m(_WandbLoggingActor pid=2143344)[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-11_11-43-48/PPO_env_df440_00000_0_2025-07-11_11-43-48/checkpoint_000339)... Done. 0.0s
2025-07-11 12:24:12,739	WARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip.
2025-07-11 12:24:12,882	INFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/qrbao/ray_results/PPO_2025-07-11_11-43-48' in 0.1428s.
== Status ==
Current time: 2025-07-11 12:24:12 (running for 00:40:24.15)
Using FIFO scheduling algorithm.
Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)
Result logdir: /tmp/ray/session_2025-07-11_11-43-47_670969_2141245/artifacts/2025-07-11_11-43-48/PPO_2025-07-11_11-43-48/driver_artifacts
Number of trials: 1/1 (1 RUNNING)
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+
| Trial name          | status   | loc                  |   iter |   total time (s) |       ts |   combined return |   return prey_1 |   return predator_0 |   return prey_0 |   return predator_1 |
|---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------|
| PPO_env_df440_00000 | RUNNING  | 192.168.0.25:2143020 |    340 |          2395.09 | 1.36e+06 |           56.6472 |        -27.4415 |             136.987 |        -136.894 |             83.9966 |
+---------------------+----------+----------------------+--------+------------------+----------+-------------------+-----------------+---------------------+-----------------+---------------------+


[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=2143020)[0m [DEBUG] on_train_result Ë¢´Ë∞ÉÁî®
[36m(MultiAgentEnvRunner pid=2143130)[0m Reset complete:[32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   Agents: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m   _cumulative_rewards keys: ['predator_0', 'predator_1', 'prey_0', 'prey_1'][32m [repeated 2x across cluster][0m
[36m(MultiAgentEnvRunner pid=2143130)[0m Agent 3 Â∑≤‰ªéÁâ©ÁêÜÁ©∫Èó¥‰∏≠ÁßªÈô§[32m [repeated 3x across cluster][0m
Traceback (most recent call last):
  File "/home/qrbao/Documents/code4/rllib/mycode/training_code2.py", line 223, in <module>
    results =run_rllib_example_script_experiment(base_config, args, keep_ray_up=True)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/utils/test_utils.py", line 1342, in run_rllib_example_script_experiment
    ).fit()
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/tuner.py", line 345, in fit
    return self._local_tuner.fit()
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py", line 506, in fit
    analysis = self._fit_internal(trainable, param_space)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py", line 622, in _fit_internal
    analysis = run(
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/tune.py", line 1026, in run
    runner.cleanup()
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py", line 1975, in cleanup
    self._cleanup_trials()
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py", line 803, in _cleanup_trials
    self._actor_manager.next(timeout=1)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/air/execution/_internal/actor_manager.py", line 213, in next
    ready, _ = ray.wait(all_futures, num_returns=1, timeout=timeout)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/worker.py", line 3080, in wait
    ready_ids, remaining_ids = worker.core_worker.wait(
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/qrbao/Documents/code4/rllib/mycode/training_code2.py", line 223, in <module>
    results =run_rllib_example_script_experiment(base_config, args, keep_ray_up=True)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/rllib/utils/test_utils.py", line 1342, in run_rllib_example_script_experiment
    ).fit()
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/tuner.py", line 345, in fit
    return self._local_tuner.fit()
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py", line 506, in fit
    analysis = self._fit_internal(trainable, param_space)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py", line 622, in _fit_internal
    analysis = run(
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/tune.py", line 1026, in run
    runner.cleanup()
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py", line 1975, in cleanup
    self._cleanup_trials()
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py", line 803, in _cleanup_trials
    self._actor_manager.next(timeout=1)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/air/execution/_internal/actor_manager.py", line 213, in next
    ready, _ = ray.wait(all_futures, num_returns=1, timeout=timeout)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 22, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 104, in wrapper
    return func(*args, **kwargs)
  File "/home/qrbao/anaconda3/envs/rllib/lib/python3.10/site-packages/ray/_private/worker.py", line 3080, in wait
    ready_ids, remaining_ids = worker.core_worker.wait(
KeyboardInterrupt
[0m
