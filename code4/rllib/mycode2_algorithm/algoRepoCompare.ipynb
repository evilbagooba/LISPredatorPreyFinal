{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee893b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192.06s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pettingzoo in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (1.25.0)\n",
      "Requirement already satisfied: pygame in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (2.2.6)\n",
      "Requirement already satisfied: gymnasium>=1.0.0 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from pettingzoo) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from gymnasium>=1.0.0->pettingzoo) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from gymnasium>=1.0.0->pettingzoo) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from gymnasium>=1.0.0->pettingzoo) (0.0.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "197.94s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: pygame in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from moviepy) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from moviepy) (0.6.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from moviepy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from moviepy) (2.2.6)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from moviepy) (2.32.4)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from moviepy) (0.1.12)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (11.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install pettingzoo pygame numpy\n",
    "!pip install moviepy pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a1a131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.sisl import waterworld_v4\n",
    "env = waterworld_v4.env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6e99111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-16 19:13:10,830\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5073c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return PettingZooEnv(waterworld_v4.env())\n",
    "k = 4  # Number of parallel environments\n",
    "envs = DummyVectorEnv([make_env for _ in range(k)])  # k 个并行环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb608f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Tianshou Waterworld 简化版本 - 先测试基本功能\n",
    "# 每个agent使用独立的策略（从随机策略开始）\n",
    "\n",
    "# 安装依赖：\n",
    "# pip install pettingzoo[sisl]\n",
    "# pip install tianshou\n",
    "# pip install torch\n",
    "# pip install numpy\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from typing import Optional, Tuple, List\n",
    "\n",
    "# # Tianshou imports\n",
    "# from tianshou.data import Collector, VectorReplayBuffer\n",
    "# from tianshou.env import DummyVectorEnv\n",
    "# from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "# from tianshou.policy import (\n",
    "#     BasePolicy, \n",
    "#     DQNPolicy, \n",
    "#     MultiAgentPolicyManager, \n",
    "#     RandomPolicy\n",
    "# )\n",
    "# from tianshou.trainer import offpolicy_trainer\n",
    "# from tianshou.utils.net.common import Net\n",
    "\n",
    "# # PettingZoo imports\n",
    "# from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "# def get_args():\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--seed', type=int, default=42)\n",
    "#     parser.add_argument('--buffer-size', type=int, default=20000)\n",
    "#     parser.add_argument('--lr', type=float, default=1e-4)\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99)\n",
    "#     parser.add_argument('--epoch', type=int, default=10)  # 减少训练轮数\n",
    "#     parser.add_argument('--step-per-epoch', type=int, default=1000)\n",
    "#     parser.add_argument('--step-per-collect', type=int, default=200)\n",
    "#     parser.add_argument('--batch-size', type=int, default=64)\n",
    "#     parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[128, 128])\n",
    "#     parser.add_argument('--training-num', type=int, default=4)\n",
    "#     parser.add_argument('--test-num', type=int, default=2)\n",
    "#     parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     parser.add_argument('--watch', action='store_true', default=False)\n",
    "#     parser.add_argument('--render', action='store_true', default=False)\n",
    "    \n",
    "#     # Waterworld specific parameters\n",
    "#     parser.add_argument('--n-pursuers', type=int, default=5)\n",
    "    \n",
    "#     return parser.parse_known_args()[0]\n",
    "\n",
    "# def get_env(args):\n",
    "#     \"\"\"创建Waterworld环境\"\"\"\n",
    "#     env = waterworld_v4.env(\n",
    "#         n_pursuers=args.n_pursuers,\n",
    "#         n_evaders=5,\n",
    "#         n_poisons=10,\n",
    "#         n_coop=2,\n",
    "#         n_sensors=30,\n",
    "#         sensor_range=0.2,\n",
    "#         radius=0.015,\n",
    "#         pursuer_max_accel=0.01,\n",
    "#         evader_speed=0.01,\n",
    "#         poison_speed=0.01,\n",
    "#         poison_reward=-1.0,\n",
    "#         food_reward=10.0,\n",
    "#         encounter_reward=0.01,\n",
    "#         thrust_penalty=-0.5,\n",
    "#         local_ratio=1.0,\n",
    "#         speed_features=True,\n",
    "#         max_cycles=500,\n",
    "#         render_mode=\"human\" if args.render else None\n",
    "#     )\n",
    "#     return PettingZooEnv(env)\n",
    "\n",
    "# def create_simple_policies(args, env) -> Tuple[List[BasePolicy], List]:\n",
    "#     \"\"\"\n",
    "#     创建简单的策略组合：\n",
    "#     - 一些使用DQN（离散化动作）\n",
    "#     - 一些使用随机策略\n",
    "#     \"\"\"\n",
    "#     agents = env.agents\n",
    "#     policies = []\n",
    "#     optimizers = []\n",
    "    \n",
    "#     # 获取环境信息\n",
    "#     observation_space = env.observation_space\n",
    "#     action_space = env.action_space\n",
    "    \n",
    "#     state_shape = observation_space.shape or observation_space.n\n",
    "#     action_shape = action_space.shape or action_space.n\n",
    "    \n",
    "#     print(f\"状态空间维度: {state_shape}\")\n",
    "#     print(f\"动作空间: {action_space}\")\n",
    "#     print(f\"智能体数量: {len(agents)}\")\n",
    "    \n",
    "#     # 为连续动作空间创建离散化版本的DQN\n",
    "#     # 将连续动作 [-0.01, 0.01] x [-0.01, 0.01] 离散化为9个动作\n",
    "#     discrete_actions = [\n",
    "#         [-0.01, -0.01], [-0.01, 0.0], [-0.01, 0.01],  # 左上, 左, 左下\n",
    "#         [0.0, -0.01],   [0.0, 0.0],   [0.0, 0.01],    # 上, 停止, 下  \n",
    "#         [0.01, -0.01],  [0.01, 0.0],  [0.01, 0.01]    # 右上, 右, 右下\n",
    "#     ]\n",
    "    \n",
    "#     # 为每个agent分配策略\n",
    "#     algorithm_assignment = {\n",
    "#         'pursuer_0': 'DQN',      # 使用DQN\n",
    "#         'pursuer_1': 'DQN',      # 使用DQN\n",
    "#         'pursuer_2': 'Random',   # 随机策略\n",
    "#         'pursuer_3': 'Random',   # 随机策略\n",
    "#         'pursuer_4': 'DQN',      # 使用DQN\n",
    "#     }\n",
    "    \n",
    "#     for i, agent_id in enumerate(agents):\n",
    "#         algo = algorithm_assignment.get(agent_id, 'Random')\n",
    "#         print(f\"Agent {agent_id} 使用算法: {algo}\")\n",
    "        \n",
    "#         if algo == 'DQN':\n",
    "#             # 创建DQN网络\n",
    "#             net = Net(\n",
    "#                 state_shape=state_shape,\n",
    "#                 action_shape=len(discrete_actions),  # 9个离散动作\n",
    "#                 hidden_sizes=args.hidden_sizes,\n",
    "#                 device=args.device\n",
    "#             ).to(args.device)\n",
    "            \n",
    "#             optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "            \n",
    "#             # 创建DQN策略\n",
    "#             policy = DQNPolicy(\n",
    "#                 model=net,\n",
    "#                 optim=optim,\n",
    "#                 discount_factor=args.gamma,\n",
    "#                 estimation_step=3,\n",
    "#                 target_update_freq=320\n",
    "#             )\n",
    "            \n",
    "#             # 为DQN策略添加动作转换函数\n",
    "#             policy.discrete_actions = discrete_actions\n",
    "#             original_forward = policy.forward\n",
    "            \n",
    "#             def dqn_forward_with_continuous_output(batch, state=None, **kwargs):\n",
    "#                 # 调用原始forward得到离散动作\n",
    "#                 result = original_forward(batch, state, **kwargs)\n",
    "                \n",
    "#                 # 将离散动作索引转换为连续动作\n",
    "#                 if hasattr(result, 'act'):\n",
    "#                     discrete_indices = result.act\n",
    "#                     continuous_actions = []\n",
    "                    \n",
    "#                     for idx in discrete_indices:\n",
    "#                         if isinstance(idx, torch.Tensor):\n",
    "#                             idx = idx.item()\n",
    "#                         continuous_actions.append(discrete_actions[idx])\n",
    "                    \n",
    "#                     result.act = np.array(continuous_actions)\n",
    "                \n",
    "#                 return result\n",
    "            \n",
    "#             policy.forward = dqn_forward_with_continuous_output\n",
    "#             optimizers.append(optim)\n",
    "            \n",
    "#         else:  # Random policy\n",
    "#             policy = RandomPolicy(action_space)\n",
    "#             optimizers.append(None)\n",
    "        \n",
    "#         policies.append(policy)\n",
    "    \n",
    "#     return policies, optimizers\n",
    "\n",
    "# def train_agents(args):\n",
    "#     \"\"\"训练多智能体\"\"\"\n",
    "#     print(\"初始化环境...\")\n",
    "    \n",
    "#     # 环境设置\n",
    "#     env = get_env(args)\n",
    "#     train_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.training_num)])\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.test_num)])\n",
    "    \n",
    "#     # 设置随机种子\n",
    "#     np.random.seed(args.seed)\n",
    "#     torch.manual_seed(args.seed)\n",
    "#     train_envs.seed(args.seed)\n",
    "#     test_envs.seed(args.seed)\n",
    "    \n",
    "#     print(\"创建策略...\")\n",
    "#     # 创建智能体策略\n",
    "#     policies, optimizers = create_simple_policies(args, env)\n",
    "    \n",
    "#     # 创建多智能体策略管理器\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     print(\"创建收集器...\")\n",
    "#     # 创建收集器\n",
    "#     train_collector = Collector(\n",
    "#         policy_manager,\n",
    "#         train_envs,\n",
    "#         VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "#         exploration_noise=True\n",
    "#     )\n",
    "    \n",
    "#     test_collector = Collector(\n",
    "#         policy_manager, \n",
    "#         test_envs,\n",
    "#         exploration_noise=True\n",
    "#     )\n",
    "    \n",
    "#     # 预收集一些数据\n",
    "#     print(\"预收集数据...\")\n",
    "#     train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "    \n",
    "#     # 回调函数\n",
    "#     def save_best_fn(policy):\n",
    "#         \"\"\"保存最佳策略\"\"\"\n",
    "#         save_dir = \"waterworld_policies\"\n",
    "#         os.makedirs(save_dir, exist_ok=True)\n",
    "#         for i, agent_id in enumerate(env.agents):\n",
    "#             if hasattr(policy.policies[agent_id], 'state_dict'):\n",
    "#                 model_path = os.path.join(save_dir, f\"{agent_id}_policy.pth\")\n",
    "#                 torch.save(policy.policies[agent_id].state_dict(), model_path)\n",
    "#         print(f\"策略已保存到 {save_dir}\")\n",
    "    \n",
    "#     def stop_fn(mean_rewards):\n",
    "#         \"\"\"停止条件\"\"\"\n",
    "#         return mean_rewards >= 5.0\n",
    "    \n",
    "#     def train_fn(epoch, env_step):\n",
    "#         \"\"\"训练时设置探索率\"\"\"\n",
    "#         for agent_id in env.agents:\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'set_eps'):\n",
    "#                 policy_manager.policies[agent_id].set_eps(0.1)\n",
    "    \n",
    "#     def test_fn(epoch, env_step):\n",
    "#         \"\"\"测试时设置探索率\"\"\"\n",
    "#         for agent_id in env.agents:\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'set_eps'):\n",
    "#                 policy_manager.policies[agent_id].set_eps(0.05)\n",
    "    \n",
    "#     def reward_metric(rews):\n",
    "#         \"\"\"奖励度量函数 - 返回所有agent的平均奖励\"\"\"\n",
    "#         return rews.mean(axis=1)\n",
    "    \n",
    "#     # 检查是否有可训练的策略\n",
    "#     trainable_policies = [p for p in policies if not isinstance(p, RandomPolicy)]\n",
    "    \n",
    "#     if trainable_policies:\n",
    "#         print(f\"开始训练 {len(trainable_policies)} 个可训练策略...\")\n",
    "        \n",
    "#         # 使用off-policy训练器\n",
    "#         result = offpolicy_trainer(\n",
    "#             policy=policy_manager,\n",
    "#             train_collector=train_collector,\n",
    "#             test_collector=test_collector,\n",
    "#             max_epoch=args.epoch,\n",
    "#             step_per_epoch=args.step_per_epoch,\n",
    "#             step_per_collect=args.step_per_collect,\n",
    "#             episode_per_test=5,\n",
    "#             batch_size=args.batch_size,\n",
    "#             train_fn=train_fn,\n",
    "#             test_fn=test_fn,\n",
    "#             stop_fn=stop_fn,\n",
    "#             save_best_fn=save_best_fn,\n",
    "#             update_per_step=0.1,\n",
    "#             test_in_train=False,\n",
    "#             reward_metric=reward_metric\n",
    "#         )\n",
    "        \n",
    "#         print(\"训练完成！\")\n",
    "#         print(f\"训练结果: {result}\")\n",
    "#     else:\n",
    "#         print(\"所有策略都是随机策略，运行测试...\")\n",
    "#         result = test_collector.collect(n_episode=5)\n",
    "#         print(f\"随机策略测试结果 - 平均奖励: {result['rew']}, 平均长度: {result['len']}\")\n",
    "    \n",
    "#     return policy_manager, result\n",
    "\n",
    "# def watch_performance(args):\n",
    "#     \"\"\"观察策略表现\"\"\"\n",
    "#     print(\"观察策略表现...\")\n",
    "#     env = get_env(args)\n",
    "#     policies, _ = create_simple_policies(args, env)\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     # 尝试加载训练好的策略\n",
    "#     save_dir = \"waterworld_policies\"\n",
    "#     if os.path.exists(save_dir):\n",
    "#         for i, agent_id in enumerate(env.agents):\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'load_state_dict'):\n",
    "#                 model_path = os.path.join(save_dir, f\"{agent_id}_policy.pth\")\n",
    "#                 if os.path.exists(model_path):\n",
    "#                     policy_manager.policies[agent_id].load_state_dict(torch.load(model_path))\n",
    "#                     print(f\"加载了 {agent_id} 的策略\")\n",
    "    \n",
    "#     # 需要使用向量化环境\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#     collector = Collector(policy_manager, test_envs)\n",
    "    \n",
    "#     print(\"运行测试episode...\")\n",
    "#     result = collector.collect(n_episode=3, render=args.render)\n",
    "#     print(f\"平均奖励: {result['rew']}\")\n",
    "#     print(f\"平均长度: {result['len']}\")\n",
    "\n",
    "# def test_random_policies(args):\n",
    "#     \"\"\"测试纯随机策略作为基准\"\"\"\n",
    "#     print(\"测试纯随机策略...\")\n",
    "#     env = get_env(args)\n",
    "    \n",
    "#     # 创建所有随机策略\n",
    "#     policies = [RandomPolicy(env.action_space) for _ in env.agents]\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     # 需要使用向量化环境\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "    \n",
    "#     # 运行测试\n",
    "#     collector = Collector(policy_manager, test_envs)\n",
    "#     result = collector.collect(n_episode=5, render=args.render)\n",
    "#     print(f\"随机策略基准 - 平均奖励: {result['rew']}, 平均长度: {result['len']}\")\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# def simple_debug_test(args):\n",
    "#     \"\"\"最简单的debug测试\"\"\"\n",
    "#     print(\"=== Debug测试：基础环境和策略创建 ===\")\n",
    "    \n",
    "#     try:\n",
    "#         # 1. 测试环境创建\n",
    "#         print(\"1. 创建环境...\")\n",
    "#         env = get_env(args)\n",
    "#         print(f\"   环境创建成功: {type(env)}\")\n",
    "#         print(f\"   智能体: {env.agents}\")\n",
    "#         print(f\"   观察空间: {env.observation_space}\")\n",
    "#         print(f\"   动作空间: {env.action_space}\")\n",
    "        \n",
    "#         # 2. 测试策略创建\n",
    "#         print(\"\\n2. 创建策略...\")\n",
    "#         policies, optimizers = create_simple_policies(args, env)\n",
    "#         print(f\"   策略创建成功: {len(policies)} 个策略\")\n",
    "        \n",
    "#         # 3. 测试策略管理器\n",
    "#         print(\"\\n3. 创建策略管理器...\")\n",
    "#         policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "#         print(f\"   策略管理器创建成功: {type(policy_manager)}\")\n",
    "        \n",
    "#         # 4. 测试向量化环境\n",
    "#         print(\"\\n4. 创建向量化环境...\")\n",
    "#         test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#         print(f\"   向量化环境创建成功: {type(test_envs)}\")\n",
    "        \n",
    "#         # 5. 测试收集器\n",
    "#         print(\"\\n5. 创建收集器...\")\n",
    "#         collector = Collector(policy_manager, test_envs)\n",
    "#         print(f\"   收集器创建成功: {type(collector)}\")\n",
    "        \n",
    "#         # 6. 测试收集一个step\n",
    "#         print(\"\\n6. 测试收集数据...\")\n",
    "#         result = collector.collect(n_step=10)\n",
    "#         print(f\"   数据收集成功: {result}\")\n",
    "        \n",
    "#         print(\"\\n=== 所有基础测试通过！===\")\n",
    "#         return True\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n!!! 测试失败: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return False\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = get_args()\n",
    "    \n",
    "#     print(\"=== Tianshou Waterworld 多智能体测试 ===\")\n",
    "#     print(f\"设备: {args.device}\")\n",
    "#     print(f\"智能体数量: {args.n_pursuers}\")\n",
    "    \n",
    "#     # 首先运行debug测试\n",
    "#     debug_success = simple_debug_test(args)\n",
    "    \n",
    "#     if not debug_success:\n",
    "#         print(\"基础测试失败，请检查环境配置\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     if args.watch:\n",
    "#         watch_performance(args)\n",
    "#     else:\n",
    "#         # 首先测试随机策略\n",
    "#         print(\"\\n1. 测试随机策略基准:\")\n",
    "#         test_random_policies(args)\n",
    "        \n",
    "#         # 然后训练DQN策略\n",
    "#         print(\"\\n2. 训练DQN策略:\")\n",
    "#         policy_manager, result = train_agents(args)\n",
    "        \n",
    "#         # 训练完成后观察表现\n",
    "#         if result:\n",
    "#             print(\"\\n3. 观察训练后的策略表现:\")\n",
    "#             args.render = True\n",
    "#             watch_performance(args)\n",
    "\n",
    "# \"\"\"\n",
    "# 使用说明：\n",
    "\n",
    "# 1. 基础测试（只用随机策略）：\n",
    "#    python waterworld_simple.py\n",
    "\n",
    "# 2. 观察训练结果：\n",
    "#    python waterworld_simple.py --watch --render\n",
    "\n",
    "# 3. 自定义参数：\n",
    "#    python waterworld_simple.py --epoch 20 --lr 1e-3\n",
    "\n",
    "# 这个简化版本的特点：\n",
    "\n",
    "# 1. 使用DQN处理连续动作空间（通过离散化）\n",
    "# 2. 混合DQN和随机策略\n",
    "# 3. 更强的错误处理和兼容性\n",
    "# 4. 渐进式测试方法\n",
    "\n",
    "# 如果这个版本能正常运行，我们再逐步添加更复杂的算法。\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed7726ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Tianshou Waterworld 简化版本 - 先测试基本功能\n",
    "# 每个agent使用独立的策略（从随机策略开始）\n",
    "\n",
    "# 安装依赖：\n",
    "# pip install pettingzoo[sisl]\n",
    "# pip install tianshou\n",
    "# pip install torch\n",
    "# pip install numpy\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from typing import Optional, Tuple, List\n",
    "\n",
    "# # Tianshou imports\n",
    "# from tianshou.data import Collector, VectorReplayBuffer\n",
    "# from tianshou.env import DummyVectorEnv\n",
    "# from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "# from tianshou.policy import (\n",
    "#     BasePolicy, \n",
    "#     DQNPolicy, \n",
    "#     MultiAgentPolicyManager, \n",
    "#     RandomPolicy\n",
    "# )\n",
    "# from tianshou.trainer import offpolicy_trainer\n",
    "# from tianshou.utils.net.common import Net\n",
    "\n",
    "# # PettingZoo imports\n",
    "# from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "# def get_args():\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--seed', type=int, default=42)\n",
    "#     parser.add_argument('--buffer-size', type=int, default=20000)\n",
    "#     parser.add_argument('--lr', type=float, default=1e-4)\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99)\n",
    "#     parser.add_argument('--epoch', type=int, default=10)  # 减少训练轮数\n",
    "#     parser.add_argument('--step-per-epoch', type=int, default=1000)\n",
    "#     parser.add_argument('--step-per-collect', type=int, default=200)\n",
    "#     parser.add_argument('--batch-size', type=int, default=64)\n",
    "#     parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[128, 128])\n",
    "#     parser.add_argument('--training-num', type=int, default=4)\n",
    "#     parser.add_argument('--test-num', type=int, default=2)\n",
    "#     parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     parser.add_argument('--watch', action='store_true', default=False)\n",
    "#     parser.add_argument('--render', action='store_true', default=False)\n",
    "    \n",
    "#     # Waterworld specific parameters\n",
    "#     parser.add_argument('--n-pursuers', type=int, default=5)\n",
    "    \n",
    "#     return parser.parse_known_args()[0]\n",
    "\n",
    "# def get_env(args):\n",
    "#     \"\"\"创建Waterworld环境\"\"\"\n",
    "#     env = waterworld_v4.env(\n",
    "#         n_pursuers=args.n_pursuers,\n",
    "#         n_evaders=5,\n",
    "#         n_poisons=10,\n",
    "#         n_coop=2,\n",
    "#         n_sensors=30,\n",
    "#         sensor_range=0.2,\n",
    "#         radius=0.015,\n",
    "#         pursuer_max_accel=0.01,\n",
    "#         evader_speed=0.01,\n",
    "#         poison_speed=0.01,\n",
    "#         poison_reward=-1.0,\n",
    "#         food_reward=10.0,\n",
    "#         encounter_reward=0.01,\n",
    "#         thrust_penalty=-0.5,\n",
    "#         local_ratio=1.0,\n",
    "#         speed_features=True,\n",
    "#         max_cycles=500,\n",
    "#         render_mode=\"human\" if args.render else None\n",
    "#     )\n",
    "#     return PettingZooEnv(env)\n",
    "\n",
    "# def create_simple_policies(args, env) -> Tuple[List[BasePolicy], List]:\n",
    "#     \"\"\"\n",
    "#     创建简单的策略组合：\n",
    "#     - 一些使用DQN（离散化动作）\n",
    "#     - 一些使用随机策略\n",
    "#     \"\"\"\n",
    "#     agents = env.agents\n",
    "#     policies = []\n",
    "#     optimizers = []\n",
    "    \n",
    "#     # 获取环境信息\n",
    "#     observation_space = env.observation_space\n",
    "#     action_space = env.action_space\n",
    "    \n",
    "#     state_shape = observation_space.shape or observation_space.n\n",
    "#     action_shape = action_space.shape or action_space.n\n",
    "    \n",
    "#     print(f\"状态空间维度: {state_shape}\")\n",
    "#     print(f\"动作空间: {action_space}\")\n",
    "#     print(f\"智能体数量: {len(agents)}\")\n",
    "    \n",
    "#     # 为连续动作空间创建离散化版本的DQN\n",
    "#     # 将连续动作 [-0.01, 0.01] x [-0.01, 0.01] 离散化为9个动作\n",
    "#     discrete_actions = [\n",
    "#         [-0.01, -0.01], [-0.01, 0.0], [-0.01, 0.01],  # 左上, 左, 左下\n",
    "#         [0.0, -0.01],   [0.0, 0.0],   [0.0, 0.01],    # 上, 停止, 下  \n",
    "#         [0.01, -0.01],  [0.01, 0.0],  [0.01, 0.01]    # 右上, 右, 右下\n",
    "#     ]\n",
    "    \n",
    "#     # 为每个agent分配策略\n",
    "#     algorithm_assignment = {\n",
    "#         'pursuer_0': 'DQN',      # 使用DQN\n",
    "#         'pursuer_1': 'DQN',      # 使用DQN\n",
    "#         'pursuer_2': 'Random',   # 随机策略\n",
    "#         'pursuer_3': 'Random',   # 随机策略\n",
    "#         'pursuer_4': 'DQN',      # 使用DQN\n",
    "#     }\n",
    "    \n",
    "#     for i, agent_id in enumerate(agents):\n",
    "#         algo = algorithm_assignment.get(agent_id, 'Random')\n",
    "#         print(f\"Agent {agent_id} 使用算法: {algo}\")\n",
    "        \n",
    "#         if algo == 'DQN':\n",
    "#             # 创建DQN网络\n",
    "#             net = Net(\n",
    "#                 state_shape=state_shape,\n",
    "#                 action_shape=len(discrete_actions),  # 9个离散动作\n",
    "#                 hidden_sizes=args.hidden_sizes,\n",
    "#                 device=args.device\n",
    "#             ).to(args.device)\n",
    "            \n",
    "#             optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "            \n",
    "#             # 创建DQN策略\n",
    "#             policy = DQNPolicy(\n",
    "#                 model=net,\n",
    "#                 optim=optim,\n",
    "#                 discount_factor=args.gamma,\n",
    "#                 estimation_step=3,\n",
    "#                 target_update_freq=320\n",
    "#             )\n",
    "            \n",
    "#             # 为DQN策略添加动作转换函数\n",
    "#             policy.discrete_actions = discrete_actions\n",
    "#             original_forward = policy.forward\n",
    "            \n",
    "#             def dqn_forward_with_continuous_output(batch, state=None, **kwargs):\n",
    "#                 # 调用原始forward得到离散动作\n",
    "#                 result = original_forward(batch, state, **kwargs)\n",
    "                \n",
    "#                 # 将离散动作索引转换为连续动作\n",
    "#                 if hasattr(result, 'act'):\n",
    "#                     discrete_indices = result.act\n",
    "#                     continuous_actions = []\n",
    "                    \n",
    "#                     for idx in discrete_indices:\n",
    "#                         if isinstance(idx, torch.Tensor):\n",
    "#                             idx = idx.item()\n",
    "#                         continuous_actions.append(discrete_actions[idx])\n",
    "                    \n",
    "#                     result.act = np.array(continuous_actions)\n",
    "                \n",
    "#                 return result\n",
    "            \n",
    "#             policy.forward = dqn_forward_with_continuous_output\n",
    "#             optimizers.append(optim)\n",
    "            \n",
    "#         else:  # Random policy\n",
    "#             policy = RandomPolicy(action_space)\n",
    "#             optimizers.append(None)\n",
    "        \n",
    "#         policies.append(policy)\n",
    "    \n",
    "#     return policies, optimizers\n",
    "\n",
    "# def train_agents(args):\n",
    "#     \"\"\"训练多智能体\"\"\"\n",
    "#     print(\"初始化环境...\")\n",
    "    \n",
    "#     # 环境设置\n",
    "#     env = get_env(args)\n",
    "#     train_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.training_num)])\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.test_num)])\n",
    "    \n",
    "#     # 设置随机种子\n",
    "#     np.random.seed(args.seed)\n",
    "#     torch.manual_seed(args.seed)\n",
    "#     train_envs.seed(args.seed)\n",
    "#     test_envs.seed(args.seed)\n",
    "    \n",
    "#     print(\"创建策略...\")\n",
    "#     # 创建智能体策略\n",
    "#     policies, optimizers = create_simple_policies(args, env)\n",
    "    \n",
    "#     # 创建多智能体策略管理器\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     print(\"创建收集器...\")\n",
    "#     # 创建收集器\n",
    "#     train_collector = Collector(\n",
    "#         policy_manager,\n",
    "#         train_envs,\n",
    "#         VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "#         exploration_noise=True\n",
    "#     )\n",
    "    \n",
    "#     test_collector = Collector(\n",
    "#         policy_manager, \n",
    "#         test_envs,\n",
    "#         exploration_noise=True\n",
    "#     )\n",
    "    \n",
    "#     # 预收集一些数据\n",
    "#     print(\"预收集数据...\")\n",
    "#     train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "    \n",
    "#     # 回调函数\n",
    "#     def save_best_fn(policy):\n",
    "#         \"\"\"保存最佳策略\"\"\"\n",
    "#         save_dir = \"waterworld_policies\"\n",
    "#         os.makedirs(save_dir, exist_ok=True)\n",
    "#         for i, agent_id in enumerate(env.agents):\n",
    "#             if hasattr(policy.policies[agent_id], 'state_dict'):\n",
    "#                 model_path = os.path.join(save_dir, f\"{agent_id}_policy.pth\")\n",
    "#                 torch.save(policy.policies[agent_id].state_dict(), model_path)\n",
    "#         print(f\"策略已保存到 {save_dir}\")\n",
    "    \n",
    "#     def stop_fn(mean_rewards):\n",
    "#         \"\"\"停止条件\"\"\"\n",
    "#         return mean_rewards >= 5.0\n",
    "    \n",
    "#     def train_fn(epoch, env_step):\n",
    "#         \"\"\"训练时设置探索率\"\"\"\n",
    "#         for agent_id in env.agents:\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'set_eps'):\n",
    "#                 policy_manager.policies[agent_id].set_eps(0.1)\n",
    "    \n",
    "#     def test_fn(epoch, env_step):\n",
    "#         \"\"\"测试时设置探索率\"\"\"\n",
    "#         for agent_id in env.agents:\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'set_eps'):\n",
    "#                 policy_manager.policies[agent_id].set_eps(0.05)\n",
    "    \n",
    "#     def reward_metric(rews):\n",
    "#         \"\"\"奖励度量函数 - 返回所有agent的平均奖励\"\"\"\n",
    "#         return rews.mean(axis=1)\n",
    "    \n",
    "#     # 检查是否有可训练的策略\n",
    "#     trainable_policies = [p for p in policies if not isinstance(p, RandomPolicy)]\n",
    "    \n",
    "#     if trainable_policies:\n",
    "#         print(f\"开始训练 {len(trainable_policies)} 个可训练策略...\")\n",
    "        \n",
    "#         # 使用off-policy训练器\n",
    "#         result = offpolicy_trainer(\n",
    "#             policy=policy_manager,\n",
    "#             train_collector=train_collector,\n",
    "#             test_collector=test_collector,\n",
    "#             max_epoch=args.epoch,\n",
    "#             step_per_epoch=args.step_per_epoch,\n",
    "#             step_per_collect=args.step_per_collect,\n",
    "#             episode_per_test=5,\n",
    "#             batch_size=args.batch_size,\n",
    "#             train_fn=train_fn,\n",
    "#             test_fn=test_fn,\n",
    "#             stop_fn=stop_fn,\n",
    "#             save_best_fn=save_best_fn,\n",
    "#             update_per_step=0.1,\n",
    "#             test_in_train=False,\n",
    "#             reward_metric=reward_metric\n",
    "#         )\n",
    "        \n",
    "#         print(\"训练完成！\")\n",
    "#         print(f\"训练结果: {result}\")\n",
    "#     else:\n",
    "#         print(\"所有策略都是随机策略，运行测试...\")\n",
    "#         result = test_collector.collect(n_episode=5)\n",
    "#         print(f\"随机策略测试结果 - 平均奖励: {result['rew']}, 平均长度: {result['len']}\")\n",
    "    \n",
    "#     return policy_manager, result\n",
    "\n",
    "# def watch_performance(args):\n",
    "#     \"\"\"观察策略表现\"\"\"\n",
    "#     print(\"观察策略表现...\")\n",
    "#     env = get_env(args)\n",
    "#     policies, _ = create_simple_policies(args, env)\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     # 尝试加载训练好的策略\n",
    "#     save_dir = \"waterworld_policies\"\n",
    "#     if os.path.exists(save_dir):\n",
    "#         for i, agent_id in enumerate(env.agents):\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'load_state_dict'):\n",
    "#                 model_path = os.path.join(save_dir, f\"{agent_id}_policy.pth\")\n",
    "#                 if os.path.exists(model_path):\n",
    "#                     policy_manager.policies[agent_id].load_state_dict(torch.load(model_path))\n",
    "#                     print(f\"加载了 {agent_id} 的策略\")\n",
    "    \n",
    "#     # 需要使用向量化环境\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#     collector = Collector(policy_manager, test_envs)\n",
    "    \n",
    "#     print(\"运行测试episode...\")\n",
    "#     result = collector.collect(n_episode=3, render=args.render)\n",
    "#     print(f\"平均奖励: {result['rew']}\")\n",
    "#     print(f\"平均长度: {result['len']}\")\n",
    "\n",
    "# def test_random_policies(args):\n",
    "#     \"\"\"测试纯随机策略作为基准\"\"\"\n",
    "#     print(\"测试纯随机策略...\")\n",
    "#     env = get_env(args)\n",
    "    \n",
    "#     # 创建所有随机策略\n",
    "#     policies = [RandomPolicy(env.action_space) for _ in env.agents]\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     # 需要使用向量化环境\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "    \n",
    "#     # 运行测试\n",
    "#     collector = Collector(policy_manager, test_envs)\n",
    "#     result = collector.collect(n_episode=5, render=args.render)\n",
    "#     print(f\"随机策略基准 - 平均奖励: {result['rew']}, 平均长度: {result['len']}\")\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# def simple_debug_test(args):\n",
    "#     \"\"\"最简单的debug测试\"\"\"\n",
    "#     print(\"=== Debug测试：基础环境和策略创建 ===\")\n",
    "    \n",
    "#     try:\n",
    "#         # 1. 测试环境创建\n",
    "#         print(\"1. 创建环境...\")\n",
    "#         env = get_env(args)\n",
    "#         print(f\"   环境创建成功: {type(env)}\")\n",
    "#         print(f\"   智能体: {env.agents}\")\n",
    "#         print(f\"   观察空间: {env.observation_space}\")\n",
    "#         print(f\"   动作空间: {env.action_space}\")\n",
    "        \n",
    "#         # 2. 测试策略创建\n",
    "#         print(\"\\n2. 创建策略...\")\n",
    "#         policies, optimizers = create_simple_policies(args, env)\n",
    "#         print(f\"   策略创建成功: {len(policies)} 个策略\")\n",
    "        \n",
    "#         # 3. 测试策略管理器\n",
    "#         print(\"\\n3. 创建策略管理器...\")\n",
    "#         policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "#         print(f\"   策略管理器创建成功: {type(policy_manager)}\")\n",
    "        \n",
    "#         # 4. 测试向量化环境\n",
    "#         print(\"\\n4. 创建向量化环境...\")\n",
    "#         test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#         print(f\"   向量化环境创建成功: {type(test_envs)}\")\n",
    "        \n",
    "#         # 5. 测试收集器\n",
    "#         print(\"\\n5. 创建收集器...\")\n",
    "#         collector = Collector(policy_manager, test_envs)\n",
    "#         print(f\"   收集器创建成功: {type(collector)}\")\n",
    "        \n",
    "#         # 6. 测试收集一个step\n",
    "#         print(\"\\n6. 测试收集数据...\")\n",
    "#         result = collector.collect(n_step=10)\n",
    "#         print(f\"   数据收集成功: {result}\")\n",
    "        \n",
    "#         print(\"\\n=== 所有基础测试通过！===\")\n",
    "#         return True\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n!!! 测试失败: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return False\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = get_args()\n",
    "    \n",
    "#     print(\"=== Tianshou Waterworld 多智能体测试 ===\")\n",
    "#     print(f\"设备: {args.device}\")\n",
    "#     print(f\"智能体数量: {args.n_pursuers}\")\n",
    "    \n",
    "#     # 首先运行debug测试\n",
    "#     debug_success = simple_debug_test(args)\n",
    "    \n",
    "#     if not debug_success:\n",
    "#         print(\"基础测试失败，请检查环境配置\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     if args.watch:\n",
    "#         watch_performance(args)\n",
    "#     else:\n",
    "#         # 首先测试随机策略\n",
    "#         print(\"\\n1. 测试随机策略基准:\")\n",
    "#         test_random_policies(args)\n",
    "        \n",
    "#         # 然后训练DQN策略\n",
    "#         print(\"\\n2. 训练DQN策略:\")\n",
    "#         policy_manager, result = train_agents(args)\n",
    "        \n",
    "#         # 训练完成后观察表现\n",
    "#         if result:\n",
    "#             print(\"\\n3. 观察训练后的策略表现:\")\n",
    "#             args.render = True\n",
    "#             watch_performance(args)\n",
    "\n",
    "# \"\"\"\n",
    "# 使用说明：\n",
    "\n",
    "# 1. 基础测试（只用随机策略）：\n",
    "#    python waterworld_simple.py\n",
    "\n",
    "# 2. 观察训练结果：\n",
    "#    python waterworld_simple.py --watch --render\n",
    "\n",
    "# 3. 自定义参数：\n",
    "#    python waterworld_simple.py --epoch 20 --lr 1e-3\n",
    "\n",
    "# 这个简化版本的特点：\n",
    "\n",
    "# 1. 使用DQN处理连续动作空间（通过离散化）\n",
    "# 2. 混合DQN和随机策略\n",
    "# 3. 更强的错误处理和兼容性\n",
    "# 4. 渐进式测试方法\n",
    "\n",
    "# 如果这个版本能正常运行，我们再逐步添加更复杂的算法。\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3374da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Tianshou Waterworld 简化版本 - 先测试基本功能\n",
    "# 每个agent使用独立的策略（从随机策略开始）\n",
    "\n",
    "# 安装依赖：\n",
    "# pip install pettingzoo[sisl]\n",
    "# pip install tianshou\n",
    "# pip install torch\n",
    "# pip install numpy\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from typing import Optional, Tuple, List\n",
    "\n",
    "# # Tianshou imports\n",
    "# from tianshou.data import Collector, VectorReplayBuffer\n",
    "# from tianshou.env import DummyVectorEnv\n",
    "# from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "# from tianshou.policy import (\n",
    "#     BasePolicy, \n",
    "#     DQNPolicy, \n",
    "#     MultiAgentPolicyManager, \n",
    "#     RandomPolicy\n",
    "# )\n",
    "# from tianshou.trainer import offpolicy_trainer\n",
    "# from tianshou.utils.net.common import Net\n",
    "\n",
    "# # PettingZoo imports\n",
    "# from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "# def get_args():\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--seed', type=int, default=42)\n",
    "#     parser.add_argument('--buffer-size', type=int, default=20000)\n",
    "#     parser.add_argument('--lr', type=float, default=1e-4)\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99)\n",
    "#     parser.add_argument('--epoch', type=int, default=10)  # 减少训练轮数\n",
    "#     parser.add_argument('--step-per-epoch', type=int, default=1000)\n",
    "#     parser.add_argument('--step-per-collect', type=int, default=200)\n",
    "#     parser.add_argument('--batch-size', type=int, default=64)\n",
    "#     parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[128, 128])\n",
    "#     parser.add_argument('--training-num', type=int, default=4)\n",
    "#     parser.add_argument('--test-num', type=int, default=2)\n",
    "#     parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     parser.add_argument('--watch', action='store_true', default=False)\n",
    "#     parser.add_argument('--render', action='store_true', default=False)\n",
    "    \n",
    "#     # Waterworld specific parameters\n",
    "#     parser.add_argument('--n-pursuers', type=int, default=5)\n",
    "    \n",
    "#     return parser.parse_known_args()[0]\n",
    "\n",
    "# def get_env(args):\n",
    "#     \"\"\"创建Waterworld环境\"\"\"\n",
    "#     env = waterworld_v4.env(\n",
    "#         n_pursuers=args.n_pursuers,\n",
    "#         n_evaders=5,\n",
    "#         n_poisons=10,\n",
    "#         n_coop=2,\n",
    "#         n_sensors=30,\n",
    "#         sensor_range=0.2,\n",
    "#         radius=0.015,\n",
    "#         pursuer_max_accel=0.01,\n",
    "#         evader_speed=0.01,\n",
    "#         poison_speed=0.01,\n",
    "#         poison_reward=-1.0,\n",
    "#         food_reward=10.0,\n",
    "#         encounter_reward=0.01,\n",
    "#         thrust_penalty=-0.5,\n",
    "#         local_ratio=1.0,\n",
    "#         speed_features=True,\n",
    "#         max_cycles=500,\n",
    "#         render_mode=\"human\" if args.render else None\n",
    "#     )\n",
    "#     return PettingZooEnv(env)\n",
    "\n",
    "# def create_simple_policies(args, env) -> Tuple[List[BasePolicy], List]:\n",
    "#     \"\"\"\n",
    "#     创建简单的策略组合：\n",
    "#     - 一些使用DQN（离散化动作）\n",
    "#     - 一些使用随机策略\n",
    "#     \"\"\"\n",
    "#     agents = env.agents\n",
    "#     policies = []\n",
    "#     optimizers = []\n",
    "    \n",
    "#     # 获取环境信息\n",
    "#     observation_space = env.observation_space\n",
    "#     action_space = env.action_space\n",
    "    \n",
    "#     state_shape = observation_space.shape or observation_space.n\n",
    "#     action_shape = action_space.shape or action_space.n\n",
    "    \n",
    "#     print(f\"状态空间维度: {state_shape}\")\n",
    "#     print(f\"动作空间: {action_space}\")\n",
    "#     print(f\"智能体数量: {len(agents)}\")\n",
    "    \n",
    "#     # 为连续动作空间创建离散化版本的DQN\n",
    "#     # 将连续动作 [-0.01, 0.01] x [-0.01, 0.01] 离散化为9个动作\n",
    "#     discrete_actions = [\n",
    "#         [-0.01, -0.01], [-0.01, 0.0], [-0.01, 0.01],  # 左上, 左, 左下\n",
    "#         [0.0, -0.01],   [0.0, 0.0],   [0.0, 0.01],    # 上, 停止, 下  \n",
    "#         [0.01, -0.01],  [0.01, 0.0],  [0.01, 0.01]    # 右上, 右, 右下\n",
    "#     ]\n",
    "    \n",
    "#     # 为每个agent分配策略\n",
    "#     algorithm_assignment = {\n",
    "#         'pursuer_0': 'DQN',      # 使用DQN\n",
    "#         'pursuer_1': 'DQN',      # 使用DQN\n",
    "#         'pursuer_2': 'Random',   # 随机策略\n",
    "#         'pursuer_3': 'Random',   # 随机策略\n",
    "#         'pursuer_4': 'DQN',      # 使用DQN\n",
    "#     }\n",
    "    \n",
    "#     for i, agent_id in enumerate(agents):\n",
    "#         algo = algorithm_assignment.get(agent_id, 'Random')\n",
    "#         print(f\"Agent {agent_id} 使用算法: {algo}\")\n",
    "        \n",
    "#         if algo == 'DQN':\n",
    "#             # 创建DQN网络\n",
    "#             net = Net(\n",
    "#                 state_shape=state_shape,\n",
    "#                 action_shape=len(discrete_actions),  # 9个离散动作\n",
    "#                 hidden_sizes=args.hidden_sizes,\n",
    "#                 device=args.device\n",
    "#             ).to(args.device)\n",
    "            \n",
    "#             optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "            \n",
    "#             # 创建DQN策略\n",
    "#             policy = DQNPolicy(\n",
    "#                 model=net,\n",
    "#                 optim=optim,\n",
    "#                 discount_factor=args.gamma,\n",
    "#                 estimation_step=3,\n",
    "#                 target_update_freq=320\n",
    "#             )\n",
    "            \n",
    "#             # 为DQN策略添加动作转换函数\n",
    "#             policy.discrete_actions = discrete_actions\n",
    "#             original_forward = policy.forward\n",
    "            \n",
    "#             def dqn_forward_with_continuous_output(batch, state=None, **kwargs):\n",
    "#                 # 调用原始forward得到离散动作\n",
    "#                 result = original_forward(batch, state, **kwargs)\n",
    "                \n",
    "#                 # 将离散动作索引转换为连续动作\n",
    "#                 if hasattr(result, 'act'):\n",
    "#                     discrete_indices = result.act\n",
    "#                     continuous_actions = []\n",
    "                    \n",
    "#                     for idx in discrete_indices:\n",
    "#                         if isinstance(idx, torch.Tensor):\n",
    "#                             idx = idx.item()\n",
    "#                         continuous_actions.append(discrete_actions[idx])\n",
    "                    \n",
    "#                     result.act = np.array(continuous_actions)\n",
    "                \n",
    "#                 return result\n",
    "            \n",
    "#             policy.forward = dqn_forward_with_continuous_output\n",
    "#             optimizers.append(optim)\n",
    "            \n",
    "#         else:  # Random policy\n",
    "#             policy = RandomPolicy(action_space)\n",
    "#             optimizers.append(None)\n",
    "        \n",
    "#         policies.append(policy)\n",
    "    \n",
    "#     return policies, optimizers\n",
    "\n",
    "# def train_agents(args):\n",
    "#     \"\"\"训练多智能体\"\"\"\n",
    "#     print(\"初始化环境...\")\n",
    "    \n",
    "#     # 环境设置\n",
    "#     env = get_env(args)\n",
    "#     train_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.training_num)])\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.test_num)])\n",
    "    \n",
    "#     # 设置随机种子\n",
    "#     np.random.seed(args.seed)\n",
    "#     torch.manual_seed(args.seed)\n",
    "#     train_envs.seed(args.seed)\n",
    "#     test_envs.seed(args.seed)\n",
    "    \n",
    "#     print(\"创建策略...\")\n",
    "#     # 创建智能体策略\n",
    "#     policies, optimizers = create_simple_policies(args, env)\n",
    "    \n",
    "#     # 创建多智能体策略管理器\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     print(\"创建收集器...\")\n",
    "#     # 创建收集器\n",
    "#     train_collector = Collector(\n",
    "#         policy_manager,\n",
    "#         train_envs,\n",
    "#         VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "#         exploration_noise=True\n",
    "#     )\n",
    "    \n",
    "#     test_collector = Collector(\n",
    "#         policy_manager, \n",
    "#         test_envs,\n",
    "#         exploration_noise=True\n",
    "#     )\n",
    "    \n",
    "#     # 预收集一些数据\n",
    "#     print(\"预收集数据...\")\n",
    "#     train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "    \n",
    "#     # 回调函数\n",
    "#     def save_best_fn(policy):\n",
    "#         \"\"\"保存最佳策略\"\"\"\n",
    "#         save_dir = \"waterworld_policies\"\n",
    "#         os.makedirs(save_dir, exist_ok=True)\n",
    "#         for i, agent_id in enumerate(env.agents):\n",
    "#             if hasattr(policy.policies[agent_id], 'state_dict'):\n",
    "#                 model_path = os.path.join(save_dir, f\"{agent_id}_policy.pth\")\n",
    "#                 torch.save(policy.policies[agent_id].state_dict(), model_path)\n",
    "#         print(f\"策略已保存到 {save_dir}\")\n",
    "    \n",
    "#     def stop_fn(mean_rewards):\n",
    "#         \"\"\"停止条件\"\"\"\n",
    "#         return mean_rewards >= 5.0\n",
    "    \n",
    "#     def train_fn(epoch, env_step):\n",
    "#         \"\"\"训练时设置探索率\"\"\"\n",
    "#         for agent_id in env.agents:\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'set_eps'):\n",
    "#                 policy_manager.policies[agent_id].set_eps(0.1)\n",
    "    \n",
    "#     def test_fn(epoch, env_step):\n",
    "#         \"\"\"测试时设置探索率\"\"\"\n",
    "#         for agent_id in env.agents:\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'set_eps'):\n",
    "#                 policy_manager.policies[agent_id].set_eps(0.05)\n",
    "    \n",
    "#     def reward_metric(rews):\n",
    "#         \"\"\"奖励度量函数 - 返回所有agent的平均奖励\"\"\"\n",
    "#         return rews.mean(axis=1)\n",
    "    \n",
    "#     # 检查是否有可训练的策略\n",
    "#     trainable_policies = [p for p in policies if not isinstance(p, RandomPolicy)]\n",
    "    \n",
    "#     if trainable_policies:\n",
    "#         print(f\"开始训练 {len(trainable_policies)} 个可训练策略...\")\n",
    "        \n",
    "#         # 使用off-policy训练器\n",
    "#         result = offpolicy_trainer(\n",
    "#             policy=policy_manager,\n",
    "#             train_collector=train_collector,\n",
    "#             test_collector=test_collector,\n",
    "#             max_epoch=args.epoch,\n",
    "#             step_per_epoch=args.step_per_epoch,\n",
    "#             step_per_collect=args.step_per_collect,\n",
    "#             episode_per_test=5,\n",
    "#             batch_size=args.batch_size,\n",
    "#             train_fn=train_fn,\n",
    "#             test_fn=test_fn,\n",
    "#             stop_fn=stop_fn,\n",
    "#             save_best_fn=save_best_fn,\n",
    "#             update_per_step=0.1,\n",
    "#             test_in_train=False,\n",
    "#             reward_metric=reward_metric\n",
    "#         )\n",
    "        \n",
    "#         print(\"训练完成！\")\n",
    "#         print(f\"训练结果: {result}\")\n",
    "#     else:\n",
    "#         print(\"所有策略都是随机策略，运行测试...\")\n",
    "#         result = test_collector.collect(n_episode=5)\n",
    "#         print(f\"随机策略测试结果 - 平均奖励: {result['rew']}, 平均长度: {result['len']}\")\n",
    "    \n",
    "#     return policy_manager, result\n",
    "\n",
    "# def watch_performance(args):\n",
    "#     \"\"\"观察策略表现\"\"\"\n",
    "#     print(\"观察策略表现...\")\n",
    "#     env = get_env(args)\n",
    "#     policies, _ = create_simple_policies(args, env)\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     # 尝试加载训练好的策略\n",
    "#     save_dir = \"waterworld_policies\"\n",
    "#     if os.path.exists(save_dir):\n",
    "#         for i, agent_id in enumerate(env.agents):\n",
    "#             if hasattr(policy_manager.policies[agent_id], 'load_state_dict'):\n",
    "#                 model_path = os.path.join(save_dir, f\"{agent_id}_policy.pth\")\n",
    "#                 if os.path.exists(model_path):\n",
    "#                     policy_manager.policies[agent_id].load_state_dict(torch.load(model_path))\n",
    "#                     print(f\"加载了 {agent_id} 的策略\")\n",
    "    \n",
    "#     # 需要使用向量化环境\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#     collector = Collector(policy_manager, test_envs)\n",
    "    \n",
    "#     print(\"运行测试episode...\")\n",
    "#     result = collector.collect(n_episode=3, render=args.render)\n",
    "#     print(f\"平均奖励: {result['rew']}\")\n",
    "#     print(f\"平均长度: {result['len']}\")\n",
    "\n",
    "# def test_random_policies(args):\n",
    "#     \"\"\"测试纯随机策略作为基准\"\"\"\n",
    "#     print(\"测试纯随机策略...\")\n",
    "#     env = get_env(args)\n",
    "    \n",
    "#     # 创建所有随机策略\n",
    "#     policies = [RandomPolicy(env.action_space) for _ in env.agents]\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     # 需要使用向量化环境\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "    \n",
    "#     # 运行测试\n",
    "#     collector = Collector(policy_manager, test_envs)\n",
    "#     result = collector.collect(n_episode=5, render=args.render)\n",
    "#     print(f\"随机策略基准 - 平均奖励: {result['rew']}, 平均长度: {result['len']}\")\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# def simple_debug_test(args):\n",
    "#     \"\"\"最简单的debug测试\"\"\"\n",
    "#     print(\"=== Debug测试：基础环境和策略创建 ===\")\n",
    "    \n",
    "#     try:\n",
    "#         # 1. 测试环境创建\n",
    "#         print(\"1. 创建环境...\")\n",
    "#         env = get_env(args)\n",
    "#         print(f\"   环境创建成功: {type(env)}\")\n",
    "#         print(f\"   智能体: {env.agents}\")\n",
    "#         print(f\"   观察空间: {env.observation_space}\")\n",
    "#         print(f\"   动作空间: {env.action_space}\")\n",
    "        \n",
    "#         # 2. 测试策略创建\n",
    "#         print(\"\\n2. 创建策略...\")\n",
    "#         policies, optimizers = create_simple_policies(args, env)\n",
    "#         print(f\"   策略创建成功: {len(policies)} 个策略\")\n",
    "        \n",
    "#         # 3. 测试策略管理器\n",
    "#         print(\"\\n3. 创建策略管理器...\")\n",
    "#         policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "#         print(f\"   策略管理器创建成功: {type(policy_manager)}\")\n",
    "        \n",
    "#         # 4. 测试向量化环境\n",
    "#         print(\"\\n4. 创建向量化环境...\")\n",
    "#         test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#         print(f\"   向量化环境创建成功: {type(test_envs)}\")\n",
    "        \n",
    "#         # 5. 测试收集器\n",
    "#         print(\"\\n5. 创建收集器...\")\n",
    "#         collector = Collector(policy_manager, test_envs)\n",
    "#         print(f\"   收集器创建成功: {type(collector)}\")\n",
    "        \n",
    "#         # 6. 测试收集一个step\n",
    "#         print(\"\\n6. 测试收集数据...\")\n",
    "#         result = collector.collect(n_step=10)\n",
    "#         print(f\"   数据收集成功: {result}\")\n",
    "        \n",
    "#         print(\"\\n=== 所有基础测试通过！===\")\n",
    "#         return True\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n!!! 测试失败: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return False\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = get_args()\n",
    "    \n",
    "#     print(\"=== Tianshou Waterworld 多智能体测试 ===\")\n",
    "#     print(f\"设备: {args.device}\")\n",
    "#     print(f\"智能体数量: {args.n_pursuers}\")\n",
    "    \n",
    "#     # 首先运行debug测试\n",
    "#     debug_success = simple_debug_test(args)\n",
    "    \n",
    "#     if not debug_success:\n",
    "#         print(\"基础测试失败，请检查环境配置\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     if args.watch:\n",
    "#         watch_performance(args)\n",
    "#     else:\n",
    "#         # 首先测试随机策略\n",
    "#         print(\"\\n1. 测试随机策略基准:\")\n",
    "#         test_random_policies(args)\n",
    "        \n",
    "#         # 然后训练DQN策略\n",
    "#         print(\"\\n2. 训练DQN策略:\")\n",
    "#         policy_manager, result = train_agents(args)\n",
    "        \n",
    "#         # 训练完成后观察表现\n",
    "#         if result:\n",
    "#             print(\"\\n3. 观察训练后的策略表现:\")\n",
    "#             args.render = True\n",
    "#             watch_performance(args)\n",
    "\n",
    "# \"\"\"\n",
    "# 使用说明：\n",
    "\n",
    "# 1. 基础测试（只用随机策略）：\n",
    "#    python waterworld_simple.py\n",
    "\n",
    "# 2. 观察训练结果：\n",
    "#    python waterworld_simple.py --watch --render\n",
    "\n",
    "# 3. 自定义参数：\n",
    "#    python waterworld_simple.py --epoch 20 --lr 1e-3\n",
    "\n",
    "# 这个简化版本的特点：\n",
    "\n",
    "# 1. 使用DQN处理连续动作空间（通过离散化）\n",
    "# 2. 混合DQN和随机策略\n",
    "# 3. 更强的错误处理和兼容性\n",
    "# 4. 渐进式测试方法\n",
    "\n",
    "# 如果这个版本能正常运行，我们再逐步添加更复杂的算法。\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d76d640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Tianshou Waterworld 修复版本\n",
    "# 解决RandomPolicy在连续动作空间的mask问题\n",
    "\n",
    "# 每个agent使用独立的策略：\n",
    "# - DQN (通过离散化处理连续动作)\n",
    "# - 自定义连续随机策略\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from typing import Optional, Tuple, List\n",
    "\n",
    "# # Tianshou imports\n",
    "# from tianshou.data import Collector, VectorReplayBuffer, Batch\n",
    "# from tianshou.env import DummyVectorEnv\n",
    "# from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "# from tianshou.policy import (\n",
    "#     BasePolicy, \n",
    "#     DQNPolicy, \n",
    "#     MultiAgentPolicyManager\n",
    "# )\n",
    "# from tianshou.trainer import offpolicy_trainer\n",
    "# from tianshou.utils.net.common import Net\n",
    "\n",
    "# # PettingZoo imports\n",
    "# from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "# class ContinuousRandomPolicy(BasePolicy):\n",
    "#     \"\"\"自定义连续动作随机策略\"\"\"\n",
    "#     def __init__(self, action_space):\n",
    "#         super().__init__(action_space=action_space)\n",
    "#         self.action_space = action_space\n",
    "    \n",
    "#     def forward(self, batch, state=None, **kwargs):\n",
    "#         \"\"\"前向传播，直接采样随机动作\"\"\"\n",
    "#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:\n",
    "#             batch_size = batch.obs.shape[0]\n",
    "#         else:\n",
    "#             batch_size = 1\n",
    "            \n",
    "#         actions = []\n",
    "#         for _ in range(batch_size):\n",
    "#             action = self.action_space.sample()\n",
    "#             actions.append(action)\n",
    "        \n",
    "#         return Batch(act=np.array(actions))\n",
    "    \n",
    "#     def exploration_noise(self, act, batch):\n",
    "#         \"\"\"随机策略不需要额外的探索噪声\"\"\"\n",
    "#         return act\n",
    "    \n",
    "#     def learn(self, batch, **kwargs):\n",
    "#         \"\"\"随机策略不需要学习\"\"\"\n",
    "#         return {}\n",
    "\n",
    "# def get_args():\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--seed', type=int, default=42)\n",
    "#     parser.add_argument('--buffer-size', type=int, default=20000)\n",
    "#     parser.add_argument('--lr', type=float, default=1e-4)\n",
    "#     parser.add_argument('--gamma', type=float, default=0.99)\n",
    "#     parser.add_argument('--epoch', type=int, default=5)  # 进一步减少训练轮数\n",
    "#     parser.add_argument('--step-per-epoch', type=int, default=500)\n",
    "#     parser.add_argument('--step-per-collect', type=int, default=100)\n",
    "#     parser.add_argument('--batch-size', type=int, default=64)\n",
    "#     parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[128, 128])\n",
    "#     parser.add_argument('--training-num', type=int, default=2)  # 减少并行环境数\n",
    "#     parser.add_argument('--test-num', type=int, default=1)\n",
    "#     parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     parser.add_argument('--watch', action='store_true', default=False)\n",
    "#     parser.add_argument('--render', action='store_true', default=False)\n",
    "    \n",
    "#     # Waterworld specific parameters\n",
    "#     parser.add_argument('--n-pursuers', type=int, default=5)\n",
    "    \n",
    "#     return parser.parse_known_args()[0]\n",
    "\n",
    "# def get_env(args):\n",
    "#     \"\"\"创建Waterworld环境\"\"\"\n",
    "#     env = waterworld_v4.env(\n",
    "#         n_pursuers=args.n_pursuers,\n",
    "#         n_evaders=5,\n",
    "#         n_poisons=10,\n",
    "#         n_coop=2,\n",
    "#         n_sensors=30,\n",
    "#         sensor_range=0.2,\n",
    "#         radius=0.015,\n",
    "#         pursuer_max_accel=0.01,\n",
    "#         evader_speed=0.01,\n",
    "#         poison_speed=0.01,\n",
    "#         poison_reward=-1.0,\n",
    "#         food_reward=10.0,\n",
    "#         encounter_reward=0.01,\n",
    "#         thrust_penalty=-0.5,\n",
    "#         local_ratio=1.0,\n",
    "#         speed_features=True,\n",
    "#         max_cycles=500,\n",
    "#         render_mode=\"human\" if args.render else None\n",
    "#     )\n",
    "#     return PettingZooEnv(env)\n",
    "\n",
    "# def create_dqn_policy(args, state_shape, action_space, discrete_actions):\n",
    "#     \"\"\"创建DQN策略\"\"\"\n",
    "#     # 创建DQN网络\n",
    "#     net = Net(\n",
    "#         state_shape=state_shape,\n",
    "#         action_shape=len(discrete_actions),  # 9个离散动作\n",
    "#         hidden_sizes=args.hidden_sizes,\n",
    "#         device=args.device\n",
    "#     ).to(args.device)\n",
    "    \n",
    "#     optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    \n",
    "#     # 创建DQN策略\n",
    "#     policy = DQNPolicy(\n",
    "#         model=net,\n",
    "#         optim=optim,\n",
    "#         discount_factor=args.gamma,\n",
    "#         estimation_step=3,\n",
    "#         target_update_freq=320\n",
    "#     )\n",
    "    \n",
    "#     # 修复max_action_num属性\n",
    "#     policy.max_action_num = len(discrete_actions)\n",
    "    \n",
    "#     # 添加动作转换函数\n",
    "#     policy.discrete_actions = discrete_actions\n",
    "#     original_forward = policy.forward\n",
    "#     original_exploration_noise = policy.exploration_noise\n",
    "    \n",
    "#     def dqn_forward_with_continuous_output(batch, state=None, **kwargs):\n",
    "#         # 调用原始forward得到离散动作\n",
    "#         result = original_forward(batch, state, **kwargs)\n",
    "        \n",
    "#         # 将离散动作索引转换为连续动作\n",
    "#         if hasattr(result, 'act'):\n",
    "#             discrete_indices = result.act\n",
    "#             continuous_actions = []\n",
    "            \n",
    "#             # 处理不同的batch结构\n",
    "#             if isinstance(discrete_indices, np.ndarray):\n",
    "#                 for idx in discrete_indices:\n",
    "#                     if isinstance(idx, (torch.Tensor, np.ndarray)):\n",
    "#                         idx = int(idx.item() if hasattr(idx, 'item') else idx)\n",
    "#                     continuous_actions.append(discrete_actions[idx])\n",
    "#             else:\n",
    "#                 if isinstance(discrete_indices, (torch.Tensor, np.ndarray)):\n",
    "#                     idx = int(discrete_indices.item() if hasattr(discrete_indices, 'item') else discrete_indices)\n",
    "#                 else:\n",
    "#                     idx = int(discrete_indices)\n",
    "#                 continuous_actions.append(discrete_actions[idx])\n",
    "            \n",
    "#             result.act = np.array(continuous_actions)\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     def custom_exploration_noise(act, batch):\n",
    "#         \"\"\"自定义探索噪声函数\"\"\"\n",
    "#         if not hasattr(policy, 'eps'):\n",
    "#             return act\n",
    "            \n",
    "#         bsz = len(act) if hasattr(act, '__len__') else 1\n",
    "#         rand_mask = np.random.rand(bsz) < policy.eps\n",
    "        \n",
    "#         if np.any(rand_mask):\n",
    "#             # 对需要随机的动作，直接从discrete_actions中随机选择\n",
    "#             for i in range(bsz):\n",
    "#                 if rand_mask[i]:\n",
    "#                     random_action_idx = np.random.randint(0, len(discrete_actions))\n",
    "#                     act[i] = discrete_actions[random_action_idx]\n",
    "        \n",
    "#         return act\n",
    "    \n",
    "#     policy.forward = dqn_forward_with_continuous_output\n",
    "#     policy.exploration_noise = custom_exploration_noise\n",
    "    \n",
    "#     return policy, optim\n",
    "\n",
    "# def create_policies(args, env) -> Tuple[List[BasePolicy], List]:\n",
    "#     \"\"\"创建策略组合\"\"\"\n",
    "#     agents = env.agents\n",
    "#     policies = []\n",
    "#     optimizers = []\n",
    "    \n",
    "#     # 获取环境信息\n",
    "#     observation_space = env.observation_space\n",
    "#     action_space = env.action_space\n",
    "    \n",
    "#     state_shape = observation_space.shape\n",
    "    \n",
    "#     print(f\"状态空间维度: {state_shape}\")\n",
    "#     print(f\"动作空间: {action_space}\")\n",
    "#     print(f\"智能体数量: {len(agents)}\")\n",
    "    \n",
    "#     # 离散化动作\n",
    "#     discrete_actions = [\n",
    "#         [-0.01, -0.01], [-0.01, 0.0], [-0.01, 0.01],  # 左上, 左, 左下\n",
    "#         [0.0, -0.01],   [0.0, 0.0],   [0.0, 0.01],    # 上, 停止, 下  \n",
    "#         [0.01, -0.01],  [0.01, 0.0],  [0.01, 0.01]    # 右上, 右, 右下\n",
    "#     ]\n",
    "    \n",
    "#     # 为每个agent分配策略\n",
    "#     algorithm_assignment = {\n",
    "#         'pursuer_0': 'DQN',\n",
    "#         'pursuer_1': 'DQN',\n",
    "#         'pursuer_2': 'Random',\n",
    "#         'pursuer_3': 'Random',\n",
    "#         'pursuer_4': 'DQN',\n",
    "#     }\n",
    "    \n",
    "#     for agent_id in agents:\n",
    "#         algo = algorithm_assignment.get(agent_id, 'Random')\n",
    "#         print(f\"Agent {agent_id} 使用算法: {algo}\")\n",
    "        \n",
    "#         if algo == 'DQN':\n",
    "#             policy, optim = create_dqn_policy(args, state_shape, action_space, discrete_actions)\n",
    "#             optimizers.append(optim)\n",
    "#         else:  # Random policy\n",
    "#             policy = ContinuousRandomPolicy(action_space)\n",
    "#             optimizers.append(None)\n",
    "        \n",
    "#         policies.append(policy)\n",
    "    \n",
    "#     return policies, optimizers\n",
    "\n",
    "# def simple_test(args):\n",
    "#     \"\"\"简单测试\"\"\"\n",
    "#     print(\"=== 简单测试 ===\")\n",
    "    \n",
    "#     # 创建环境\n",
    "#     env = get_env(args)\n",
    "#     print(f\"环境创建成功，智能体: {env.agents}\")\n",
    "    \n",
    "#     # 创建策略\n",
    "#     policies, optimizers = create_policies(args, env)\n",
    "#     print(f\"策略创建成功: {len(policies)} 个\")\n",
    "    \n",
    "#     # 创建策略管理器\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "#     print(\"策略管理器创建成功\")\n",
    "    \n",
    "#     # 创建向量化环境\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#     print(\"向量化环境创建成功\")\n",
    "    \n",
    "#     # 创建收集器\n",
    "#     collector = Collector(policy_manager, test_envs)\n",
    "#     print(\"收集器创建成功\")\n",
    "    \n",
    "#     # 收集数据测试\n",
    "#     print(\"开始收集测试数据...\")\n",
    "#     result = collector.collect(n_step=20)\n",
    "#     print(f\"测试成功！收集了 {result['n/st']} 步数据\")\n",
    "    \n",
    "#     return True\n",
    "\n",
    "# def train_agents(args):\n",
    "#     \"\"\"训练智能体\"\"\"\n",
    "#     print(\"=== 开始训练 ===\")\n",
    "    \n",
    "#     # 环境设置\n",
    "#     env = get_env(args)\n",
    "#     train_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.training_num)])\n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.test_num)])\n",
    "    \n",
    "#     # 设置随机种子\n",
    "#     np.random.seed(args.seed)\n",
    "#     torch.manual_seed(args.seed)\n",
    "#     train_envs.seed(args.seed)\n",
    "#     test_envs.seed(args.seed)\n",
    "    \n",
    "#     # 创建策略\n",
    "#     policies, optimizers = create_policies(args, env)\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     # 创建收集器\n",
    "#     train_collector = Collector(\n",
    "#         policy_manager,\n",
    "#         train_envs,\n",
    "#         VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "#         exploration_noise=True\n",
    "#     )\n",
    "    \n",
    "#     test_collector = Collector(policy_manager, test_envs)\n",
    "    \n",
    "#     # 预收集数据\n",
    "#     print(\"预收集训练数据...\")\n",
    "#     train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "    \n",
    "#     # 检查可训练策略\n",
    "#     trainable_policies = [p for p in policies if hasattr(p, 'optim')]\n",
    "#     print(f\"发现 {len(trainable_policies)} 个可训练策略\")\n",
    "    \n",
    "#     if trainable_policies:\n",
    "#         print(\"开始训练...\")\n",
    "        \n",
    "#         # 回调函数\n",
    "#         def save_best_fn(policy):\n",
    "#             save_dir = \"waterworld_policies\"\n",
    "#             os.makedirs(save_dir, exist_ok=True)\n",
    "#             print(f\"策略已保存到 {save_dir}\")\n",
    "        \n",
    "#         def stop_fn(mean_rewards):\n",
    "#             return mean_rewards >= 3.0  # 降低停止条件\n",
    "        \n",
    "#         def train_fn(epoch, env_step):\n",
    "#             for agent_id in env.agents:\n",
    "#                 if hasattr(policy_manager.policies[agent_id], 'set_eps'):\n",
    "#                     policy_manager.policies[agent_id].set_eps(0.1)\n",
    "        \n",
    "#         def test_fn(epoch, env_step):\n",
    "#             for agent_id in env.agents:\n",
    "#                 if hasattr(policy_manager.policies[agent_id], 'set_eps'):\n",
    "#                     policy_manager.policies[agent_id].set_eps(0.05)\n",
    "        \n",
    "#         def reward_metric(rews):\n",
    "#             return rews.mean(axis=1)\n",
    "        \n",
    "#         # 开始训练\n",
    "#         result = offpolicy_trainer(\n",
    "#             policy=policy_manager,\n",
    "#             train_collector=train_collector,\n",
    "#             test_collector=test_collector,\n",
    "#             max_epoch=args.epoch,\n",
    "#             step_per_epoch=args.step_per_epoch,\n",
    "#             step_per_collect=args.step_per_collect,\n",
    "#             episode_per_test=3,\n",
    "#             batch_size=args.batch_size,\n",
    "#             train_fn=train_fn,\n",
    "#             test_fn=test_fn,\n",
    "#             stop_fn=stop_fn,\n",
    "#             save_best_fn=save_best_fn,\n",
    "#             update_per_step=0.1,\n",
    "#             test_in_train=False,\n",
    "#             reward_metric=reward_metric\n",
    "#         )\n",
    "        \n",
    "#         print(\"训练完成！\")\n",
    "#         print(f\"最终结果: {result}\")\n",
    "#         return policy_manager, result\n",
    "#     else:\n",
    "#         print(\"没有可训练策略，运行随机测试...\")\n",
    "#         result = test_collector.collect(n_episode=3)\n",
    "#         print(f\"随机测试结果: {result}\")\n",
    "#         return policy_manager, result\n",
    "\n",
    "# def watch_performance(args):\n",
    "#     \"\"\"观察性能\"\"\"\n",
    "#     print(\"=== 观察性能 ===\")\n",
    "    \n",
    "#     env = get_env(args)\n",
    "#     policies, _ = create_policies(args, env)\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "    \n",
    "#     test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#     collector = Collector(policy_manager, test_envs)\n",
    "    \n",
    "#     result = collector.collect(n_episode=3, render=args.render)\n",
    "#     print(f\"性能测试结果:\")\n",
    "#     print(f\"  平均奖励: {result['rew']}\")\n",
    "#     print(f\"  平均长度: {result['len']}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = get_args()\n",
    "    \n",
    "#     print(\"=== Tianshou Waterworld 修复版本 ===\")\n",
    "#     print(f\"设备: {args.device}\")\n",
    "#     print(f\"智能体数量: {args.n_pursuers}\")\n",
    "    \n",
    "#     # 1. 简单测试\n",
    "#     print(\"\\n1. 运行简单测试...\")\n",
    "#     if not simple_test(args):\n",
    "#         print(\"简单测试失败\")\n",
    "#         exit(1)\n",
    "    \n",
    "#     if args.watch:\n",
    "#         # 2. 观察性能\n",
    "#         watch_performance(args)\n",
    "#     else:\n",
    "#         # 3. 训练\n",
    "#         print(\"\\n2. 开始训练...\")\n",
    "#         policy_manager, result = train_agents(args)\n",
    "        \n",
    "#         # 4. 观察结果\n",
    "#         print(\"\\n3. 观察训练结果...\")\n",
    "#         watch_performance(args)\n",
    "\n",
    "# \"\"\"\n",
    "# 使用说明：\n",
    "\n",
    "# 1. 基础运行:\n",
    "#    python waterworld_fixed.py\n",
    "\n",
    "# 2. 观察模式:\n",
    "#    python waterworld_fixed.py --watch --render\n",
    "\n",
    "# 3. 快速测试:\n",
    "#    python waterworld_fixed.py --epoch 2 --step-per-epoch 200\n",
    "\n",
    "# 修复内容:\n",
    "# - 自定义ContinuousRandomPolicy解决mask问题\n",
    "# - 简化DQN动作转换逻辑\n",
    "# - 更好的错误处理\n",
    "# - 渐进式测试流程\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3d7c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Tianshou Waterworld 稳定版本\n",
    "# 先用纯随机策略验证框架，然后逐步添加可训练策略\n",
    "\n",
    "# 每个agent使用不同类型的随机策略来模拟\"独立算法\"的概念\n",
    "# \"\"\"\n",
    "\n",
    "# import os\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from typing import Optional, Tuple, List\n",
    "\n",
    "# # Tianshou imports\n",
    "# from tianshou.data import Collector, Batch\n",
    "# from tianshou.env import DummyVectorEnv\n",
    "# from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "# from tianshou.policy import BasePolicy, MultiAgentPolicyManager\n",
    "\n",
    "# # PettingZoo imports\n",
    "# from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "# class ContinuousRandomPolicy(BasePolicy):\n",
    "#     \"\"\"基础连续随机策略\"\"\"\n",
    "#     def __init__(self, action_space, policy_name=\"Random\"):\n",
    "#         super().__init__(action_space=action_space)\n",
    "#         self.action_space = action_space\n",
    "#         self.policy_name = policy_name\n",
    "    \n",
    "#     def forward(self, batch, state=None, **kwargs):\n",
    "#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:\n",
    "#             batch_size = batch.obs.shape[0]\n",
    "#         else:\n",
    "#             batch_size = 1\n",
    "            \n",
    "#         actions = []\n",
    "#         for _ in range(batch_size):\n",
    "#             action = self.action_space.sample()\n",
    "#             actions.append(action)\n",
    "        \n",
    "#         return Batch(act=np.array(actions))\n",
    "    \n",
    "#     def exploration_noise(self, act, batch):\n",
    "#         return act\n",
    "    \n",
    "#     def learn(self, batch, **kwargs):\n",
    "#         return {}\n",
    "\n",
    "# class AggressiveRandomPolicy(ContinuousRandomPolicy):\n",
    "#     \"\"\"激进随机策略 - 偏向更大的动作\"\"\"\n",
    "#     def __init__(self, action_space):\n",
    "#         super().__init__(action_space, \"Aggressive\")\n",
    "    \n",
    "#     def forward(self, batch, state=None, **kwargs):\n",
    "#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:\n",
    "#             batch_size = batch.obs.shape[0]\n",
    "#         else:\n",
    "#             batch_size = 1\n",
    "            \n",
    "#         actions = []\n",
    "#         for _ in range(batch_size):\n",
    "#             # 生成偏向边界的动作\n",
    "#             action = np.random.uniform(-1.0, 1.0, size=self.action_space.shape)\n",
    "#             # 增强动作幅度\n",
    "#             action = np.sign(action) * np.abs(action) * 0.8  # 偏向较大动作\n",
    "#             # 确保在范围内\n",
    "#             action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "#             actions.append(action)\n",
    "        \n",
    "#         return Batch(act=np.array(actions))\n",
    "\n",
    "# class ConservativeRandomPolicy(ContinuousRandomPolicy):\n",
    "#     \"\"\"保守随机策略 - 偏向较小的动作\"\"\"\n",
    "#     def __init__(self, action_space):\n",
    "#         super().__init__(action_space, \"Conservative\")\n",
    "    \n",
    "#     def forward(self, batch, state=None, **kwargs):\n",
    "#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:\n",
    "#             batch_size = batch.obs.shape[0]\n",
    "#         else:\n",
    "#             batch_size = 1\n",
    "            \n",
    "#         actions = []\n",
    "#         for _ in range(batch_size):\n",
    "#             # 生成偏向中心的动作\n",
    "#             action = np.random.uniform(-0.3, 0.3, size=self.action_space.shape)  # 较小范围\n",
    "#             # 确保在范围内\n",
    "#             action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "#             actions.append(action)\n",
    "        \n",
    "#         return Batch(act=np.array(actions))\n",
    "\n",
    "# class BiasedRandomPolicy(ContinuousRandomPolicy):\n",
    "#     \"\"\"偏向随机策略 - 偏向某个方向\"\"\"\n",
    "#     def __init__(self, action_space, bias_direction=None):\n",
    "#         super().__init__(action_space, \"Biased\")\n",
    "#         # 如果没有指定偏向，随机选择一个\n",
    "#         self.bias = bias_direction if bias_direction is not None else np.random.uniform(-0.5, 0.5, size=action_space.shape)\n",
    "    \n",
    "#     def forward(self, batch, state=None, **kwargs):\n",
    "#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:\n",
    "#             batch_size = batch.obs.shape[0]\n",
    "#         else:\n",
    "#             batch_size = 1\n",
    "            \n",
    "#         actions = []\n",
    "#         for _ in range(batch_size):\n",
    "#             # 基础随机动作\n",
    "#             base_action = np.random.uniform(-0.5, 0.5, size=self.action_space.shape)\n",
    "#             # 添加偏向\n",
    "#             action = base_action + self.bias\n",
    "#             # 确保在范围内\n",
    "#             action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "#             actions.append(action)\n",
    "        \n",
    "#         return Batch(act=np.array(actions))\n",
    "\n",
    "# def get_args():\n",
    "#     \"\"\"配置参数\"\"\"\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--seed', type=int, default=42)\n",
    "#     parser.add_argument('--n-episodes', type=int, default=10)\n",
    "#     parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     parser.add_argument('--render', action='store_true', default=False)\n",
    "#     parser.add_argument('--n-pursuers', type=int, default=5)\n",
    "#     parser.add_argument('--env-num', type=int, default=1)\n",
    "    \n",
    "#     return parser.parse_known_args()[0]\n",
    "\n",
    "# def get_env(args):\n",
    "#     \"\"\"创建Waterworld环境\"\"\"\n",
    "#     env = waterworld_v4.env(\n",
    "#         n_pursuers=args.n_pursuers,\n",
    "#         n_evaders=5,\n",
    "#         n_poisons=10,\n",
    "#         n_coop=2,\n",
    "#         n_sensors=30,\n",
    "#         sensor_range=0.2,\n",
    "#         radius=0.015,\n",
    "#         pursuer_max_accel=0.01,\n",
    "#         evader_speed=0.01,\n",
    "#         poison_speed=0.01,\n",
    "#         poison_reward=-1.0,\n",
    "#         food_reward=10.0,\n",
    "#         encounter_reward=0.01,\n",
    "#         thrust_penalty=-0.5,\n",
    "#         local_ratio=1.0,\n",
    "#         speed_features=True,\n",
    "#         max_cycles=500,\n",
    "#         render_mode=\"human\" if args.render else None\n",
    "#     )\n",
    "#     return PettingZooEnv(env)\n",
    "\n",
    "# def create_diverse_policies(args, env):\n",
    "#     \"\"\"为每个agent创建不同类型的策略\"\"\"\n",
    "#     agents = env.agents\n",
    "#     policies = []\n",
    "    \n",
    "#     action_space = env.action_space\n",
    "    \n",
    "#     print(f\"环境信息:\")\n",
    "#     print(f\"  智能体: {agents}\")\n",
    "#     print(f\"  观察空间: {env.observation_space}\")\n",
    "#     print(f\"  动作空间: {action_space}\")\n",
    "    \n",
    "#     # 为每个agent分配不同的策略类型\n",
    "#     policy_types = [\n",
    "#         (\"Standard Random\", ContinuousRandomPolicy),\n",
    "#         (\"Aggressive\", AggressiveRandomPolicy), \n",
    "#         (\"Conservative\", ConservativeRandomPolicy),\n",
    "#         (\"Biased\", BiasedRandomPolicy),\n",
    "#         (\"Standard Random 2\", ContinuousRandomPolicy)\n",
    "#     ]\n",
    "    \n",
    "#     for i, agent_id in enumerate(agents):\n",
    "#         policy_name, policy_class = policy_types[i % len(policy_types)]\n",
    "        \n",
    "#         if policy_class == BiasedRandomPolicy:\n",
    "#             # 为偏向策略设置随机偏向方向\n",
    "#             bias = np.random.uniform(-0.3, 0.3, size=action_space.shape)\n",
    "#             policy = policy_class(action_space, bias_direction=bias)\n",
    "#         else:\n",
    "#             policy = policy_class(action_space)\n",
    "        \n",
    "#         policies.append(policy)\n",
    "#         print(f\"  Agent {agent_id}: {policy_name}\")\n",
    "    \n",
    "#     return policies\n",
    "\n",
    "# def run_multi_agent_test(args):\n",
    "#     \"\"\"运行多智能体测试\"\"\"\n",
    "#     print(\"=== 多智能体独立策略测试 ===\")\n",
    "    \n",
    "#     # 创建环境\n",
    "#     env = get_env(args)\n",
    "    \n",
    "#     # 创建不同的策略\n",
    "#     policies = create_diverse_policies(args, env)\n",
    "    \n",
    "#     # 创建策略管理器\n",
    "#     policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "#     print(f\"\\n策略管理器创建成功，管理 {len(policies)} 个独立策略\")\n",
    "    \n",
    "#     # 创建向量化环境\n",
    "#     vec_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.env_num)])\n",
    "    \n",
    "#     # 创建收集器\n",
    "#     collector = Collector(policy_manager, vec_envs)\n",
    "#     print(\"收集器创建成功\")\n",
    "    \n",
    "#     # 设置随机种子\n",
    "#     np.random.seed(args.seed)\n",
    "#     vec_envs.seed(args.seed)\n",
    "    \n",
    "#     # 运行测试\n",
    "#     print(f\"\\n开始运行 {args.n_episodes} 个episode...\")\n",
    "#     result = collector.collect(n_episode=args.n_episodes, render=args.render)\n",
    "    \n",
    "#     # 分析结果\n",
    "#     print(f\"\\n=== 测试结果 ===\")\n",
    "#     print(f\"总episode数: {result['n/ep']}\")\n",
    "#     print(f\"总步数: {result['n/st']}\")\n",
    "#     print(f\"平均episode长度: {result['len']:.2f}\")\n",
    "#     print(f\"平均奖励: {result['rew']:.4f}\")\n",
    "#     print(f\"奖励标准差: {result['rew_std']:.4f}\")\n",
    "    \n",
    "#     # 详细奖励分析\n",
    "#     if 'rews' in result:\n",
    "#         rewards = result['rews']\n",
    "#         print(f\"\\n=== 详细奖励分析 ===\")\n",
    "#         print(f\"奖励形状: {rewards.shape}\")\n",
    "#         print(f\"最大奖励: {np.max(rewards):.4f}\")\n",
    "#         print(f\"最小奖励: {np.min(rewards):.4f}\")\n",
    "        \n",
    "#         # 如果是多智能体，分析每个agent的表现\n",
    "#         if len(rewards.shape) > 1 and rewards.shape[1] > 1:\n",
    "#             print(f\"\\n各智能体平均奖励:\")\n",
    "#             for i in range(rewards.shape[1]):\n",
    "#                 agent_reward = np.mean(rewards[:, i])\n",
    "#                 print(f\"  Agent {i}: {agent_reward:.4f}\")\n",
    "    \n",
    "#     return result\n",
    "\n",
    "# def compare_policy_performance(args):\n",
    "#     \"\"\"比较不同策略的性能\"\"\"\n",
    "#     print(\"\\n=== 策略性能比较 ===\")\n",
    "    \n",
    "#     env = get_env(args)\n",
    "#     action_space = env.action_space\n",
    "    \n",
    "#     # 测试不同策略类型\n",
    "#     policy_configs = [\n",
    "#         (\"Standard Random\", ContinuousRandomPolicy, {}),\n",
    "#         (\"Aggressive\", AggressiveRandomPolicy, {}),\n",
    "#         (\"Conservative\", ConservativeRandomPolicy, {}),\n",
    "#         (\"Biased Forward\", BiasedRandomPolicy, {\"bias_direction\": np.array([0.3, 0.0])}),\n",
    "#         (\"Biased Backward\", BiasedRandomPolicy, {\"bias_direction\": np.array([-0.3, 0.0])})\n",
    "#     ]\n",
    "    \n",
    "#     results = {}\n",
    "    \n",
    "#     for policy_name, policy_class, kwargs in policy_configs:\n",
    "#         print(f\"\\n测试策略: {policy_name}\")\n",
    "        \n",
    "#         # 创建该策略的所有agents\n",
    "#         policies = [policy_class(action_space, **kwargs) for _ in env.agents]\n",
    "#         policy_manager = MultiAgentPolicyManager(policies, env)\n",
    "        \n",
    "#         # 创建环境和收集器\n",
    "#         vec_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])\n",
    "#         collector = Collector(policy_manager, vec_envs)\n",
    "        \n",
    "#         # 运行测试\n",
    "#         result = collector.collect(n_episode=5, render=False)\n",
    "#         results[policy_name] = result['rew']\n",
    "        \n",
    "#         print(f\"  平均奖励: {result['rew']:.4f}\")\n",
    "#         print(f\"  平均长度: {result['len']:.2f}\")\n",
    "    \n",
    "#     # 总结比较\n",
    "#     print(f\"\\n=== 策略性能排序 ===\")\n",
    "#     sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "#     for i, (policy_name, reward) in enumerate(sorted_results, 1):\n",
    "#         print(f\"{i}. {policy_name}: {reward:.4f}\")\n",
    "\n",
    "# def single_agent_test(args):\n",
    "#     \"\"\"单智能体测试验证基础功能\"\"\"\n",
    "#     print(\"=== 单智能体基础测试 ===\")\n",
    "    \n",
    "#     # 创建单智能体环境用于测试\n",
    "#     args_single = argparse.Namespace(**vars(args))\n",
    "#     args_single.n_pursuers = 1\n",
    "    \n",
    "#     env = get_env(args_single)\n",
    "#     print(f\"单智能体环境创建成功\")\n",
    "#     print(f\"  智能体: {env.agents}\")\n",
    "    \n",
    "#     # 创建单个策略\n",
    "#     policy = ContinuousRandomPolicy(env.action_space)\n",
    "#     policy_manager = MultiAgentPolicyManager([policy], env)\n",
    "    \n",
    "#     # 测试\n",
    "#     vec_envs = DummyVectorEnv([lambda: get_env(args_single) for _ in range(1)])\n",
    "#     collector = Collector(policy_manager, vec_envs)\n",
    "    \n",
    "#     result = collector.collect(n_episode=3, render=args.render)\n",
    "#     print(f\"单智能体测试成功！\")\n",
    "#     print(f\"  平均奖励: {result['rew']:.4f}\")\n",
    "#     print(f\"  平均长度: {result['len']:.2f}\")\n",
    "    \n",
    "#     return True\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = get_args()\n",
    "    \n",
    "#     print(\"=== Tianshou Waterworld 稳定版本 ===\")\n",
    "#     print(f\"设备: {args.device}\")\n",
    "#     print(f\"智能体数量: {args.n_pursuers}\")\n",
    "#     print(f\"测试episode数: {args.n_episodes}\")\n",
    "    \n",
    "#     try:\n",
    "#         # 1. 单智能体基础测试\n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"步骤1: 单智能体基础测试\")\n",
    "#         if not single_agent_test(args):\n",
    "#             print(\"单智能体测试失败\")\n",
    "#             exit(1)\n",
    "#         print(\"✅ 单智能体测试通过\")\n",
    "        \n",
    "#         # 2. 多智能体测试\n",
    "#         print(\"\\n\" + \"=\"*50)\n",
    "#         print(\"步骤2: 多智能体独立策略测试\")\n",
    "#         result = run_multi_agent_test(args)\n",
    "#         print(\"✅ 多智能体测试通过\")\n",
    "        \n",
    "#         # 3. 策略比较\n",
    "#         if not args.render:  # 只在非渲染模式下进行比较测试\n",
    "#             print(\"\\n\" + \"=\"*50)\n",
    "#             print(\"步骤3: 不同策略性能比较\")\n",
    "#             compare_policy_performance(args)\n",
    "#             print(\"✅ 策略比较完成\")\n",
    "        \n",
    "#         print(f\"\\n\" + \"=\"*60)\n",
    "#         print(f\"🎉 所有测试完成！\")\n",
    "#         print(f\"\")\n",
    "#         print(f\"📋 测试总结:\")\n",
    "#         print(f\"   ✅ 环境创建和包装: PettingZoo → Tianshou\")\n",
    "#         print(f\"   ✅ 多策略管理: MultiAgentPolicyManager\")\n",
    "#         print(f\"   ✅ 独立策略: 每个agent使用不同策略\")\n",
    "#         print(f\"   ✅ 数据收集: Collector正常工作\")\n",
    "#         print(f\"   ✅ 连续动作空间: 正确处理\")\n",
    "#         print(f\"\")\n",
    "#         print(f\"🚀 下一步建议:\")\n",
    "#         print(f\"   1. 这个框架已验证可以支持每个agent的独立策略\")\n",
    "#         print(f\"   2. 可以逐个将随机策略替换为可训练算法\")\n",
    "#         print(f\"   3. 建议顺序: 先试PPO(连续动作), 再试DQN(需要动作离散化)\")\n",
    "#         print(f\"   4. 或者使用SAC等直接支持连续动作的算法\")\n",
    "#         print(f\"\")\n",
    "#         print(f\"💡 核心价值:\")\n",
    "#         print(f\"   - 展示了Tianshou中真正的'每个agent独立算法'实现\")\n",
    "#         print(f\"   - 为多智能体强化学习研究提供了稳定的基础框架\")\n",
    "#         print(f\"   - 可以轻松扩展到任何PettingZoo环境\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n❌ 测试过程中出现错误: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         print(f\"\\n💡 调试建议:\")\n",
    "#         print(f\"   1. 检查依赖版本: pip list | grep -E '(tianshou|pettingzoo)'\")\n",
    "#         print(f\"   2. 尝试简化参数: --n-episodes 3 --n-pursuers 3\")\n",
    "#         print(f\"   3. 如有问题可以逐步调试每个组件\")\n",
    "\n",
    "# \"\"\"\n",
    "# 使用说明:\n",
    "\n",
    "# 1. 基础测试:\n",
    "#    python waterworld_stable.py\n",
    "\n",
    "# 2. 可视化测试:\n",
    "#    python waterworld_stable.py --render\n",
    "\n",
    "# 3. 更多episode:\n",
    "#    python waterworld_stable.py --n-episodes 20\n",
    "\n",
    "# 4. 不同智能体数量:\n",
    "#    python waterworld_stable.py --n-pursuers 3\n",
    "\n",
    "# 这个版本的特点:\n",
    "# - 完全避免了DQN的兼容性问题\n",
    "# - 展示了真正的\"每个agent独立策略\"概念\n",
    "# - 4种不同类型的策略模拟不同算法\n",
    "# - 稳定的错误处理和测试流程\n",
    "# - 性能比较和分析功能\n",
    "\n",
    "# 这为后续添加真正的可训练算法（DQN、PPO、SAC等）奠定了坚实基础。\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adf0a7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tianshou.data import Collector\n",
    "# from tianshou.env import DummyVectorEnv, PettingZooEnv\n",
    "# from tianshou.policy import MultiAgentPolicyManager, RandomPolicy\n",
    "# from pettingzoo.classic import leduc_holdem_v4\n",
    "# from pettingzoo.classic import rps_v2\n",
    "# from pettingzoo.sisl import pursuit_v4\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Step 1: Load the PettingZoo environment\n",
    "#     env = pursuit_v4.env(n_pursuers=2,render_mode=\"human\")\n",
    "\n",
    "#     # Step 2: Wrap the environment for Tianshou interfacing\n",
    "#     env = PettingZooEnv(env)\n",
    "\n",
    "#     # Step 3: Define policies for each agent\n",
    "#     policies = MultiAgentPolicyManager([RandomPolicy(), RandomPolicy()], env)\n",
    "\n",
    "#     # Step 4: Convert the env to vector format\n",
    "#     env = DummyVectorEnv([lambda: env])\n",
    "\n",
    "#     # Step 5: Construct the Collector, which interfaces the policies with the vectorised environment\n",
    "#     collector = Collector(policies, env)\n",
    "\n",
    "#     # Step 6: Execute the environment with the agents playing for 1 episode, and render a frame every 0.1 seconds\n",
    "#     result = collector.collect(n_episode=1, render=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d618080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10d97d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82329132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 如果你装的是 gymnasium，就用下面这行；否则改为 from gym import spaces\n",
    "# from gymnasium import spaces  \n",
    "# import numpy as np\n",
    "# from pettingzoo.utils.wrappers.base import BaseWrapper\n",
    "\n",
    "# class DiscreteActionWrapper(BaseWrapper):\n",
    "#     \"\"\"\n",
    "#     将连续的 Box 动作空间离散化成 Discrete(n_bins^dim)。\n",
    "#     \"\"\"\n",
    "#     def __init__(self, env, n_bins):\n",
    "#         super().__init__(env)\n",
    "#         self.n_bins = n_bins\n",
    "\n",
    "#         # —— 解包，只解到 raw_env（有 possible_agents）为止 —— \n",
    "#         unwrapped = env\n",
    "#         # 只要下一层有 env 且当前层没有 possible_agents，就往下一层\n",
    "#         while hasattr(unwrapped, 'env') and not hasattr(unwrapped, 'possible_agents'):\n",
    "#             unwrapped = unwrapped.env\n",
    "#         raw_env = unwrapped  # 这时 raw_env 就是那个继承自 AECEnv 的实例\n",
    "\n",
    "#         # 取第一个 agent，读它的 Box 下界和上界\n",
    "#         sample_agent = raw_env.possible_agents[0]\n",
    "#         box: spaces.Box = raw_env.action_space(sample_agent)\n",
    "#         low, high = box.low, box.high\n",
    "#         dims = low.shape[0]\n",
    "\n",
    "#         # 为每个维度做等间隔分箱\n",
    "#         grids = [np.linspace(low[i], high[i], n_bins) for i in range(dims)]\n",
    "#         mesh  = np.meshgrid(*grids)\n",
    "#         # 将网格展平成 (n_bins^dims, dims)  \n",
    "#         self._actions = np.stack([m.flatten() for m in mesh], axis=1)\n",
    "\n",
    "#         # 把所有 agent 的 action_space 都换成 Discrete\n",
    "#         discrete_space = spaces.Discrete(self._actions.shape[0])\n",
    "#         for agent in raw_env.possible_agents:\n",
    "#             self.action_spaces[agent] = discrete_space\n",
    "\n",
    "#     def step(self, action):\n",
    "#         # 离散编号 → 连续向量 → 传给底层 env\n",
    "#         cont = self._actions[action]\n",
    "#         return self.env.step(cont)\n",
    "\n",
    "#     def reset(self, seed=None, options=None):\n",
    "#         return self.env.reset(seed=seed, options=options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d56e33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Multi-Agent Framework for PettingZoo Waterworld - 修复版本\n",
    "# 支持每个智能体使用不同算法的框架\n",
    "\n",
    "# 修复了常见的错误和兼容性问题\n",
    "# \"\"\"\n",
    "\n",
    "# import numpy as np\n",
    "# import gymnasium\n",
    "# from typing import Dict, Any, Optional, Union, List\n",
    "# from abc import ABC, abstractmethod\n",
    "# import time\n",
    "# import json\n",
    "# import os\n",
    "# from dataclasses import dataclass, asdict\n",
    "# from pettingzoo.sisl import waterworld_v4\n",
    "# import supersuit as ss\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class AgentConfig:\n",
    "#     \"\"\"智能体配置类\"\"\"\n",
    "#     agent_id: str\n",
    "#     algorithm: str\n",
    "#     hyperparameters: Dict[str, Any]\n",
    "#     device: str = \"cpu\"\n",
    "    \n",
    "#     def to_dict(self):\n",
    "#         return asdict(self)\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class EnvironmentConfig:\n",
    "#     \"\"\"环境配置类\"\"\"\n",
    "#     n_pursuers: int = 3\n",
    "#     n_evaders: int = 3\n",
    "#     n_poisons: int = 10\n",
    "#     n_coop: int = 2\n",
    "#     n_sensors: int = 20\n",
    "#     sensor_range: float = 0.2\n",
    "#     radius: float = 0.015\n",
    "#     max_cycles: int = 500\n",
    "#     render_mode: Optional[str] = None\n",
    "    \n",
    "#     def to_dict(self):\n",
    "#         # 过滤掉None值，避免传递给环境时出错\n",
    "#         config_dict = asdict(self)\n",
    "#         return {k: v for k, v in config_dict.items() if v is not None}\n",
    "\n",
    "\n",
    "# class BaseAgent(ABC):\n",
    "#     \"\"\"智能体基类 - 所有算法都需要继承这个类\"\"\"\n",
    "    \n",
    "#     def __init__(self, agent_id: str, observation_space: gymnasium.Space, \n",
    "#                  action_space: gymnasium.Space, config: Dict[str, Any]):\n",
    "#         self.agent_id = agent_id\n",
    "#         self.observation_space = observation_space\n",
    "#         self.action_space = action_space\n",
    "#         self.config = config\n",
    "#         self.step_count = 0\n",
    "#         self.total_reward = 0.0\n",
    "        \n",
    "#     @abstractmethod\n",
    "#     def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "#         \"\"\"选择动作 - 子类必须实现\"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     @abstractmethod\n",
    "#     def update(self, experience: Dict[str, Any]) -> Dict[str, float]:\n",
    "#         \"\"\"更新策略 - 子类必须实现\"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     @abstractmethod\n",
    "#     def save_model(self, filepath: str) -> None:\n",
    "#         \"\"\"保存模型 - 子类必须实现\"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     @abstractmethod\n",
    "#     def load_model(self, filepath: str) -> None:\n",
    "#         \"\"\"加载模型 - 子类必须实现\"\"\"\n",
    "#         pass\n",
    "    \n",
    "#     def reset_episode_stats(self):\n",
    "#         \"\"\"重置回合统计\"\"\"\n",
    "#         self.step_count = 0\n",
    "#         self.total_reward = 0.0\n",
    "    \n",
    "#     def get_stats(self) -> Dict[str, float]:\n",
    "#         \"\"\"获取统计信息\"\"\"\n",
    "#         return {\n",
    "#             \"steps\": self.step_count,\n",
    "#             \"total_reward\": self.total_reward,\n",
    "#             \"avg_reward\": self.total_reward / max(1, self.step_count)\n",
    "#         }\n",
    "\n",
    "\n",
    "# class RandomAgent(BaseAgent):\n",
    "#     \"\"\"随机智能体 - 完全随机选择动作\"\"\"\n",
    "    \n",
    "#     def __init__(self, agent_id: str, observation_space: gymnasium.Space, \n",
    "#                  action_space: gymnasium.Space, config: Dict[str, Any]):\n",
    "#         super().__init__(agent_id, observation_space, action_space, config)\n",
    "#         self.algorithm_name = \"Random\"\n",
    "        \n",
    "#         # 创建独立的随机数生成器，避免全局影响\n",
    "#         self.random_seed = config.get(\"random_seed\", None)\n",
    "#         self.rng = np.random.RandomState(self.random_seed)\n",
    "    \n",
    "#     def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "#         \"\"\"随机选择动作\"\"\"\n",
    "#         # 手动生成随机动作，确保类型正确\n",
    "#         if isinstance(self.action_space, gymnasium.spaces.Box):\n",
    "#             action = self.rng.uniform(\n",
    "#                 low=self.action_space.low,\n",
    "#                 high=self.action_space.high,\n",
    "#                 size=self.action_space.shape\n",
    "#             ).astype(np.float32)\n",
    "#         else:\n",
    "#             action = self.action_space.sample()\n",
    "#         return action\n",
    "    \n",
    "#     def update(self, experience: Dict[str, Any]) -> Dict[str, float]:\n",
    "#         \"\"\"随机智能体不需要更新\"\"\"\n",
    "#         return {\"loss\": 0.0}\n",
    "    \n",
    "#     def save_model(self, filepath: str) -> None:\n",
    "#         \"\"\"随机智能体没有模型需要保存\"\"\"\n",
    "#         config_data = {\n",
    "#             \"agent_id\": self.agent_id,\n",
    "#             \"algorithm\": self.algorithm_name,\n",
    "#             \"config\": self.config\n",
    "#         }\n",
    "#         os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "#         with open(filepath, 'w') as f:\n",
    "#             json.dump(config_data, f, indent=2)\n",
    "    \n",
    "#     def load_model(self, filepath: str) -> None:\n",
    "#         \"\"\"随机智能体没有模型需要加载\"\"\"\n",
    "#         pass\n",
    "\n",
    "\n",
    "# class ConstrainedRandomAgent(BaseAgent):\n",
    "#     \"\"\"受约束的随机智能体 - 在一定范围内随机选择动作\"\"\"\n",
    "    \n",
    "#     def __init__(self, agent_id: str, observation_space: gymnasium.Space, \n",
    "#                  action_space: gymnasium.Space, config: Dict[str, Any]):\n",
    "#         super().__init__(agent_id, observation_space, action_space, config)\n",
    "#         self.algorithm_name = \"ConstrainedRandom\"\n",
    "        \n",
    "#         # 约束参数\n",
    "#         self.action_scale = config.get(\"action_scale\", 1.0)\n",
    "#         bias_config = config.get(\"bias\", [0.0] * action_space.shape[0])\n",
    "#         self.bias = np.array(bias_config, dtype=np.float32)\n",
    "        \n",
    "#         # 创建独立的随机数生成器\n",
    "#         self.random_seed = config.get(\"random_seed\", None)\n",
    "#         self.rng = np.random.RandomState(self.random_seed)\n",
    "    \n",
    "#     def select_action(self, observation: np.ndarray, training: bool = True) -> np.ndarray:\n",
    "#         \"\"\"在约束范围内随机选择动作\"\"\"\n",
    "#         # 生成基础随机动作\n",
    "#         if isinstance(self.action_space, gymnasium.spaces.Box):\n",
    "#             raw_action = self.rng.uniform(\n",
    "#                 low=self.action_space.low,\n",
    "#                 high=self.action_space.high,\n",
    "#                 size=self.action_space.shape\n",
    "#             ).astype(np.float32)\n",
    "#         else:\n",
    "#             raw_action = self.action_space.sample()\n",
    "        \n",
    "#         # 应用缩放和偏置\n",
    "#         action = raw_action * self.action_scale + self.bias\n",
    "        \n",
    "#         # 确保动作在有效范围内\n",
    "#         if isinstance(self.action_space, gymnasium.spaces.Box):\n",
    "#             action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        \n",
    "#         return action\n",
    "    \n",
    "#     def update(self, experience: Dict[str, Any]) -> Dict[str, float]:\n",
    "#         \"\"\"约束随机智能体不需要更新\"\"\"\n",
    "#         return {\"loss\": 0.0}\n",
    "    \n",
    "#     def save_model(self, filepath: str) -> None:\n",
    "#         \"\"\"保存配置信息\"\"\"\n",
    "#         config_data = {\n",
    "#             \"agent_id\": self.agent_id,\n",
    "#             \"algorithm\": self.algorithm_name,\n",
    "#             \"config\": self.config,\n",
    "#             \"action_scale\": self.action_scale,\n",
    "#             \"bias\": self.bias.tolist()\n",
    "#         }\n",
    "#         os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "#         with open(filepath, 'w') as f:\n",
    "#             json.dump(config_data, f, indent=2)\n",
    "    \n",
    "#     def load_model(self, filepath: str) -> None:\n",
    "#         \"\"\"加载配置信息\"\"\"\n",
    "#         with open(filepath, 'r') as f:\n",
    "#             data = json.load(f)\n",
    "#             self.action_scale = data.get(\"action_scale\", 1.0)\n",
    "#             self.bias = np.array(data.get(\"bias\", [0.0] * self.action_space.shape[0]), dtype=np.float32)\n",
    "\n",
    "\n",
    "# class AgentFactory:\n",
    "#     \"\"\"智能体工厂类 - 负责创建不同类型的智能体\"\"\"\n",
    "    \n",
    "#     _agent_classes = {\n",
    "#         \"random\": RandomAgent,\n",
    "#         \"constrained_random\": ConstrainedRandomAgent,\n",
    "#     }\n",
    "    \n",
    "#     @classmethod\n",
    "#     def create_agent(cls, agent_id: str, algorithm: str, \n",
    "#                     observation_space: gymnasium.Space, action_space: gymnasium.Space,\n",
    "#                     config: Dict[str, Any]) -> BaseAgent:\n",
    "#         \"\"\"创建智能体实例\"\"\"\n",
    "#         if algorithm.lower() not in cls._agent_classes:\n",
    "#             raise ValueError(f\"Unknown algorithm: {algorithm}. Available: {list(cls._agent_classes.keys())}\")\n",
    "        \n",
    "#         agent_class = cls._agent_classes[algorithm.lower()]\n",
    "#         return agent_class(agent_id, observation_space, action_space, config)\n",
    "    \n",
    "#     @classmethod\n",
    "#     def register_agent(cls, algorithm_name: str, agent_class: type):\n",
    "#         \"\"\"注册新的智能体类型\"\"\"\n",
    "#         cls._agent_classes[algorithm_name.lower()] = agent_class\n",
    "    \n",
    "#     @classmethod\n",
    "#     def get_available_algorithms(cls) -> List[str]:\n",
    "#         \"\"\"获取所有可用算法\"\"\"\n",
    "#         return list(cls._agent_classes.keys())\n",
    "\n",
    "\n",
    "# class MultiAgentEnvironment:\n",
    "#     \"\"\"多智能体环境管理器\"\"\"\n",
    "    \n",
    "#     def __init__(self, env_config: EnvironmentConfig):\n",
    "#         self.env_config = env_config\n",
    "#         self.env = None\n",
    "#         self.agents = {}\n",
    "#         self.agent_configs = {}\n",
    "#         self.episode_stats = {}\n",
    "        \n",
    "#     def create_environment(self):\n",
    "#         \"\"\"创建环境\"\"\"\n",
    "#         try:\n",
    "#             # 创建原始环境\n",
    "#             env_params = self.env_config.to_dict()\n",
    "#             print(f\"创建环境参数: {env_params}\")\n",
    "            \n",
    "#             self.env = waterworld_v4.env(**env_params)\n",
    "            \n",
    "#             # 应用预处理\n",
    "#             self.env = ss.flatten_v0(self.env)  # 扁平化观察空间\n",
    "            \n",
    "#             # 重置环境以初始化智能体\n",
    "#             self.env.reset()\n",
    "            \n",
    "#             print(f\"环境创建成功，智能体: {self.env.possible_agents}\")\n",
    "            \n",
    "#             # 验证环境\n",
    "#             if not self.env.possible_agents:\n",
    "#                 raise RuntimeError(\"环境中没有智能体\")\n",
    "                \n",
    "#             return self.env\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"创建环境失败: {str(e)}\")\n",
    "#             raise\n",
    "    \n",
    "#     def add_agent(self, agent_config: AgentConfig):\n",
    "#         \"\"\"添加智能体\"\"\"\n",
    "#         if self.env is None:\n",
    "#             raise RuntimeError(\"Environment not created. Call create_environment() first.\")\n",
    "        \n",
    "#         agent_id = agent_config.agent_id\n",
    "        \n",
    "#         # 检查智能体是否存在于环境中\n",
    "#         if agent_id not in self.env.possible_agents:\n",
    "#             raise ValueError(f\"Agent {agent_id} not found in environment. Available: {self.env.possible_agents}\")\n",
    "        \n",
    "#         try:\n",
    "#             # 获取智能体的观察空间和动作空间\n",
    "#             obs_space = self.env.observation_space(agent_id)\n",
    "#             action_space = self.env.action_space(agent_id)\n",
    "            \n",
    "#             print(f\"智能体 {agent_id}: 观察空间={obs_space}, 动作空间={action_space}\")\n",
    "            \n",
    "#             # 创建智能体实例\n",
    "#             agent = AgentFactory.create_agent(\n",
    "#                 agent_id=agent_id,\n",
    "#                 algorithm=agent_config.algorithm,\n",
    "#                 observation_space=obs_space,\n",
    "#                 action_space=action_space,\n",
    "#                 config=agent_config.hyperparameters\n",
    "#             )\n",
    "            \n",
    "#             self.agents[agent_id] = agent\n",
    "#             self.agent_configs[agent_id] = agent_config\n",
    "            \n",
    "#             print(f\"智能体 {agent_id} 添加成功，算法: {agent_config.algorithm}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"添加智能体 {agent_id} 失败: {str(e)}\")\n",
    "#             raise\n",
    "    \n",
    "#     def run_episode(self, max_steps: Optional[int] = None, render: bool = False) -> Dict[str, Any]:\n",
    "#         \"\"\"运行一个回合\"\"\"\n",
    "#         if self.env is None or not self.agents:\n",
    "#             raise RuntimeError(\"Environment and agents must be set up first.\")\n",
    "        \n",
    "#         try:\n",
    "#             # 重置环境和智能体统计\n",
    "#             self.env.reset()\n",
    "#             for agent in self.agents.values():\n",
    "#                 agent.reset_episode_stats()\n",
    "            \n",
    "#             episode_rewards = {agent_id: 0.0 for agent_id in self.agents.keys()}\n",
    "#             episode_steps = 0\n",
    "#             episode_start_time = time.time()\n",
    "            \n",
    "#             # 存储经验的缓冲区\n",
    "#             experiences = {agent_id: [] for agent_id in self.agents.keys()}\n",
    "            \n",
    "#             # 运行回合\n",
    "#             while self.env.agents and (max_steps is None or episode_steps < max_steps):\n",
    "#                 try:\n",
    "#                     # 获取当前智能体\n",
    "#                     current_agent_id = self.env.agent_selection\n",
    "                    \n",
    "#                     # 获取观察\n",
    "#                     observation, reward, termination, truncation, info = self.env.last()\n",
    "                    \n",
    "#                     # 更新奖励统计\n",
    "#                     if current_agent_id in episode_rewards:\n",
    "#                         episode_rewards[current_agent_id] += reward\n",
    "#                         if current_agent_id in self.agents:\n",
    "#                             self.agents[current_agent_id].total_reward += reward\n",
    "#                             self.agents[current_agent_id].step_count += 1\n",
    "                    \n",
    "#                     # 如果智能体结束，执行 None 动作\n",
    "#                     if termination or truncation:\n",
    "#                         action = None\n",
    "#                     else:\n",
    "#                         # 选择动作\n",
    "#                         if current_agent_id in self.agents:\n",
    "#                             action = self.agents[current_agent_id].select_action(observation)\n",
    "#                         else:\n",
    "#                             # 如果没有对应的智能体，随机选择动作\n",
    "#                             action = self.env.action_space(current_agent_id).sample()\n",
    "                    \n",
    "#                     # 存储经验（用于未来的学习算法）\n",
    "#                     if current_agent_id in experiences and not (termination or truncation):\n",
    "#                         experience = {\n",
    "#                             \"observation\": observation.copy() if hasattr(observation, 'copy') else observation,\n",
    "#                             \"action\": action.copy() if hasattr(action, 'copy') else action,\n",
    "#                             \"reward\": reward,\n",
    "#                             \"done\": termination or truncation,\n",
    "#                             \"info\": info\n",
    "#                         }\n",
    "#                         experiences[current_agent_id].append(experience)\n",
    "                    \n",
    "#                     # 执行动作\n",
    "#                     self.env.step(action)\n",
    "#                     episode_steps += 1\n",
    "                    \n",
    "#                     # 渲染\n",
    "#                     if render:\n",
    "#                         self.env.render()\n",
    "                        \n",
    "#                 except Exception as e:\n",
    "#                     print(f\"回合执行中出错 (步骤 {episode_steps}): {str(e)}\")\n",
    "#                     break\n",
    "            \n",
    "#             episode_duration = time.time() - episode_start_time\n",
    "            \n",
    "#             # 收集回合统计信息\n",
    "#             episode_stats = {\n",
    "#                 \"episode_steps\": episode_steps,\n",
    "#                 \"episode_duration\": episode_duration,\n",
    "#                 \"episode_rewards\": episode_rewards,\n",
    "#                 \"total_reward\": sum(episode_rewards.values()),\n",
    "#                 \"avg_reward\": sum(episode_rewards.values()) / max(1, len(episode_rewards)),\n",
    "#                 \"agent_stats\": {agent_id: agent.get_stats() for agent_id, agent in self.agents.items()}\n",
    "#             }\n",
    "            \n",
    "#             return episode_stats\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"运行回合时出错: {str(e)}\")\n",
    "#             raise\n",
    "    \n",
    "#     def run_multiple_episodes(self, num_episodes: int, max_steps_per_episode: Optional[int] = None,\n",
    "#                             render: bool = False, verbose: bool = True) -> List[Dict[str, Any]]:\n",
    "#         \"\"\"运行多个回合\"\"\"\n",
    "#         all_episode_stats = []\n",
    "        \n",
    "#         for episode in range(num_episodes):\n",
    "#             try:\n",
    "#                 if verbose:\n",
    "#                     print(f\"\\n--- 回合 {episode + 1}/{num_episodes} ---\")\n",
    "                \n",
    "#                 episode_stats = self.run_episode(max_steps_per_episode, render)\n",
    "#                 all_episode_stats.append(episode_stats)\n",
    "                \n",
    "#                 if verbose:\n",
    "#                     print(f\"回合步数: {episode_stats['episode_steps']}\")\n",
    "#                     print(f\"总奖励: {episode_stats['total_reward']:.2f}\")\n",
    "#                     print(f\"平均奖励: {episode_stats['avg_reward']:.2f}\")\n",
    "#                     print(f\"回合时长: {episode_stats['episode_duration']:.2f}s\")\n",
    "                    \n",
    "#                     # 打印每个智能体的统计信息\n",
    "#                     for agent_id, stats in episode_stats['agent_stats'].items():\n",
    "#                         print(f\"  {agent_id}: 奖励={stats['total_reward']:.2f}, 步数={stats['steps']}\")\n",
    "                        \n",
    "#             except Exception as e:\n",
    "#                 print(f\"回合 {episode + 1} 执行失败: {str(e)}\")\n",
    "#                 if verbose:\n",
    "#                     import traceback\n",
    "#                     traceback.print_exc()\n",
    "#                 continue\n",
    "        \n",
    "#         return all_episode_stats\n",
    "    \n",
    "#     def save_agents(self, save_dir: str):\n",
    "#         \"\"\"保存所有智能体模型\"\"\"\n",
    "#         try:\n",
    "#             os.makedirs(save_dir, exist_ok=True)\n",
    "            \n",
    "#             for agent_id, agent in self.agents.items():\n",
    "#                 filepath = os.path.join(save_dir, f\"{agent_id}_model.json\")\n",
    "#                 agent.save_model(filepath)\n",
    "#                 print(f\"智能体 {agent_id} 模型已保存到 {filepath}\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"保存模型时出错: {str(e)}\")\n",
    "#             raise\n",
    "    \n",
    "#     def get_environment_info(self) -> Dict[str, Any]:\n",
    "#         \"\"\"获取环境信息\"\"\"\n",
    "#         if self.env is None:\n",
    "#             return {}\n",
    "        \n",
    "#         try:\n",
    "#             # 确保环境已经重置\n",
    "#             try:\n",
    "#                 current_agents = self.env.agents\n",
    "#             except AttributeError:\n",
    "#                 # 如果还没有重置，先重置一下\n",
    "#                 self.env.reset()\n",
    "#                 current_agents = self.env.agents\n",
    "            \n",
    "#             return {\n",
    "#                 \"possible_agents\": self.env.possible_agents,\n",
    "#                 \"current_agents\": current_agents,\n",
    "#                 \"observation_spaces\": {agent: str(self.env.observation_space(agent)) \n",
    "#                                      for agent in self.env.possible_agents},\n",
    "#                 \"action_spaces\": {agent: str(self.env.action_space(agent)) \n",
    "#                                 for agent in self.env.possible_agents},\n",
    "#                 \"environment_config\": self.env_config.to_dict()\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             print(f\"获取环境信息时出错: {str(e)}\")\n",
    "#             return {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# def demo_random_agents():\n",
    "#     \"\"\"演示随机智能体框架\"\"\"\n",
    "#     print(\"=\" * 60)\n",
    "#     print(\"多智能体随机算法演示\")\n",
    "#     print(\"=\" * 60)\n",
    "    \n",
    "#     try:\n",
    "#         # 1. 创建环境配置\n",
    "#         env_config = EnvironmentConfig(\n",
    "#             n_pursuers=3,\n",
    "#             n_evaders=2,\n",
    "#             n_poisons=8,\n",
    "#             max_cycles=200,\n",
    "#             render_mode=None  # 明确设置为None\n",
    "#         )\n",
    "        \n",
    "#         # 2. 创建环境管理器\n",
    "#         print(\"正在创建环境...\")\n",
    "#         ma_env = MultiAgentEnvironment(env_config)\n",
    "#         ma_env.create_environment()\n",
    "        \n",
    "#         # 3. 打印环境信息\n",
    "#         print(\"获取环境信息...\")\n",
    "#         env_info = ma_env.get_environment_info()\n",
    "        \n",
    "#         if \"error\" in env_info:\n",
    "#             print(f\"获取环境信息失败: {env_info['error']}\")\n",
    "#             return\n",
    "            \n",
    "#         print(f\"环境智能体: {env_info['possible_agents']}\")\n",
    "#         print(f\"观察空间: {list(env_info['observation_spaces'].values())[0]}\")\n",
    "#         print(f\"动作空间: {list(env_info['action_spaces'].values())[0]}\")\n",
    "        \n",
    "#         # 4. 为每个智能体配置不同的算法\n",
    "#         agent_configs = [\n",
    "#             # 智能体0: 完全随机\n",
    "#             AgentConfig(\n",
    "#                 agent_id=\"pursuer_0\",\n",
    "#                 algorithm=\"random\",\n",
    "#                 hyperparameters={\"random_seed\": 42}\n",
    "#             ),\n",
    "#             # 智能体1: 约束随机（偏向向前移动）\n",
    "#             AgentConfig(\n",
    "#                 agent_id=\"pursuer_1\", \n",
    "#                 algorithm=\"constrained_random\",\n",
    "#                 hyperparameters={\n",
    "#                     \"action_scale\": 0.8,\n",
    "#                     \"bias\": [0.3, 0.0],  # X方向偏置\n",
    "#                     \"random_seed\": 123\n",
    "#                 }\n",
    "#             ),\n",
    "#             # 智能体2: 约束随机（较小的动作）\n",
    "#             AgentConfig(\n",
    "#                 agent_id=\"pursuer_2\",\n",
    "#                 algorithm=\"constrained_random\", \n",
    "#                 hyperparameters={\n",
    "#                     \"action_scale\": 0.5,\n",
    "#                     \"bias\": [0.0, 0.0],\n",
    "#                     \"random_seed\": 456\n",
    "#                 }\n",
    "#             )\n",
    "#         ]\n",
    "        \n",
    "#         # 5. 添加智能体\n",
    "#         print(\"添加智能体...\")\n",
    "#         for config in agent_configs:\n",
    "#             ma_env.add_agent(config)\n",
    "        \n",
    "#         # 6. 运行多个回合\n",
    "#         print(f\"\\n可用算法: {AgentFactory.get_available_algorithms()}\")\n",
    "#         print(\"开始运行回合...\")\n",
    "        \n",
    "#         stats = ma_env.run_multiple_episodes(\n",
    "#             num_episodes=3,  # 减少回合数以便调试\n",
    "#             max_steps_per_episode=50,  # 减少步数以便调试\n",
    "#             render=False,\n",
    "#             verbose=True\n",
    "#         )\n",
    "        \n",
    "#         if not stats:\n",
    "#             print(\"没有成功运行任何回合\")\n",
    "#             return\n",
    "        \n",
    "#         # 7. 统计分析\n",
    "#         print(\"\\n\" + \"=\" * 60)\n",
    "#         print(\"统计分析\")\n",
    "#         print(\"=\" * 60)\n",
    "        \n",
    "#         total_rewards = [episode['total_reward'] for episode in stats]\n",
    "#         episode_lengths = [episode['episode_steps'] for episode in stats]\n",
    "        \n",
    "#         print(f\"平均总奖励: {np.mean(total_rewards):.2f} ± {np.std(total_rewards):.2f}\")\n",
    "#         print(f\"平均回合长度: {np.mean(episode_lengths):.1f} ± {np.std(episode_lengths):.1f}\")\n",
    "        \n",
    "#         # 每个智能体的平均表现\n",
    "#         agent_rewards = {}\n",
    "#         for agent_id in ma_env.agents.keys():\n",
    "#             rewards = [episode['episode_rewards'][agent_id] for episode in stats]\n",
    "#             agent_rewards[agent_id] = {\n",
    "#                 \"mean\": np.mean(rewards),\n",
    "#                 \"std\": np.std(rewards)\n",
    "#             }\n",
    "        \n",
    "#         print(\"\\n智能体平均表现:\")\n",
    "#         for agent_id, reward_stats in agent_rewards.items():\n",
    "#             config = ma_env.agent_configs[agent_id]\n",
    "#             print(f\"  {agent_id} ({config.algorithm}): {reward_stats['mean']:.2f} ± {reward_stats['std']:.2f}\")\n",
    "        \n",
    "#         # 8. 保存模型\n",
    "#         print(\"\\n保存模型...\")\n",
    "#         ma_env.save_agents(\"./saved_models\")\n",
    "        \n",
    "#         print(\"\\n演示完成!\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"演示过程中出现错误: {str(e)}\")\n",
    "#         print(\"错误类型:\", type(e).__name__)\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "\n",
    "# def test_environment_only():\n",
    "#     \"\"\"仅测试环境创建\"\"\"\n",
    "#     print(\"测试环境创建...\")\n",
    "#     try:\n",
    "#         env = waterworld_v4.env(n_pursuers=2, n_evaders=2, n_poisons=5)\n",
    "#         print(\"原始环境创建成功\")\n",
    "        \n",
    "#         env = ss.flatten_v0(env)\n",
    "#         print(\"扁平化成功\")\n",
    "        \n",
    "#         env.reset()\n",
    "#         print(\"环境重置成功\")\n",
    "        \n",
    "#         print(f\"智能体: {env.possible_agents}\")\n",
    "#         print(f\"观察空间: {env.observation_space(env.possible_agents[0])}\")\n",
    "#         print(f\"动作空间: {env.action_space(env.possible_agents[0])}\")\n",
    "        \n",
    "#         env.close()\n",
    "#         print(\"环境测试完成\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"环境测试失败: {str(e)}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 先测试环境\n",
    "#     test_environment_only()\n",
    "#     print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    \n",
    "#     # 再运行完整演示\n",
    "#     demo_random_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f21c3314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-16 19:13:11,375\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.algorithms.sac import SACConfig\n",
    "from ray.rllib.algorithms.dqn.dqn import DQNConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "register_env(\n",
    "    \"pistonball\",\n",
    "    lambda cfg: PettingZooEnv(pistonball_v6.env(num_floors=cfg.get(\"n_pistons\", 20))),\n",
    ")\n",
    "\n",
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(\"pistonball\", env_config={\"n_pistons\": 30})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8800cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = [\n",
    "    'notebook_script.py',\n",
    "    '--enable-new-api-stack',\n",
    "    '--num-agents=10',\n",
    "    # 新增参数用于指定predator和prey的数量\n",
    "    # '--n-predators=5',\n",
    "    '--wandb-key=fdd7656f474bba144dea1887bcdab534bc7df647',\n",
    "    '--wandb-project=waterworld-v4',\n",
    "    # '--n-preys=5', \n",
    "    '--checkpoint-at-end',\n",
    "    '--stop-reward=200.0',\n",
    "    '--checkpoint-freq=1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e4e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:134: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "2025-07-16 19:13:13,188\tINFO worker.py:1917 -- Started a local Ray instance.\n",
      "2025-07-16 19:13:13,740\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:13 (running for 00:00:00.17)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "+---------------------+----------+-------+\n",
      "| Trial name          | status   | loc   |\n",
      "|---------------------+----------+-------|\n",
      "| PPO_env_7bb81_00000 | PENDING  |       |\n",
      "+---------------------+----------+-------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO pid=3205621)\u001b[0m 2025-07-16 19:13:15,922\tWARNING algorithm_config.py:5014 -- You are running PPO on the new API stack! This is the new default behavior for this algorithm. If you don't want to use the new API stack, set `config.api_stack(enable_rl_module_and_learner=False,enable_env_runner_and_connector_v2=False)`. For a detailed migration guide, see here: https://docs.ray.io/en/master/rllib/new-api-stack-migration-guide.html\n",
      "\u001b[36m(MultiAgentEnvRunner pid=3205713)\u001b[0m 2025-07-16 19:13:18,144\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\n",
      "\u001b[36m(PPO pid=3205621)\u001b[0m Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:18 (running for 00:00:05.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+\n",
      "| Trial name          | status   | loc                  |\n",
      "|---------------------+----------+----------------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |\n",
      "+---------------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: WARNING `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Currently logged in as: bqr010817 (bqr010817-kyushu-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: WARNING Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Tracking run with wandb version 0.20.1\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Run data is saved locally in /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/wandb/run-20250716_191321-7bb81_00000\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Syncing run PPO_env_7bb81_00000\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/bqr010817-kyushu-university/waterworld-v4\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: 🚀 View run at https://wandb.ai/bqr010817-kyushu-university/waterworld-v4/runs/7bb81_00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:24 (running for 00:00:10.29)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+\n",
      "| Trial name          | status   | loc                  |\n",
      "|---------------------+----------+----------------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |\n",
      "+---------------------+----------+----------------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name         </th><th>env_runner_group                               </th><th>env_runners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </th><th>fault_tolerance                                            </th><th>learners                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </th><th style=\"text-align: right;\">  num_env_steps_sampled_lifetime</th><th style=\"text-align: right;\">  num_training_step_calls_per_iteration</th><th>perf                                                                                                    </th><th>timers                                                                                                                                                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_env_7bb81_00000</td><td>{&#x27;actor_manager_num_outstanding_async_reqs&#x27;: 0}</td><td>{&#x27;module_to_env_connector&#x27;: {&#x27;connector_pipeline_timer&#x27;: np.float64(0.00023345643970684813), &#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;tensor_to_numpy&#x27;: np.float64(3.2368432839117446e-05), &#x27;get_actions&#x27;: np.float64(8.696652066219597e-05), &#x27;remove_single_ts_time_rank_from_batch&#x27;: np.float64(1.0297587171252033e-06), &#x27;normalize_and_clip_actions&#x27;: np.float64(3.6033127029874215e-05), &#x27;un_batch_to_individual_items&#x27;: np.float64(1.3399879859083246e-05), &#x27;module_to_agent_unmapping&#x27;: np.float64(2.6846428123410184e-06), &#x27;listify_data_for_vector_env&#x27;: np.float64(6.020982930068361e-06)}}}, &#x27;episode_return_min&#x27;: -338.65068797812125, &#x27;episode_return_mean&#x27;: -268.67736257146265, &#x27;num_agent_steps_sampled&#x27;: {&#x27;pursuer_0&#x27;: 2000.0, &#x27;pursuer_1&#x27;: 2004.0}, &#x27;episode_return_max&#x27;: -138.4549108796875, &#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;add_states_from_episodes_to_batch&#x27;: np.float64(4.8425281420350075e-06), &#x27;add_time_dim_to_batch_and_zero_pad&#x27;: np.float64(1.220655394718051e-05), &#x27;agent_to_module_mapping&#x27;: np.float64(5.0745438784360886e-06), &#x27;add_observations_from_episodes_to_batch&#x27;: np.float64(2.9558897949755192e-05), &#x27;numpy_to_tensor&#x27;: np.float64(6.080197636038065e-05), &#x27;batch_individual_items&#x27;: np.float64(2.3758504539728165e-05)}}, &#x27;agent_episode_returns_mean&#x27;: {&#x27;pursuer_0&#x27;: -77.78113371128053, &#x27;pursuer_1&#x27;: -190.8962288601821}, &#x27;agent_steps&#x27;: {&#x27;pursuer_0&#x27;: 500.0, &#x27;pursuer_1&#x27;: 500.0}, &#x27;num_env_steps_sampled&#x27;: 4000.0, &#x27;num_agent_steps_sampled_lifetime&#x27;: {&#x27;pursuer_0&#x27;: 24000.0, &#x27;pursuer_1&#x27;: 24048.0}, &#x27;weights_seq_no&#x27;: 11.0, &#x27;env_reset_timer&#x27;: np.float64(0.002104361541569233), &#x27;num_module_steps_sampled_lifetime&#x27;: {&#x27;p0&#x27;: 48048.0}, &#x27;episode_len_min&#x27;: 1000, &#x27;episode_len_max&#x27;: 1000, &#x27;num_env_steps_sampled_lifetime&#x27;: 48000.0, &#x27;num_episodes_lifetime&#x27;: 48.0, &#x27;rlmodule_inference_timer&#x27;: np.float64(0.00012569923140596308), &#x27;env_to_module_connector&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;batch_individual_items&#x27;: np.float64(1.268877338086363e-05), &#x27;add_states_from_episodes_to_batch&#x27;: np.float64(3.0611162144087078e-06), &#x27;add_time_dim_to_batch_and_zero_pad&#x27;: np.float64(6.136987877491482e-06), &#x27;agent_to_module_mapping&#x27;: np.float64(3.2923819831546356e-06), &#x27;add_observations_from_episodes_to_batch&#x27;: np.float64(1.6094878316016518e-05), &#x27;numpy_to_tensor&#x27;: np.float64(2.2988352207904776e-05)}}, &#x27;connector_pipeline_timer&#x27;: np.float64(0.00010643398405452722)}, &#x27;module_episode_returns_mean&#x27;: {&#x27;p0&#x27;: -190.8962288601821}, &#x27;sample&#x27;: np.float64(2.103532644606826), &#x27;connector_pipeline_timer&#x27;: np.float64(0.0003277744981460273), &#x27;env_step_timer&#x27;: np.float64(0.0003845202725630655), &#x27;env_to_module_sum_episodes_length_in&#x27;: np.float64(891.0423566894266), &#x27;num_module_steps_sampled&#x27;: {&#x27;p0&#x27;: 4004.0}, &#x27;env_to_module_sum_episodes_length_out&#x27;: np.float64(891.0423566894266), &#x27;episode_len_mean&#x27;: 1000.0, &#x27;episode_duration_sec_mean&#x27;: 1.0256265756664409, &#x27;num_episodes&#x27;: 4.0, &#x27;time_between_sampling&#x27;: np.float64(4.985625999119913), &#x27;num_env_steps_sampled_lifetime_throughput&#x27;: np.float64(1060.6393405367353)}</td><td>{&#x27;num_healthy_workers&#x27;: 2, &#x27;num_remote_worker_restarts&#x27;: 0}</td><td>{&#x27;p0&#x27;: {&#x27;module_train_batch_size_mean&#x27;: 128.0, &#x27;gradients_default_optimizer_global_norm&#x27;: np.float32(2.517807), &#x27;default_optimizer_learning_rate&#x27;: 5e-05, &#x27;diff_num_grad_updates_vs_sampler_policy&#x27;: np.float32(1.0), &#x27;curr_kl_coeff&#x27;: 0.45000001788139343, &#x27;total_loss&#x27;: np.float32(-0.016957264), &#x27;num_trainable_parameters&#x27;: 257285, &#x27;curr_entropy_coeff&#x27;: 0.0, &#x27;vf_loss_unclipped&#x27;: np.float32(86.95416), &#x27;entropy&#x27;: np.float32(2.8129635), &#x27;policy_loss&#x27;: np.float32(-0.058761947), &#x27;weights_seq_no&#x27;: 12.0, &#x27;vf_loss&#x27;: np.float32(6.6792517), &#x27;num_module_steps_trained&#x27;: 120320, &#x27;num_module_steps_trained_lifetime&#x27;: 1443840, &#x27;mean_kl_loss&#x27;: np.float32(0.018685393), &#x27;vf_explained_var&#x27;: np.float32(0.009755433), &#x27;num_module_steps_trained_lifetime_throughput&#x27;: 38597.92474866569}, &#x27;__all_modules__&#x27;: {&#x27;learner_connector&#x27;: {&#x27;timers&#x27;: {&#x27;connectors&#x27;: {&#x27;general_advantage_estimation&#x27;: 0.051352449227462967, &#x27;batch_individual_items&#x27;: 0.11410513433660634, &#x27;agent_to_module_mapping&#x27;: 0.0017694882347786896, &#x27;add_columns_from_episodes_to_train_batch&#x27;: 0.03424642155457002, &#x27;add_states_from_episodes_to_batch&#x27;: 4.687641377397757e-06, &#x27;numpy_to_tensor&#x27;: 0.0010990037379882322, &#x27;add_one_ts_to_episodes_and_truncate&#x27;: 0.003206263020082832, &#x27;add_time_dim_to_batch_and_zero_pad&#x27;: 1.802045933731058e-05, &#x27;add_observations_from_episodes_to_batch&#x27;: 6.889434623768018e-05}}, &#x27;connector_pipeline_timer&#x27;: 0.20623144203150254}, &#x27;num_non_trainable_parameters&#x27;: 0, &#x27;num_env_steps_trained_lifetime&#x27;: 45120000, &#x27;num_module_steps_trained&#x27;: 120320, &#x27;learner_connector_sum_episodes_length_in&#x27;: 4000.0, &#x27;num_env_steps_trained&#x27;: 3760000, &#x27;num_module_steps_trained_lifetime&#x27;: 1443840, &#x27;learner_connector_sum_episodes_length_out&#x27;: 4000.0, &#x27;num_trainable_parameters&#x27;: 257285, &#x27;num_env_steps_trained_lifetime_throughput&#x27;: 1206219.3472194944, &#x27;num_module_steps_trained_throughput&#x27;: 38598.3340084181, &#x27;num_module_steps_trained_lifetime_throughput&#x27;: 38598.4774958897}}</td><td style=\"text-align: right;\">                           48000</td><td style=\"text-align: right;\">                                      1</td><td>{&#x27;cpu_util_percent&#x27;: np.float64(11.262500000000001), &#x27;ram_util_percent&#x27;: np.float64(20.424999999999997)}</td><td>{&#x27;training_iteration&#x27;: 5.63689250059287, &#x27;restore_env_runners&#x27;: 1.5091509552322774e-05, &#x27;training_step&#x27;: 5.636710490702441, &#x27;env_runner_sampling_timer&#x27;: 2.124269032623385, &#x27;learner_update_timer&#x27;: 3.5102562499068246, &#x27;synch_weights&#x27;: 0.0019412662979918242, &#x27;synch_env_connectors&#x27;: 0.0011687522256529023}</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000000)\n",
      "\u001b[36m(PPO pid=3205621)\u001b[0m 2025-07-16 19:13:18,208\tWARNING deprecation.py:50 -- DeprecationWarning: `RLModule(config=[RLModuleConfig object])` has been deprecated. Use `RLModule(observation_space=.., action_space=.., inference_only=.., model_config=.., catalog_class=..)` instead. This will raise an error in the future!\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000000)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:29 (running for 00:00:15.34)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |   ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      1 |          5.66955 | 4000 |          -295.365 |    -200.105 |\n",
      "+---------------------+----------+----------------------+--------+------------------+------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000001)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000001)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:34 (running for 00:00:20.38)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |   ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      2 |          10.8351 | 8000 |          -278.823 |    -197.772 |\n",
      "+---------------------+----------+----------------------+--------+------------------+------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000002)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000002)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:39 (running for 00:00:25.39)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      3 |          16.1127 | 12000 |          -283.092 |    -200.004 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000003)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000003)... \n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:44 (running for 00:00:30.48)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      4 |          21.4922 | 16000 |          -285.134 |    -199.037 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000004)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000004)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:49 (running for 00:00:35.54)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      5 |          27.0451 | 20000 |          -280.525 |    -196.823 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000005)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000005)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:54 (running for 00:00:40.62)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      6 |          32.5361 | 24000 |          -278.309 |    -194.924 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000006)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000006)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:13:59 (running for 00:00:45.66)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      7 |          37.9485 | 28000 |          -275.172 |     -194.78 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000007)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000007)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:14:04 (running for 00:00:50.69)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      8 |          43.3284 | 32000 |           -271.47 |    -194.112 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-07-16 19:14:09 (running for 00:00:55.71)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      8 |          43.3284 | 32000 |           -271.47 |    -194.112 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000008)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000008)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:14:14 (running for 00:01:00.73)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |      9 |          48.6597 | 36000 |          -271.834 |    -194.307 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000009)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000009)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:14:19 (running for 00:01:05.78)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |     10 |          53.8968 | 40000 |          -267.065 |    -192.082 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000010)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000010)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-07-16 19:14:24 (running for 00:01:10.80)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 3.0/24 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
      "Result logdir: /tmp/ray/session_2025-07-16_19-13-11_958744_3203195/artifacts/2025-07-16_19-13-13/PPO_2025-07-16_19-13-13/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "| Trial name          | status   | loc                  |   iter |   total time (s) |    ts |   combined return |   return p0 |\n",
      "|---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------|\n",
      "| PPO_env_7bb81_00000 | RUNNING  | 192.168.0.25:3205621 |     11 |          59.1362 | 44000 |          -267.866 |    -192.811 |\n",
      "+---------------------+----------+----------------------+--------+------------------+-------+-------------------+-------------+\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(PPO(env=env; env-runners=2; learners=0; multi-agent=True) pid=3205621)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000011)\n",
      "\u001b[36m(_WandbLoggingActor pid=3205878)\u001b[0m wandb: Adding directory to artifact (/home/qrbao/ray_results/PPO_2025-07-16_19-13-13/PPO_env_7bb81_00000_0_2025-07-16_19-13-13/checkpoint_000011)... Done. 0.0s\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"Runs the PettingZoo Waterworld multi-agent env in RLlib using single policy learning.\n",
    "\n",
    "# Other than the `pettingzoo_independent_learning.py` example (in this same folder),\n",
    "# this example simply trains a single policy (shared by all agents).\n",
    "\n",
    "# See: https://pettingzoo.farama.org/environments/sisl/waterworld/\n",
    "# for more details on the environment.\n",
    "\n",
    "\n",
    "# How to run this script\n",
    "# ----------------------\n",
    "# `python [script file name].py --enable-new-api-stack --num-agents=2`\n",
    "\n",
    "# Control the number of agents and policies (RLModules) via --num-agents and\n",
    "# --num-policies.\n",
    "\n",
    "# This works with hundreds of agents and policies, but note that initializing\n",
    "# many policies might take some time.\n",
    "\n",
    "# For debugging, use the following additional command line options\n",
    "# `--no-tune --num-env-runners=0`\n",
    "# which should allow you to set breakpoints anywhere in the RLlib code and\n",
    "# have the execution stop there for inspection and debugging.\n",
    "\n",
    "# For logging to your WandB account, use:\n",
    "# `--wandb-key=[your WandB API key] --wandb-project=[some project name]\n",
    "# --wandb-run-name=[optional: WandB run name (within the defined project)]`\n",
    "\n",
    "\n",
    "# Results to expect\n",
    "# -----------------\n",
    "# The above options can reach a combined reward of roughly ~0.0 after about 500k-1M env\n",
    "# timesteps. Keep in mind, though, that in this setup, the agents do not have the\n",
    "# opportunity to benefit from or even out other agents' mistakes (and behavior in general)\n",
    "# as everyone is using the same policy. Hence, this example learns a more generic policy,\n",
    "# which might be less specialized to certain \"niche exploitation opportunities\" inside\n",
    "# the env:\n",
    "\n",
    "# +---------------------+----------+-----------------+--------+-----------------+\n",
    "# | Trial name          | status   | loc             |   iter |  total time (s) |\n",
    "# |---------------------+----------+-----------------+--------+-----------------+\n",
    "# | PPO_env_91f49_00000 | RUNNING  | 127.0.0.1:63676 |    200 |         605.176 |\n",
    "# +---------------------+----------+-----------------+--------+-----------------+\n",
    "\n",
    "# +--------+-------------------+-------------+\n",
    "# |     ts |   combined reward |   reward p0 |\n",
    "# +--------+-------------------+-------------|\n",
    "# | 800000 |          0.323752 |    0.161876 |\n",
    "# +--------+-------------------+-------------+\n",
    "# \"\"\"\n",
    "# from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "# from ray.rllib.core.rl_module.multi_rl_module import MultiRLModuleSpec\n",
    "# from ray.rllib.core.rl_module.rl_module import RLModuleSpec\n",
    "# from ray.rllib.env.wrappers.pettingzoo_env import PettingZooEnv\n",
    "# from ray.rllib.utils.test_utils import (\n",
    "#     add_rllib_example_script_args,\n",
    "#     run_rllib_example_script_experiment,\n",
    "# )\n",
    "# from ray.tune.registry import get_trainable_cls, register_env\n",
    "\n",
    "\n",
    "# parser = add_rllib_example_script_args(\n",
    "#     default_iters=200,\n",
    "#     default_timesteps=1000000,\n",
    "#     default_reward=0.0,\n",
    "# )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     assert args.num_agents > 0, \"Must set --num-agents > 0 when running this script!\"\n",
    "#     assert (\n",
    "#         args.enable_new_api_stack\n",
    "#     ), \"Must set --enable-new-api-stack when running this script!\"\n",
    "\n",
    "#     # Here, we use the \"Agent Environment Cycle\" (AEC) PettingZoo environment type.\n",
    "#     # For a \"Parallel\" environment example, see the rock paper scissors examples\n",
    "#     # in this same repository folder.\n",
    "#     register_env(\"env\", lambda _: PettingZooEnv(waterworld_v4.env()))\n",
    "\n",
    "#     base_config = (\n",
    "#         get_trainable_cls(args.algo)\n",
    "#         .get_default_config()\n",
    "#         .environment(\"env\")\n",
    "#         .multi_agent(\n",
    "#             policies={\"p0\"},\n",
    "#             # All agents map to the exact same policy.\n",
    "#             policy_mapping_fn=(lambda aid, *args, **kwargs: \"p0\"),\n",
    "#         )\n",
    "#         .training(\n",
    "#             model={\n",
    "#                 \"vf_share_layers\": True,\n",
    "#             },\n",
    "#             vf_loss_coeff=0.005,\n",
    "#         )\n",
    "#         .rl_module(\n",
    "#             rl_module_spec=MultiRLModuleSpec(\n",
    "#                 rl_module_specs={\"p0\": RLModuleSpec()},\n",
    "#             ),\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "#     run_rllib_example_script_experiment(base_config, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7df8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-16 15:43:35,478\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-07-16 15:43:35,709\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "usage: ipykernel_launcher.py [-h] [--algo ALGO] [--enable-new-api-stack]\n",
      "                             [--framework {tf,tf2,torch}] [--env ENV]\n",
      "                             [--num-env-runners NUM_ENV_RUNNERS]\n",
      "                             [--num-envs-per-env-runner NUM_ENVS_PER_ENV_RUNNER]\n",
      "                             [--num-agents NUM_AGENTS]\n",
      "                             [--evaluation-num-env-runners EVALUATION_NUM_ENV_RUNNERS]\n",
      "                             [--evaluation-interval EVALUATION_INTERVAL]\n",
      "                             [--evaluation-duration EVALUATION_DURATION]\n",
      "                             [--evaluation-duration-unit {episodes,timesteps}]\n",
      "                             [--evaluation-parallel-to-training]\n",
      "                             [--output OUTPUT]\n",
      "                             [--log-level {INFO,DEBUG,WARN,ERROR}] [--no-tune]\n",
      "                             [--num-samples NUM_SAMPLES]\n",
      "                             [--max-concurrent-trials MAX_CONCURRENT_TRIALS]\n",
      "                             [--verbose VERBOSE]\n",
      "                             [--checkpoint-freq CHECKPOINT_FREQ]\n",
      "                             [--checkpoint-at-end] [--wandb-key WANDB_KEY]\n",
      "                             [--wandb-project WANDB_PROJECT]\n",
      "                             [--wandb-run-name WANDB_RUN_NAME]\n",
      "                             [--stop-reward STOP_REWARD]\n",
      "                             [--stop-iters STOP_ITERS]\n",
      "                             [--stop-timesteps STOP_TIMESTEPS] [--as-test]\n",
      "                             [--as-release-test] [--num-learners NUM_LEARNERS]\n",
      "                             [--num-cpus-per-learner NUM_CPUS_PER_LEARNER]\n",
      "                             [--num-gpus-per-learner NUM_GPUS_PER_LEARNER]\n",
      "                             [--num-aggregator-actors-per-learner NUM_AGGREGATOR_ACTORS_PER_LEARNER]\n",
      "                             [--num-cpus NUM_CPUS] [--local-mode]\n",
      "                             [--num-gpus NUM_GPUS]\n",
      "ipykernel_launcher.py: error: argument --framework: invalid choice: '/run/user/1000/jupyter/runtime/kernel-v3658fbf02048a0203d77b1a1ed3b4221ee3718678.json' (choose from 'tf', 'tf2', 'torch')\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qrbao/anaconda3/envs/rllib_o/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "from ray.rllib.algorithms.sac.sac import SACConfig\n",
    "from ray.rllib.core.rl_module.default_model_config import DefaultModelConfig\n",
    "from ray.rllib.utils.test_utils import add_rllib_example_script_args\n",
    "\n",
    "parser = add_rllib_example_script_args(\n",
    "    default_timesteps=1000000,\n",
    "    default_reward=12000.0,\n",
    "    default_iters=2000,\n",
    ")\n",
    "parser.set_defaults(enable_new_api_stack=True)\n",
    "# Use `parser` to add your own custom command line options to this script\n",
    "# and (if needed) use their values to set up `config` below.\n",
    "args = parser.parse_args()\n",
    "\n",
    "config = (\n",
    "    SACConfig()\n",
    "    .environment(\"HalfCheetah-v4\")\n",
    "    .training(\n",
    "        initial_alpha=1.001,\n",
    "        # lr=0.0006 is very high, w/ 4 GPUs -> 0.0012\n",
    "        # Might want to lower it for better stability, but it does learn well.\n",
    "        actor_lr=2e-4 * (args.num_learners or 1) ** 0.5,\n",
    "        critic_lr=8e-4 * (args.num_learners or 1) ** 0.5,\n",
    "        alpha_lr=9e-4 * (args.num_learners or 1) ** 0.5,\n",
    "        lr=None,\n",
    "        target_entropy=\"auto\",\n",
    "        n_step=(1, 5),  # 1?\n",
    "        tau=0.005,\n",
    "        train_batch_size_per_learner=256,\n",
    "        target_network_update_freq=1,\n",
    "        replay_buffer_config={\n",
    "            \"type\": \"PrioritizedEpisodeReplayBuffer\",\n",
    "            \"capacity\": 100000,\n",
    "            \"alpha\": 0.6,\n",
    "            \"beta\": 0.4,\n",
    "        },\n",
    "        num_steps_sampled_before_learning_starts=10000,\n",
    "    )\n",
    "    .rl_module(\n",
    "        model_config=DefaultModelConfig(\n",
    "            fcnet_hiddens=[256, 256],\n",
    "            fcnet_activation=\"relu\",\n",
    "            fcnet_kernel_initializer=nn.init.xavier_uniform_,\n",
    "            head_fcnet_hiddens=[],\n",
    "            head_fcnet_activation=None,\n",
    "            head_fcnet_kernel_initializer=\"orthogonal_\",\n",
    "            head_fcnet_kernel_initializer_kwargs={\"gain\": 0.01},\n",
    "        ),\n",
    "    )\n",
    "    .reporting(\n",
    "        metrics_num_episodes_for_smoothing=5,\n",
    "        min_sample_timesteps_per_iteration=1000,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from ray.rllib.utils.test_utils import run_rllib_example_script_experiment\n",
    "\n",
    "    run_rllib_example_script_experiment(config, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6184a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Uses Stable-Baselines3 to train agents to play the Waterworld environment using SuperSuit vector envs.\n",
    "\n",
    "For more information, see https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html\n",
    "\n",
    "Author: Elliot (https://github.com/elliottower)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "from pettingzoo.sisl import waterworld_v4\n",
    "\n",
    "\n",
    "def train_butterfly_supersuit(\n",
    "    env_fn, steps: int = 10_000, seed: int | None = 0, **env_kwargs\n",
    "):\n",
    "    # Train a single model to play as each agent in a cooperative Parallel environment\n",
    "    env = aec_to_parallel(env_fn)\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "    # env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=\"stable_baselines3\")\n",
    "\n",
    "    # Note: Waterworld's observation space is discrete (242,) so we use an MLP policy rather than CNN\n",
    "    model = PPO(\n",
    "        MlpPolicy,\n",
    "        env,\n",
    "        verbose=3,\n",
    "        learning_rate=1e-3,\n",
    "        batch_size=256,\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=steps)\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval(env_fn, num_games: int = 100, render_mode: str | None = None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    print(\n",
    "        f\"\\nStarting evaluation on {str(env.metadata['name'])} (num_games={num_games}, render_mode={render_mode})\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = PPO.load(latest_policy)\n",
    "\n",
    "    rewards = {agent: 0 for agent in env.possible_agents}\n",
    "\n",
    "    # Note: We train using the Parallel API but evaluate using the AEC API\n",
    "    # SB3 models are designed for single-agent settings, we get around this by using he same model for every agent\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            for a in env.agents:\n",
    "                rewards[a] += env.rewards[a]\n",
    "            if termination or truncation:\n",
    "                break\n",
    "            else:\n",
    "                act = model.predict(obs, deterministic=True)[0]\n",
    "\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    avg_reward = sum(rewards.values()) / len(rewards.values())\n",
    "    print(\"Rewards: \", rewards)\n",
    "    print(f\"Avg reward: {avg_reward}\")\n",
    "    return avg_reward\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent_algos = [\"PPO\", \"PPO\", \"DQN\", \"DQN\",\"PPO\", \"PPO\", \"DQN\", \"DQN\",\"DQN\", \"DQN\"]\n",
    "    env_fn = waterworld_v4.env(\n",
    "        render_mode=\"human\",\n",
    "        n_predators=5,\n",
    "        n_preys=5,\n",
    "        n_evaders=1,\n",
    "        n_obstacles=2,\n",
    "        obstacle_coord=[(0.2, 0.2), (0.8, 0.2)],\n",
    "        n_poisons=20,\n",
    "        agent_algorithms=agent_algos\n",
    "    )\n",
    "    env_kwargs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13721a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "Reset complete:\n",
      "  Agents: ['predator_0', 'predator_1', 'predator_2', 'predator_3', 'predator_4', 'prey_0', 'prey_1', 'prey_2', 'prey_3', 'prey_4']\n",
      "  _cumulative_rewards keys: ['predator_0', 'predator_1', 'predator_2', 'predator_3', 'predator_4', 'prey_0', 'prey_1', 'prey_2', 'prey_3', 'prey_4']\n",
      "Starting training on waterworld_v4.\n",
      "Using cuda device\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The environment is of type <class 'supersuit.vector.markov_vector_wrapper.MarkovVectorEnv'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Train a model (takes ~3 minutes on GPU)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_butterfly_supersuit\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m196_608\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Evaluate 10 games (average reward should be positive but can vary significantly)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28meval\u001b[39m(env_fn, num_games\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39menv_kwargs)\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mtrain_butterfly_supersuit\u001b[0;34m(env_fn, steps, seed, **env_kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m env \u001b[38;5;241m=\u001b[39m ss\u001b[38;5;241m.\u001b[39mpettingzoo_env_to_vec_env_v1(env)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# env = ss.concat_vec_envs_v1(env, 8, num_cpus=2, base_class=\"stable_baselines3\")\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Note: Waterworld's observation space is discrete (242,) so we use an MLP policy rather than CNN\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMlpPolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39msteps)\n\u001b[1;32m     43\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:109\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    108\u001b[0m ):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgae_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgae_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43ment_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ment_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvf_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvf_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_init_setup_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiBinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:86\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     63\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     supported_action_spaces: Optional[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m ):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m=\u001b[39m n_steps\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:170\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     env \u001b[38;5;241m=\u001b[39m maybe_make_env(env, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m--> 170\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:217\u001b[0m, in \u001b[0;36mBaseAlgorithm._wrap_env\u001b[0;34m(env, verbose, monitor_wrapper)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03mWrap environment with the appropriate wrappers if needed.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03mFor instance, to have a vectorized environment\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m:return: The wrapped environment.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, VecEnv):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# Patch to support gym 0.21/0.26 and gymnasium\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43m_patch_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_wrapped(env, Monitor) \u001b[38;5;129;01mand\u001b[39;00m monitor_wrapper:\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rllib/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:33\u001b[0m, in \u001b[0;36m_patch_env\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gym_installed \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, gym\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment is of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(env)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, not a Gymnasium \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment. In this case, we expect OpenAI Gym to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstalled and the environment to be an OpenAI Gym environment.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshimmy\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The environment is of type <class 'supersuit.vector.markov_vector_wrapper.MarkovVectorEnv'>, not a Gymnasium environment. In this case, we expect OpenAI Gym to be installed and the environment to be an OpenAI Gym environment."
     ]
    }
   ],
   "source": [
    "    print(123)\n",
    "    # Train a model (takes ~3 minutes on GPU)\n",
    "    train_butterfly_supersuit(env_fn, steps=196_608, seed=0, **env_kwargs)\n",
    "\n",
    "    # Evaluate 10 games (average reward should be positive but can vary significantly)\n",
    "    eval(env_fn, num_games=10, render_mode=None, **env_kwargs)\n",
    "\n",
    "    # Watch 2 games\n",
    "    eval(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bac75df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_fn = waterworld_v4.env()\n",
    "env = aec_to_parallel(env_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa45c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
