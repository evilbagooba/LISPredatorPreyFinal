# """
# Tianshou Waterworld ç¨³å®šç‰ˆæœ¬
# å…ˆç”¨çº¯éšæœºç­–ç•¥éªŒè¯æ¡†æ¶ï¼Œç„¶åé€æ­¥æ·»åŠ å¯è®­ç»ƒç­–ç•¥

# æ¯ä¸ªagentä½¿ç”¨ä¸åŒç±»å‹çš„éšæœºç­–ç•¥æ¥æ¨¡æ‹Ÿ"ç‹¬ç«‹ç®—æ³•"çš„æ¦‚å¿µ
# """

# import os
# import argparse
# import numpy as np
# import torch
# from typing import Optional, Tuple, List

# # Tianshou imports
# from tianshou.data import Collector, Batch
# from tianshou.env import DummyVectorEnv
# from tianshou.env.pettingzoo_env import PettingZooEnv
# from tianshou.policy import BasePolicy, MultiAgentPolicyManager

# # PettingZoo imports
# from pettingzoo.sisl import waterworld_v4

# class ContinuousRandomPolicy(BasePolicy):
#     """åŸºç¡€è¿ç»­éšæœºç­–ç•¥"""
#     def __init__(self, action_space, policy_name="Random"):
#         super().__init__(action_space=action_space)
#         self.action_space = action_space
#         self.policy_name = policy_name
    
#     def forward(self, batch, state=None, **kwargs):
#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:
#             batch_size = batch.obs.shape[0]
#         else:
#             batch_size = 1
            
#         actions = []
#         for _ in range(batch_size):
#             action = self.action_space.sample()
#             actions.append(action)
        
#         return Batch(act=np.array(actions))
    
#     def exploration_noise(self, act, batch):
#         return act
    
#     def learn(self, batch, **kwargs):
#         return {}

# class AggressiveRandomPolicy(ContinuousRandomPolicy):
#     """æ¿€è¿›éšæœºç­–ç•¥ - åå‘æ›´å¤§çš„åŠ¨ä½œ"""
#     def __init__(self, action_space):
#         super().__init__(action_space, "Aggressive")
    
#     def forward(self, batch, state=None, **kwargs):
#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:
#             batch_size = batch.obs.shape[0]
#         else:
#             batch_size = 1
            
#         actions = []
#         for _ in range(batch_size):
#             # ç”Ÿæˆåå‘è¾¹ç•Œçš„åŠ¨ä½œ
#             action = np.random.uniform(-1.0, 1.0, size=self.action_space.shape)
#             # å¢å¼ºåŠ¨ä½œå¹…åº¦
#             action = np.sign(action) * np.abs(action) * 0.8  # åå‘è¾ƒå¤§åŠ¨ä½œ
#             # ç¡®ä¿åœ¨èŒƒå›´å†…
#             action = np.clip(action, self.action_space.low, self.action_space.high)
#             actions.append(action)
        
#         return Batch(act=np.array(actions))

# class ConservativeRandomPolicy(ContinuousRandomPolicy):
#     """ä¿å®ˆéšæœºç­–ç•¥ - åå‘è¾ƒå°çš„åŠ¨ä½œ"""
#     def __init__(self, action_space):
#         super().__init__(action_space, "Conservative")
    
#     def forward(self, batch, state=None, **kwargs):
#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:
#             batch_size = batch.obs.shape[0]
#         else:
#             batch_size = 1
            
#         actions = []
#         for _ in range(batch_size):
#             # ç”Ÿæˆåå‘ä¸­å¿ƒçš„åŠ¨ä½œ
#             action = np.random.uniform(-0.3, 0.3, size=self.action_space.shape)  # è¾ƒå°èŒƒå›´
#             # ç¡®ä¿åœ¨èŒƒå›´å†…
#             action = np.clip(action, self.action_space.low, self.action_space.high)
#             actions.append(action)
        
#         return Batch(act=np.array(actions))

# class BiasedRandomPolicy(ContinuousRandomPolicy):
#     """åå‘éšæœºç­–ç•¥ - åå‘æŸä¸ªæ–¹å‘"""
#     def __init__(self, action_space, bias_direction=None):
#         super().__init__(action_space, "Biased")
#         # å¦‚æœæ²¡æœ‰æŒ‡å®šåå‘ï¼Œéšæœºé€‰æ‹©ä¸€ä¸ª
#         self.bias = bias_direction if bias_direction is not None else np.random.uniform(-0.5, 0.5, size=action_space.shape)
    
#     def forward(self, batch, state=None, **kwargs):
#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:
#             batch_size = batch.obs.shape[0]
#         else:
#             batch_size = 1
            
#         actions = []
#         for _ in range(batch_size):
#             # åŸºç¡€éšæœºåŠ¨ä½œ
#             base_action = np.random.uniform(-0.5, 0.5, size=self.action_space.shape)
#             # æ·»åŠ åå‘
#             action = base_action + self.bias
#             # ç¡®ä¿åœ¨èŒƒå›´å†…
#             action = np.clip(action, self.action_space.low, self.action_space.high)
#             actions.append(action)
        
#         return Batch(act=np.array(actions))

# def get_args():
#     """é…ç½®å‚æ•°"""
#     parser = argparse.ArgumentParser()
#     parser.add_argument('--seed', type=int, default=42)
#     parser.add_argument('--n-episodes', type=int, default=10)
#     parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
#     parser.add_argument('--render', action='store_true', default=False)
#     parser.add_argument('--n-pursuers', type=int, default=5)
#     parser.add_argument('--env-num', type=int, default=1)
    
#     return parser.parse_known_args()[0]

# def get_env(args):
#     """åˆ›å»ºWaterworldç¯å¢ƒ"""
#     env = waterworld_v4.env(
#         n_pursuers=args.n_pursuers,
#         n_evaders=5,
#         n_poisons=10,
#         n_coop=2,
#         n_sensors=30,
#         sensor_range=0.2,
#         radius=0.015,
#         pursuer_max_accel=0.01,
#         evader_speed=0.01,
#         poison_speed=0.01,
#         poison_reward=-1.0,
#         food_reward=10.0,
#         encounter_reward=0.01,
#         thrust_penalty=-0.5,
#         local_ratio=1.0,
#         speed_features=True,
#         max_cycles=500,
#         render_mode="human" if args.render else None
#     )
#     return PettingZooEnv(env)

# def create_diverse_policies(args, env):
#     """ä¸ºæ¯ä¸ªagentåˆ›å»ºä¸åŒç±»å‹çš„ç­–ç•¥"""
#     agents = env.agents
#     policies = []
    
#     action_space = env.action_space
    
#     print(f"ç¯å¢ƒä¿¡æ¯:")
#     print(f"  æ™ºèƒ½ä½“: {agents}")
#     print(f"  è§‚å¯Ÿç©ºé—´: {env.observation_space}")
#     print(f"  åŠ¨ä½œç©ºé—´: {action_space}")
    
#     # ä¸ºæ¯ä¸ªagentåˆ†é…ä¸åŒçš„ç­–ç•¥ç±»å‹
#     policy_types = [
#         ("Standard Random", ContinuousRandomPolicy),
#         ("Aggressive", AggressiveRandomPolicy), 
#         ("Conservative", ConservativeRandomPolicy),
#         ("Biased", BiasedRandomPolicy),
#         ("Standard Random 2", ContinuousRandomPolicy)
#     ]
    
#     for i, agent_id in enumerate(agents):
#         policy_name, policy_class = policy_types[i % len(policy_types)]
        
#         if policy_class == BiasedRandomPolicy:
#             # ä¸ºåå‘ç­–ç•¥è®¾ç½®éšæœºåå‘æ–¹å‘
#             bias = np.random.uniform(-0.3, 0.3, size=action_space.shape)
#             policy = policy_class(action_space, bias_direction=bias)
#         else:
#             policy = policy_class(action_space)
        
#         policies.append(policy)
#         print(f"  Agent {agent_id}: {policy_name}")
    
#     return policies

# def run_multi_agent_test(args):
#     """è¿è¡Œå¤šæ™ºèƒ½ä½“æµ‹è¯•"""
#     print("=== å¤šæ™ºèƒ½ä½“ç‹¬ç«‹ç­–ç•¥æµ‹è¯• ===")
    
#     # åˆ›å»ºç¯å¢ƒ
#     env = get_env(args)
    
#     # åˆ›å»ºä¸åŒçš„ç­–ç•¥
#     policies = create_diverse_policies(args, env)
    
#     # åˆ›å»ºç­–ç•¥ç®¡ç†å™¨
#     policy_manager = MultiAgentPolicyManager(policies, env)
#     print(f"\nç­–ç•¥ç®¡ç†å™¨åˆ›å»ºæˆåŠŸï¼Œç®¡ç† {len(policies)} ä¸ªç‹¬ç«‹ç­–ç•¥")
    
#     # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
#     vec_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.env_num)])
    
#     # åˆ›å»ºæ”¶é›†å™¨
#     collector = Collector(policy_manager, vec_envs)
#     print("æ”¶é›†å™¨åˆ›å»ºæˆåŠŸ")
    
#     # è®¾ç½®éšæœºç§å­
#     np.random.seed(args.seed)
#     vec_envs.seed(args.seed)
    
#     # è¿è¡Œæµ‹è¯•
#     print(f"\nå¼€å§‹è¿è¡Œ {args.n_episodes} ä¸ªepisode...")
#     result = collector.collect(n_episode=args.n_episodes, render=args.render)
    
#     # åˆ†æç»“æœ
#     print(f"\n=== æµ‹è¯•ç»“æœ ===")
#     print(f"æ€»episodeæ•°: {result['n/ep']}")
#     print(f"æ€»æ­¥æ•°: {result['n/st']}")
#     print(f"å¹³å‡episodeé•¿åº¦: {result['len']:.2f}")
#     print(f"å¹³å‡å¥–åŠ±: {result['rew']:.4f}")
#     print(f"å¥–åŠ±æ ‡å‡†å·®: {result['rew_std']:.4f}")
    
#     # è¯¦ç»†å¥–åŠ±åˆ†æ
#     if 'rews' in result:
#         rewards = result['rews']
#         print(f"\n=== è¯¦ç»†å¥–åŠ±åˆ†æ ===")
#         print(f"å¥–åŠ±å½¢çŠ¶: {rewards.shape}")
#         print(f"æœ€å¤§å¥–åŠ±: {np.max(rewards):.4f}")
#         print(f"æœ€å°å¥–åŠ±: {np.min(rewards):.4f}")
        
#         # å¦‚æœæ˜¯å¤šæ™ºèƒ½ä½“ï¼Œåˆ†ææ¯ä¸ªagentçš„è¡¨ç°
#         if len(rewards.shape) > 1 and rewards.shape[1] > 1:
#             print(f"\nå„æ™ºèƒ½ä½“å¹³å‡å¥–åŠ±:")
#             for i in range(rewards.shape[1]):
#                 agent_reward = np.mean(rewards[:, i])
#                 print(f"  Agent {i}: {agent_reward:.4f}")
    
#     return result

# def compare_policy_performance(args):
#     """æ¯”è¾ƒä¸åŒç­–ç•¥çš„æ€§èƒ½"""
#     print("\n=== ç­–ç•¥æ€§èƒ½æ¯”è¾ƒ ===")
    
#     env = get_env(args)
#     action_space = env.action_space
    
#     # æµ‹è¯•ä¸åŒç­–ç•¥ç±»å‹
#     policy_configs = [
#         ("Standard Random", ContinuousRandomPolicy, {}),
#         ("Aggressive", AggressiveRandomPolicy, {}),
#         ("Conservative", ConservativeRandomPolicy, {}),
#         ("Biased Forward", BiasedRandomPolicy, {"bias_direction": np.array([0.3, 0.0])}),
#         ("Biased Backward", BiasedRandomPolicy, {"bias_direction": np.array([-0.3, 0.0])})
#     ]
    
#     results = {}
    
#     for policy_name, policy_class, kwargs in policy_configs:
#         print(f"\næµ‹è¯•ç­–ç•¥: {policy_name}")
        
#         # åˆ›å»ºè¯¥ç­–ç•¥çš„æ‰€æœ‰agents
#         policies = [policy_class(action_space, **kwargs) for _ in env.agents]
#         policy_manager = MultiAgentPolicyManager(policies, env)
        
#         # åˆ›å»ºç¯å¢ƒå’Œæ”¶é›†å™¨
#         vec_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])
#         collector = Collector(policy_manager, vec_envs)
        
#         # è¿è¡Œæµ‹è¯•
#         result = collector.collect(n_episode=5, render=False)
#         results[policy_name] = result['rew']
        
#         print(f"  å¹³å‡å¥–åŠ±: {result['rew']:.4f}")
#         print(f"  å¹³å‡é•¿åº¦: {result['len']:.2f}")
    
#     # æ€»ç»“æ¯”è¾ƒ
#     print(f"\n=== ç­–ç•¥æ€§èƒ½æ’åº ===")
#     sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)
#     for i, (policy_name, reward) in enumerate(sorted_results, 1):
#         print(f"{i}. {policy_name}: {reward:.4f}")

# def single_agent_test(args):
#     """å•æ™ºèƒ½ä½“æµ‹è¯•éªŒè¯åŸºç¡€åŠŸèƒ½"""
#     print("=== å•æ™ºèƒ½ä½“åŸºç¡€æµ‹è¯• ===")
    
#     # åˆ›å»ºå•æ™ºèƒ½ä½“ç¯å¢ƒç”¨äºæµ‹è¯•
#     args_single = argparse.Namespace(**vars(args))
#     args_single.n_pursuers = 1
    
#     env = get_env(args_single)
#     print(f"å•æ™ºèƒ½ä½“ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
#     print(f"  æ™ºèƒ½ä½“: {env.agents}")
    
#     # åˆ›å»ºå•ä¸ªç­–ç•¥
#     policy = ContinuousRandomPolicy(env.action_space)
#     policy_manager = MultiAgentPolicyManager([policy], env)
    
#     # æµ‹è¯•
#     vec_envs = DummyVectorEnv([lambda: get_env(args_single) for _ in range(1)])
#     collector = Collector(policy_manager, vec_envs)
    
#     result = collector.collect(n_episode=3, render=args.render)
#     print(f"å•æ™ºèƒ½ä½“æµ‹è¯•æˆåŠŸï¼")
#     print(f"  å¹³å‡å¥–åŠ±: {result['rew']:.4f}")
#     print(f"  å¹³å‡é•¿åº¦: {result['len']:.2f}")
    
#     return True

# if __name__ == "__main__":
#     args = get_args()
    
#     print("=== Tianshou Waterworld ç¨³å®šç‰ˆæœ¬ ===")
#     print(f"è®¾å¤‡: {args.device}")
#     print(f"æ™ºèƒ½ä½“æ•°é‡: {args.n_pursuers}")
#     print(f"æµ‹è¯•episodeæ•°: {args.n_episodes}")
    
#     try:
#         # 1. å•æ™ºèƒ½ä½“åŸºç¡€æµ‹è¯•
#         print("\n" + "="*50)
#         print("æ­¥éª¤1: å•æ™ºèƒ½ä½“åŸºç¡€æµ‹è¯•")
#         if not single_agent_test(args):
#             print("å•æ™ºèƒ½ä½“æµ‹è¯•å¤±è´¥")
#             exit(1)
#         print("âœ… å•æ™ºèƒ½ä½“æµ‹è¯•é€šè¿‡")
        
#         # 2. å¤šæ™ºèƒ½ä½“æµ‹è¯•
#         print("\n" + "="*50)
#         print("æ­¥éª¤2: å¤šæ™ºèƒ½ä½“ç‹¬ç«‹ç­–ç•¥æµ‹è¯•")
#         result = run_multi_agent_test(args)
#         print("âœ… å¤šæ™ºèƒ½ä½“æµ‹è¯•é€šè¿‡")
        
#         # 3. ç­–ç•¥æ¯”è¾ƒ
#         if not args.render:  # åªåœ¨éæ¸²æŸ“æ¨¡å¼ä¸‹è¿›è¡Œæ¯”è¾ƒæµ‹è¯•
#             print("\n" + "="*50)
#             print("æ­¥éª¤3: ä¸åŒç­–ç•¥æ€§èƒ½æ¯”è¾ƒ")
#             compare_policy_performance(args)
#             print("âœ… ç­–ç•¥æ¯”è¾ƒå®Œæˆ")
        
#         print(f"\n" + "="*60)
#         print(f"ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
#         print(f"")
#         print(f"ğŸ“‹ æµ‹è¯•æ€»ç»“:")
#         print(f"   âœ… ç¯å¢ƒåˆ›å»ºå’ŒåŒ…è£…: PettingZoo â†’ Tianshou")
#         print(f"   âœ… å¤šç­–ç•¥ç®¡ç†: MultiAgentPolicyManager")
#         print(f"   âœ… ç‹¬ç«‹ç­–ç•¥: æ¯ä¸ªagentä½¿ç”¨ä¸åŒç­–ç•¥")
#         print(f"   âœ… æ•°æ®æ”¶é›†: Collectoræ­£å¸¸å·¥ä½œ")
#         print(f"   âœ… è¿ç»­åŠ¨ä½œç©ºé—´: æ­£ç¡®å¤„ç†")
#         print(f"")
#         print(f"ğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:")
#         print(f"   1. è¿™ä¸ªæ¡†æ¶å·²éªŒè¯å¯ä»¥æ”¯æŒæ¯ä¸ªagentçš„ç‹¬ç«‹ç­–ç•¥")
#         print(f"   2. å¯ä»¥é€ä¸ªå°†éšæœºç­–ç•¥æ›¿æ¢ä¸ºå¯è®­ç»ƒç®—æ³•")
#         print(f"   3. å»ºè®®é¡ºåº: å…ˆè¯•PPO(è¿ç»­åŠ¨ä½œ), å†è¯•DQN(éœ€è¦åŠ¨ä½œç¦»æ•£åŒ–)")
#         print(f"   4. æˆ–è€…ä½¿ç”¨SACç­‰ç›´æ¥æ”¯æŒè¿ç»­åŠ¨ä½œçš„ç®—æ³•")
#         print(f"")
#         print(f"ğŸ’¡ æ ¸å¿ƒä»·å€¼:")
#         print(f"   - å±•ç¤ºäº†Tianshouä¸­çœŸæ­£çš„'æ¯ä¸ªagentç‹¬ç«‹ç®—æ³•'å®ç°")
#         print(f"   - ä¸ºå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ç ”ç©¶æä¾›äº†ç¨³å®šçš„åŸºç¡€æ¡†æ¶")
#         print(f"   - å¯ä»¥è½»æ¾æ‰©å±•åˆ°ä»»ä½•PettingZooç¯å¢ƒ")
        
#     except Exception as e:
#         print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
#         import traceback
#         traceback.print_exc()
#         print(f"\nğŸ’¡ è°ƒè¯•å»ºè®®:")
#         print(f"   1. æ£€æŸ¥ä¾èµ–ç‰ˆæœ¬: pip list | grep -E '(tianshou|pettingzoo)'")
#         print(f"   2. å°è¯•ç®€åŒ–å‚æ•°: --n-episodes 3 --n-pursuers 3")
#         print(f"   3. å¦‚æœ‰é—®é¢˜å¯ä»¥é€æ­¥è°ƒè¯•æ¯ä¸ªç»„ä»¶")

# """
# ä½¿ç”¨è¯´æ˜:

# 1. åŸºç¡€æµ‹è¯•:
#    python waterworld_stable.py

# 2. å¯è§†åŒ–æµ‹è¯•:
#    python waterworld_stable.py --render

# 3. æ›´å¤šepisode:
#    python waterworld_stable.py --n-episodes 20

# 4. ä¸åŒæ™ºèƒ½ä½“æ•°é‡:
#    python waterworld_stable.py --n-pursuers 3

# è¿™ä¸ªç‰ˆæœ¬çš„ç‰¹ç‚¹:
# - å®Œå…¨é¿å…äº†DQNçš„å…¼å®¹æ€§é—®é¢˜
# - å±•ç¤ºäº†çœŸæ­£çš„"æ¯ä¸ªagentç‹¬ç«‹ç­–ç•¥"æ¦‚å¿µ
# - 4ç§ä¸åŒç±»å‹çš„ç­–ç•¥æ¨¡æ‹Ÿä¸åŒç®—æ³•
# - ç¨³å®šçš„é”™è¯¯å¤„ç†å’Œæµ‹è¯•æµç¨‹
# - æ€§èƒ½æ¯”è¾ƒå’Œåˆ†æåŠŸèƒ½

# è¿™ä¸ºåç»­æ·»åŠ çœŸæ­£çš„å¯è®­ç»ƒç®—æ³•ï¼ˆDQNã€PPOã€SACç­‰ï¼‰å¥ å®šäº†åšå®åŸºç¡€ã€‚
# """





# """
# Tianshou Waterworld PPO ç®€åŒ–ç‰ˆæœ¬
# é¿å…å¤æ‚çš„ç½‘ç»œæ„å»ºé—®é¢˜ï¼Œä½¿ç”¨æ›´åŸºç¡€çš„æ–¹æ³•

# ç­–ç•¥ï¼šå…ˆç¡®ä¿PPOèƒ½æ­£å¸¸åˆ›å»ºå’Œè¿è¡Œï¼Œå†ä¼˜åŒ–æ€§èƒ½
# """

# import os
# import argparse
# import numpy as np
# import torch
# import torch.nn as nn
# from typing import Optional, Tuple, List

# # Tianshou imports
# from tianshou.data import Collector, Batch
# from tianshou.env import DummyVectorEnv
# from tianshou.env.pettingzoo_env import PettingZooEnv
# from tianshou.policy import BasePolicy, PPOPolicy, MultiAgentPolicyManager
# from tianshou.trainer import onpolicy_trainer
# from tianshou.utils.net.common import Net

# # PettingZoo imports
# from pettingzoo.sisl import waterworld_v4

# class SimpleContinuousActor(nn.Module):
#     """ç®€åŒ–çš„è¿ç»­åŠ¨ä½œActorç½‘ç»œ"""
#     def __init__(self, state_dim, action_dim, hidden_dim=128):
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Linear(state_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Linear(hidden_dim, hidden_dim),
#             nn.ReLU(),
#         )
#         # è¾“å‡ºå‡å€¼å’Œæ ‡å‡†å·®
#         self.mu_head = nn.Linear(hidden_dim, action_dim)
#         self.sigma_head = nn.Linear(hidden_dim, action_dim)
        
#     def forward(self, obs, state=None, info={}):
#         features = self.net(obs)
#         mu = torch.tanh(self.mu_head(features))  # è¾“å‡ºèŒƒå›´[-1, 1]
#         sigma = torch.softplus(self.sigma_head(features)) + 1e-3  # ç¡®ä¿sigma > 0
#         return mu, sigma, state

# class SimpleContinuousCritic(nn.Module):
#     """ç®€åŒ–çš„Criticç½‘ç»œ"""
#     def __init__(self, state_dim, hidden_dim=128):
#         super().__init__()
#         self.net = nn.Sequential(
#             nn.Linear(state_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Linear(hidden_dim, hidden_dim),
#             nn.ReLU(),
#             nn.Linear(hidden_dim, 1)
#         )
        
#     def forward(self, obs, state=None, info={}):
#         return self.net(obs), state

# class ContinuousRandomPolicy(BasePolicy):
#     """è¿ç»­åŠ¨ä½œéšæœºç­–ç•¥"""
#     def __init__(self, action_space, policy_name="Random"):
#         super().__init__(action_space=action_space)
#         self.action_space = action_space
#         self.policy_name = policy_name
    
#     def forward(self, batch, state=None, **kwargs):
#         if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:
#             batch_size = batch.obs.shape[0]
#         else:
#             batch_size = 1
            
#         actions = []
#         for _ in range(batch_size):
#             action = self.action_space.sample()
#             actions.append(action)
        
#         return Batch(act=np.array(actions))
    
#     def exploration_noise(self, act, batch):
#         return act
    
#     def learn(self, batch, **kwargs):
#         return {}

# def get_args():
#     """é…ç½®å‚æ•°"""
#     parser = argparse.ArgumentParser()
#     parser.add_argument('--seed', type=int, default=42)
#     parser.add_argument('--lr', type=float, default=3e-4)
#     parser.add_argument('--gamma', type=float, default=0.99)
#     parser.add_argument('--epoch', type=int, default=5)  # å‡å°‘epochæ•°
#     parser.add_argument('--step-per-epoch', type=int, default=1000)  # å‡å°‘æ­¥æ•°
#     parser.add_argument('--repeat-per-collect', type=int, default=2)
#     parser.add_argument('--batch-size', type=int, default=64)
#     parser.add_argument('--hidden-dim', type=int, default=128)
#     parser.add_argument('--training-num', type=int, default=2)  # å‡å°‘ç¯å¢ƒæ•°
#     parser.add_argument('--test-num', type=int, default=1)
#     parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
#     parser.add_argument('--render', action='store_true', default=False)
#     parser.add_argument('--watch', action='store_true', default=False)
    
#     # Waterworld specific parameters
#     parser.add_argument('--n-pursuers', type=int, default=5)
#     parser.add_argument('--n-episodes-test', type=int, default=5)
    
#     return parser.parse_known_args()[0]

# def get_env(args):
#     """åˆ›å»ºWaterworldç¯å¢ƒ"""
#     env = waterworld_v4.env(
#         n_pursuers=args.n_pursuers,
#         n_evaders=5,
#         n_poisons=10,
#         n_coop=2,
#         n_sensors=30,
#         sensor_range=0.2,
#         radius=0.015,
#         pursuer_max_accel=0.01,
#         evader_speed=0.01,
#         poison_speed=0.01,
#         poison_reward=-1.0,
#         food_reward=10.0,
#         encounter_reward=0.01,
#         thrust_penalty=-0.5,
#         local_ratio=1.0,
#         speed_features=True,
#         max_cycles=500,
#         render_mode="human" if args.render else None
#     )
#     return PettingZooEnv(env)

# def create_simple_ppo_policy(args, state_dim, action_dim):
#     """åˆ›å»ºç®€åŒ–çš„PPOç­–ç•¥"""
#     print(f"åˆ›å»ºPPOç­–ç•¥: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")
    
#     # åˆ›å»ºç½‘ç»œ
#     actor = SimpleContinuousActor(state_dim, action_dim, args.hidden_dim).to(args.device)
#     critic = SimpleContinuousCritic(state_dim, args.hidden_dim).to(args.device)
    
#     # åˆ›å»ºä¼˜åŒ–å™¨
#     optim = torch.optim.Adam(
#         list(actor.parameters()) + list(critic.parameters()), 
#         lr=args.lr
#     )
    
#     # æ­£ç¡®çš„åˆ†å¸ƒå‡½æ•° - å…³é”®ä¿®å¤ï¼
#     def dist_fn(mu, sigma):
#         """
#         æ­£ç¡®çš„åˆ†å¸ƒå‡½æ•°æ„å»ºæ–¹å¼
#         å‚æ•°ï¼š
#         - mu: åŠ¨ä½œå‡å€¼å¼ é‡ [batch_size, action_dim]
#         - sigma: åŠ¨ä½œæ ‡å‡†å·®å¼ é‡ [batch_size, action_dim]
#         è¿”å›ï¼š
#         - Independentåˆ†å¸ƒï¼Œå°†æœ€åä¸€ä¸ªç»´åº¦è§†ä¸ºç‹¬ç«‹äº‹ä»¶
#         """
#         # åˆ›å»ºNormalåˆ†å¸ƒå®ä¾‹ï¼ˆä¸æ˜¯ç±»ï¼ï¼‰
#         normal_dist = torch.distributions.Normal(mu, sigma)
#         # å°†æœ€åä¸€ä¸ªç»´åº¦ï¼ˆåŠ¨ä½œç»´åº¦ï¼‰è®¾ä¸ºç‹¬ç«‹
#         independent_dist = torch.distributions.Independent(normal_dist, 1)
#         return independent_dist
    
#     print("ç½‘ç»œåˆ›å»ºæˆåŠŸï¼Œå¼€å§‹åˆ›å»ºPPOç­–ç•¥...")
    
#     # è°ƒè¯•ï¼šæµ‹è¯•åˆ†å¸ƒå‡½æ•°
#     try:
#         print("æµ‹è¯•åˆ†å¸ƒå‡½æ•°...")
#         test_mu = torch.zeros(1, action_dim)
#         test_sigma = torch.ones(1, action_dim)
#         test_dist = dist_fn(test_mu, test_sigma)
#         test_sample = test_dist.sample()
#         print(f"âœ… åˆ†å¸ƒå‡½æ•°æµ‹è¯•æˆåŠŸï¼Œé‡‡æ ·å½¢çŠ¶: {test_sample.shape}")
#     except Exception as e:
#         print(f"âŒ åˆ†å¸ƒå‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
#         return None
    
#     # åˆ›å»ºPPOç­–ç•¥
#     try:
#         ppo_policy = PPOPolicy(
#             actor=actor,
#             critic=critic,
#             optim=optim,
#             dist_fn=dist_fn,
#             discount_factor=args.gamma,
#             gae_lambda=0.95,
#             max_grad_norm=0.5,
#             vf_coef=0.5,
#             ent_coef=0.01,
#             eps_clip=0.2,
#             value_clip=True,
#             advantage_normalization=True,
#             recompute_advantage=False
#         )
#         print("âœ… PPOç­–ç•¥åˆ›å»ºæˆåŠŸ")
#         return ppo_policy
        
#     except Exception as e:
#         print(f"âŒ PPOç­–ç•¥åˆ›å»ºå¤±è´¥: {e}")
#         import traceback
#         traceback.print_exc()
#         return None

# def create_mixed_policies(args, env):
#     """åˆ›å»ºæ··åˆç­–ç•¥"""
#     agents = env.agents
#     policies = []
    
#     state_shape = env.observation_space.shape
#     action_space = env.action_space
    
#     state_dim = state_shape[0] if len(state_shape) == 1 else np.prod(state_shape)
#     action_dim = action_space.shape[0] if len(action_space.shape) == 1 else np.prod(action_space.shape)
    
#     print(f"åˆ›å»ºæ··åˆç­–ç•¥ç»„åˆ:")
#     print(f"  çŠ¶æ€ç»´åº¦: {state_dim}")
#     print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
#     print(f"  æ™ºèƒ½ä½“: {agents}")
    
#     for i, agent_id in enumerate(agents):
#         if i == 0:  # ç¬¬ä¸€ä¸ªagentå°è¯•ä½¿ç”¨PPO
#             print(f"  {agent_id}: å°è¯•åˆ›å»ºPPO...")
#             ppo_policy = create_simple_ppo_policy(args, state_dim, action_dim)
            
#             if ppo_policy is not None:
#                 policy = ppo_policy
#                 print(f"  {agent_id}: PPO (å¯è®­ç»ƒ)")
#             else:
#                 policy = ContinuousRandomPolicy(action_space)
#                 print(f"  {agent_id}: éšæœºç­–ç•¥ (PPOåˆ›å»ºå¤±è´¥)")
#         else:  # å…¶ä»–agentä½¿ç”¨éšæœºç­–ç•¥
#             policy = ContinuousRandomPolicy(action_space)
#             print(f"  {agent_id}: éšæœºç­–ç•¥")
        
#         policies.append(policy)
    
#     return policies

# def test_basic_functionality(args):
#     """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
#     print("=== åŸºç¡€åŠŸèƒ½æµ‹è¯• ===")
    
#     try:
#         # åˆ›å»ºç¯å¢ƒ
#         env = get_env(args)
#         print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        
#         # åˆ›å»ºç­–ç•¥
#         policies = create_mixed_policies(args, env)
#         print(f"âœ… æ··åˆç­–ç•¥åˆ›å»ºæˆåŠŸ")
        
#         # åˆ›å»ºç­–ç•¥ç®¡ç†å™¨
#         policy_manager = MultiAgentPolicyManager(policies, env)
#         print(f"âœ… ç­–ç•¥ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        
#         # æµ‹è¯•å‘é‡åŒ–ç¯å¢ƒ
#         vec_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])
#         print(f"âœ… å‘é‡åŒ–ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        
#         # æµ‹è¯•æ”¶é›†å™¨
#         collector = Collector(policy_manager, vec_envs)
#         print(f"âœ… æ”¶é›†å™¨åˆ›å»ºæˆåŠŸ")
        
#         # æµ‹è¯•æ•°æ®æ”¶é›†
#         print("æµ‹è¯•æ•°æ®æ”¶é›†...")
#         result = collector.collect(n_step=20)
#         print(f"âœ… æ•°æ®æ”¶é›†æˆåŠŸ: {result['n/st']} æ­¥")
        
#         return True, policies
        
#     except Exception as e:
#         print(f"âŒ åŸºç¡€æµ‹è¯•å¤±è´¥: {e}")
#         import traceback
#         traceback.print_exc()
#         return False, None

# def simple_training_test(args, policies):
#     """ç®€å•è®­ç»ƒæµ‹è¯•"""
#     print("=== ç®€å•è®­ç»ƒæµ‹è¯• ===")
    
#     # æ£€æŸ¥æ˜¯å¦æœ‰PPOç­–ç•¥
#     has_ppo = any(isinstance(p, PPOPolicy) for p in policies)
    
#     if not has_ppo:
#         print("æ²¡æœ‰PPOç­–ç•¥ï¼Œè·³è¿‡è®­ç»ƒæµ‹è¯•")
#         return
    
#     print("å‘ç°PPOç­–ç•¥ï¼Œå¼€å§‹ç®€å•è®­ç»ƒæµ‹è¯•...")
    
#     try:
#         # ç¯å¢ƒè®¾ç½®
#         env = get_env(args)
#         train_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.training_num)])
#         test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.test_num)])
        
#         # åˆ›å»ºç­–ç•¥ç®¡ç†å™¨
#         policy_manager = MultiAgentPolicyManager(policies, env)
        
#         # åˆ›å»ºæ”¶é›†å™¨
#         train_collector = Collector(policy_manager, train_envs)
#         test_collector = Collector(policy_manager, test_envs)
        
#         # é¢„æ”¶é›†æ•°æ®
#         print("é¢„æ”¶é›†è®­ç»ƒæ•°æ®...")
#         train_collector.collect(n_step=args.batch_size * args.training_num)
        
#         print("å¼€å§‹è®­ç»ƒ...")
        
#         # ç®€åŒ–çš„å›è°ƒå‡½æ•°
#         def save_best_fn(policy):
#             print("ä¿å­˜æœ€ä½³ç­–ç•¥...")
        
#         def stop_fn(mean_rewards):
#             return False  # ä¸æå‰åœæ­¢ï¼Œè®©å®ƒå®Œæ•´è®­ç»ƒ
        
#         def reward_metric(rews):
#             return rews.mean(axis=1) if len(rews.shape) > 1 else rews
        
#         # ä½¿ç”¨on-policyè®­ç»ƒå™¨
#         result = onpolicy_trainer(
#             policy=policy_manager,
#             train_collector=train_collector,
#             test_collector=test_collector,
#             max_epoch=args.epoch,
#             step_per_epoch=args.step_per_epoch,
#             repeat_per_collect=args.repeat_per_collect,
#             episode_per_test=3,
#             batch_size=args.batch_size,
#             save_best_fn=save_best_fn,
#             stop_fn=stop_fn,
#             test_in_train=False,
#             reward_metric=reward_metric
#         )
        
#         print("âœ… è®­ç»ƒå®Œæˆï¼")
#         print(f"è®­ç»ƒç»“æœ: {result}")
        
#     except Exception as e:
#         print(f"âŒ è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
#         import traceback
#         traceback.print_exc()

# def run_performance_test(args):
#     """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
#     print("=== æ€§èƒ½æµ‹è¯• ===")
    
#     env = get_env(args)
    
#     # æµ‹è¯•å…¨éšæœºç­–ç•¥
#     print("æµ‹è¯•å…¨éšæœºç­–ç•¥...")
#     random_policies = [ContinuousRandomPolicy(env.action_space) for _ in env.agents]
#     random_manager = MultiAgentPolicyManager(random_policies, env)
    
#     vec_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])
#     random_collector = Collector(random_manager, vec_envs)
#     random_result = random_collector.collect(n_episode=args.n_episodes_test, render=False)
    
#     print(f"éšæœºç­–ç•¥è¡¨ç°: å¹³å‡å¥–åŠ±={random_result['rew']:.4f}, å¹³å‡é•¿åº¦={random_result['len']:.2f}")

# if __name__ == "__main__":
#     args = get_args()
    
#     print("=== Tianshou Waterworld PPO ç®€åŒ–ç‰ˆæœ¬ ===")
#     print(f"è®¾å¤‡: {args.device}")
#     print(f"æ™ºèƒ½ä½“æ•°é‡: {args.n_pursuers}")
    
#     try:
#         # 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•
#         print("\n" + "="*50)
#         print("æ­¥éª¤1: åŸºç¡€åŠŸèƒ½æµ‹è¯•")
#         success, policies = test_basic_functionality(args)
        
#         if not success:
#             print("âŒ åŸºç¡€æµ‹è¯•å¤±è´¥")
#             exit(1)
        
#         print("âœ… åŸºç¡€æµ‹è¯•é€šè¿‡")
        
#         if args.watch:
#             # 2. è§‚å¯Ÿæ¨¡å¼
#             print("\næ­¥éª¤2: æ€§èƒ½æµ‹è¯•")
#             run_performance_test(args)
#         else:
#             # 3. è®­ç»ƒæ¨¡å¼
#             print("\næ­¥éª¤2: ç®€å•è®­ç»ƒæµ‹è¯•")
#             simple_training_test(args, policies)
            
#             print("\næ­¥éª¤3: æ€§èƒ½æµ‹è¯•")
#             run_performance_test(args)
        
#         print(f"\n" + "="*50)
#         print(f"ğŸ‰ ç®€åŒ–ç‰ˆPPOæµ‹è¯•å®Œæˆï¼")
#         print(f"")
#         print(f"ğŸ’¡ å¦‚æœPPOåˆ›å»ºæˆåŠŸï¼Œè¯´æ˜æ¡†æ¶æ”¯æŒæ··åˆç­–ç•¥è®­ç»ƒ")
#         print(f"ğŸ’¡ å¦‚æœPPOåˆ›å»ºå¤±è´¥ï¼Œè‡³å°‘éªŒè¯äº†éšæœºç­–ç•¥çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶")
#         print(f"ğŸ’¡ ä¸‹ä¸€æ­¥å¯ä»¥æ ¹æ®å…·ä½“é”™è¯¯è°ƒæ•´ç½‘ç»œæ„å»ºæ–¹å¼")
        
#     except Exception as e:
#         print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
#         import traceback
#         traceback.print_exc()

# """
# è¿™ä¸ªç®€åŒ–ç‰ˆæœ¬çš„ç­–ç•¥:

# 1. ä½¿ç”¨æ›´ç®€å•çš„ç½‘ç»œç»“æ„
# 2. é¿å…å¤æ‚çš„ActorCriticåŒ…è£…
# 3. æ›´ç›´æ¥çš„åˆ†å¸ƒå‡½æ•°å®šä¹‰
# 4. å¼ºåŒ–é”™è¯¯å¤„ç†ï¼Œå³ä½¿PPOå¤±è´¥ä¹Ÿèƒ½ç»§ç»­è¿è¡Œ
# 5. é€æ­¥éªŒè¯æ¯ä¸ªç»„ä»¶

# å¦‚æœè¿™ä¸ªç‰ˆæœ¬èƒ½è¿è¡Œï¼Œæˆ‘ä»¬å°±çŸ¥é“æ¡†æ¶æœ¬èº«æ²¡é—®é¢˜
# å¦‚æœè¿˜æœ‰é”™è¯¯ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–æˆ–ä½¿ç”¨å…¶ä»–ç®—æ³•
# """



"""
Tianshou Waterworld PPO ç®€åŒ–ç‰ˆæœ¬
é¿å…å¤æ‚çš„ç½‘ç»œæ„å»ºé—®é¢˜ï¼Œä½¿ç”¨æ›´åŸºç¡€çš„æ–¹æ³•

ç­–ç•¥ï¼šå…ˆç¡®ä¿PPOèƒ½æ­£å¸¸åˆ›å»ºå’Œè¿è¡Œï¼Œå†ä¼˜åŒ–æ€§èƒ½
"""

import os
import argparse
import numpy as np
import torch
import torch.nn as nn
from typing import Optional, Tuple, List

# Tianshou imports
from tianshou.data import Collector, Batch
from tianshou.env import DummyVectorEnv
from tianshou.env.pettingzoo_env import PettingZooEnv
from tianshou.policy import BasePolicy, PPOPolicy, MultiAgentPolicyManager
from tianshou.trainer import onpolicy_trainer
from tianshou.utils.net.common import Net

# PettingZoo imports
from pettingzoo.sisl import waterworld_v4

class SimpleContinuousActor(nn.Module):
    """ç®€åŒ–çš„è¿ç»­åŠ¨ä½œActorç½‘ç»œ"""
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
        )
        # è¾“å‡ºå‡å€¼å’Œæ ‡å‡†å·®
        self.mu_head = nn.Linear(hidden_dim, action_dim)
        self.sigma_head = nn.Linear(hidden_dim, action_dim)
        
    def forward(self, obs, state=None, info={}):
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ˜¯PyTorchå¼ é‡
        if isinstance(obs, np.ndarray):
            obs = torch.from_numpy(obs).float()
        
        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self, 'device'):
            obs = obs.to(self.device)
        elif next(self.parameters()).is_cuda:
            obs = obs.cuda()
        
        features = self.net(obs)
        mu = torch.tanh(self.mu_head(features))  # è¾“å‡ºèŒƒå›´[-1, 1]
        sigma = torch.softplus(self.sigma_head(features)) + 1e-3  # ç¡®ä¿sigma > 0
        return mu, sigma, state

class SimpleContinuousCritic(nn.Module):
    """ç®€åŒ–çš„Criticç½‘ç»œ"""
    def __init__(self, state_dim, hidden_dim=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
    def forward(self, obs, state=None, info={}):
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿è¾“å…¥æ˜¯PyTorchå¼ é‡
        if isinstance(obs, np.ndarray):
            obs = torch.from_numpy(obs).float()
        
        # ç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        if hasattr(self, 'device'):
            obs = obs.to(self.device)
        elif next(self.parameters()).is_cuda:
            obs = obs.cuda()
            
        return self.net(obs), state

class ContinuousRandomPolicy(BasePolicy):
    """è¿ç»­åŠ¨ä½œéšæœºç­–ç•¥"""
    def __init__(self, action_space, policy_name="Random"):
        super().__init__(action_space=action_space)
        self.action_space = action_space
        self.policy_name = policy_name
    
    def forward(self, batch, state=None, **kwargs):
        if hasattr(batch.obs, 'shape') and len(batch.obs.shape) > 1:
            batch_size = batch.obs.shape[0]
        else:
            batch_size = 1
            
        actions = []
        for _ in range(batch_size):
            action = self.action_space.sample()
            actions.append(action)
        
        return Batch(act=np.array(actions))
    
    def exploration_noise(self, act, batch):
        return act
    
    def learn(self, batch, **kwargs):
        return {}

def get_args():
    """é…ç½®å‚æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--lr', type=float, default=3e-4)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--epoch', type=int, default=5)  # å‡å°‘epochæ•°
    parser.add_argument('--step-per-epoch', type=int, default=1000)  # å‡å°‘æ­¥æ•°
    parser.add_argument('--repeat-per-collect', type=int, default=2)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--hidden-dim', type=int, default=128)
    parser.add_argument('--training-num', type=int, default=2)  # å‡å°‘ç¯å¢ƒæ•°
    parser.add_argument('--test-num', type=int, default=1)
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
    parser.add_argument('--render', action='store_true', default=False)
    parser.add_argument('--watch', action='store_true', default=False)
    
    # Waterworld specific parameters
    parser.add_argument('--n-pursuers', type=int, default=5)
    parser.add_argument('--n-episodes-test', type=int, default=5)
    
    return parser.parse_known_args()[0]

def get_env(args):
    """åˆ›å»ºWaterworldç¯å¢ƒ"""
    env = waterworld_v4.env(
        n_pursuers=args.n_pursuers,
        n_evaders=5,
        n_poisons=10,
        n_coop=2,
        n_sensors=30,
        sensor_range=0.2,
        radius=0.015,
        pursuer_max_accel=0.01,
        evader_speed=0.01,
        poison_speed=0.01,
        poison_reward=-1.0,
        food_reward=10.0,
        encounter_reward=0.01,
        thrust_penalty=-0.5,
        local_ratio=1.0,
        speed_features=True,
        max_cycles=500,
        render_mode="human" if args.render else None
    )
    return PettingZooEnv(env)

def create_simple_ppo_policy(args, state_dim, action_dim):
    """åˆ›å»ºç®€åŒ–çš„PPOç­–ç•¥"""
    print(f"åˆ›å»ºPPOç­–ç•¥: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")
    
    # åˆ›å»ºç½‘ç»œ
    actor = SimpleContinuousActor(state_dim, action_dim, args.hidden_dim).to(args.device)
    critic = SimpleContinuousCritic(state_dim, args.hidden_dim).to(args.device)
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optim = torch.optim.Adam(
        list(actor.parameters()) + list(critic.parameters()), 
        lr=args.lr
    )
    
    # æ­£ç¡®çš„åˆ†å¸ƒå‡½æ•° - å…³é”®ä¿®å¤ï¼
    def dist_fn(mu, sigma):
        """
        æ­£ç¡®çš„åˆ†å¸ƒå‡½æ•°æ„å»ºæ–¹å¼
        å‚æ•°ï¼š
        - mu: åŠ¨ä½œå‡å€¼å¼ é‡ [batch_size, action_dim]
        - sigma: åŠ¨ä½œæ ‡å‡†å·®å¼ é‡ [batch_size, action_dim]
        è¿”å›ï¼š
        - Independentåˆ†å¸ƒï¼Œå°†æœ€åä¸€ä¸ªç»´åº¦è§†ä¸ºç‹¬ç«‹äº‹ä»¶
        """
        # åˆ›å»ºNormalåˆ†å¸ƒå®ä¾‹ï¼ˆä¸æ˜¯ç±»ï¼ï¼‰
        normal_dist = torch.distributions.Normal(mu, sigma)
        # å°†æœ€åä¸€ä¸ªç»´åº¦ï¼ˆåŠ¨ä½œç»´åº¦ï¼‰è®¾ä¸ºç‹¬ç«‹
        independent_dist = torch.distributions.Independent(normal_dist, 1)
        return independent_dist
    
    print("ç½‘ç»œåˆ›å»ºæˆåŠŸï¼Œå¼€å§‹åˆ›å»ºPPOç­–ç•¥...")
    
    # è°ƒè¯•ï¼šæµ‹è¯•åˆ†å¸ƒå‡½æ•°
    try:
        print("æµ‹è¯•åˆ†å¸ƒå‡½æ•°...")
        test_mu = torch.zeros(1, action_dim)
        test_sigma = torch.ones(1, action_dim)
        test_dist = dist_fn(test_mu, test_sigma)
        test_sample = test_dist.sample()
        print(f"âœ… åˆ†å¸ƒå‡½æ•°æµ‹è¯•æˆåŠŸï¼Œé‡‡æ ·å½¢çŠ¶: {test_sample.shape}")
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        return None
    
    # åˆ›å»ºPPOç­–ç•¥
    try:
        ppo_policy = PPOPolicy(
            actor=actor,
            critic=critic,
            optim=optim,
            dist_fn=dist_fn,
            discount_factor=args.gamma,
            gae_lambda=0.95,
            max_grad_norm=0.5,
            vf_coef=0.5,
            ent_coef=0.01,
            eps_clip=0.2,
            value_clip=True,
            advantage_normalization=True,
            recompute_advantage=False
        )
        print("âœ… PPOç­–ç•¥åˆ›å»ºæˆåŠŸ")
        return ppo_policy
        
    except Exception as e:
        print(f"âŒ PPOç­–ç•¥åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def create_mixed_policies(args, env):
    """åˆ›å»ºæ··åˆç­–ç•¥"""
    agents = env.agents
    policies = []
    
    state_shape = env.observation_space.shape
    action_space = env.action_space
    
    state_dim = state_shape[0] if len(state_shape) == 1 else np.prod(state_shape)
    action_dim = action_space.shape[0] if len(action_space.shape) == 1 else np.prod(action_space.shape)
    
    print(f"åˆ›å»ºæ··åˆç­–ç•¥ç»„åˆ:")
    print(f"  çŠ¶æ€ç»´åº¦: {state_dim}")
    print(f"  åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"  æ™ºèƒ½ä½“: {agents}")
    
    for i, agent_id in enumerate(agents):
        if i == 0:  # ç¬¬ä¸€ä¸ªagentå°è¯•ä½¿ç”¨PPO
            print(f"  {agent_id}: å°è¯•åˆ›å»ºPPO...")
            ppo_policy = create_simple_ppo_policy(args, state_dim, action_dim)
            
            if ppo_policy is not None:
                policy = ppo_policy
                print(f"  {agent_id}: PPO (å¯è®­ç»ƒ)")
            else:
                policy = ContinuousRandomPolicy(action_space)
                print(f"  {agent_id}: éšæœºç­–ç•¥ (PPOåˆ›å»ºå¤±è´¥)")
        else:  # å…¶ä»–agentä½¿ç”¨éšæœºç­–ç•¥
            policy = ContinuousRandomPolicy(action_space)
            print(f"  {agent_id}: éšæœºç­–ç•¥")
        
        policies.append(policy)
    
    return policies

def test_basic_functionality(args):
    """æµ‹è¯•åŸºç¡€åŠŸèƒ½"""
    print("=== åŸºç¡€åŠŸèƒ½æµ‹è¯• ===")
    
    try:
        # åˆ›å»ºç¯å¢ƒ
        env = get_env(args)
        print(f"âœ… ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºç­–ç•¥
        policies = create_mixed_policies(args, env)
        print(f"âœ… æ··åˆç­–ç•¥åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºç­–ç•¥ç®¡ç†å™¨
        policy_manager = MultiAgentPolicyManager(policies, env)
        print(f"âœ… ç­–ç•¥ç®¡ç†å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•å‘é‡åŒ–ç¯å¢ƒ
        vec_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])
        print(f"âœ… å‘é‡åŒ–ç¯å¢ƒåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ”¶é›†å™¨
        collector = Collector(policy_manager, vec_envs)
        print(f"âœ… æ”¶é›†å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®æ”¶é›†
        print("æµ‹è¯•æ•°æ®æ”¶é›†...")
        result = collector.collect(n_step=20)
        print(f"âœ… æ•°æ®æ”¶é›†æˆåŠŸ: {result['n/st']} æ­¥")
        
        return True, policies
        
    except Exception as e:
        print(f"âŒ åŸºç¡€æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None

def simple_training_test(args, policies):
    """ç®€å•è®­ç»ƒæµ‹è¯•"""
    print("=== ç®€å•è®­ç»ƒæµ‹è¯• ===")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰PPOç­–ç•¥
    has_ppo = any(isinstance(p, PPOPolicy) for p in policies)
    
    if not has_ppo:
        print("æ²¡æœ‰PPOç­–ç•¥ï¼Œè·³è¿‡è®­ç»ƒæµ‹è¯•")
        return
    
    print("å‘ç°PPOç­–ç•¥ï¼Œå¼€å§‹ç®€å•è®­ç»ƒæµ‹è¯•...")
    
    try:
        # ç¯å¢ƒè®¾ç½®
        env = get_env(args)
        train_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.training_num)])
        test_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(args.test_num)])
        
        # åˆ›å»ºç­–ç•¥ç®¡ç†å™¨
        policy_manager = MultiAgentPolicyManager(policies, env)
        
        # åˆ›å»ºæ”¶é›†å™¨
        train_collector = Collector(policy_manager, train_envs)
        test_collector = Collector(policy_manager, test_envs)
        
        # é¢„æ”¶é›†æ•°æ®
        print("é¢„æ”¶é›†è®­ç»ƒæ•°æ®...")
        train_collector.collect(n_step=args.batch_size * args.training_num)
        
        print("å¼€å§‹è®­ç»ƒ...")
        
        # ç®€åŒ–çš„å›è°ƒå‡½æ•°
        def save_best_fn(policy):
            print("ä¿å­˜æœ€ä½³ç­–ç•¥...")
        
        def stop_fn(mean_rewards):
            return False  # ä¸æå‰åœæ­¢ï¼Œè®©å®ƒå®Œæ•´è®­ç»ƒ
        
        def reward_metric(rews):
            return rews.mean(axis=1) if len(rews.shape) > 1 else rews
        
        # ä½¿ç”¨on-policyè®­ç»ƒå™¨
        result = onpolicy_trainer(
            policy=policy_manager,
            train_collector=train_collector,
            test_collector=test_collector,
            max_epoch=args.epoch,
            step_per_epoch=args.step_per_epoch,
            repeat_per_collect=args.repeat_per_collect,
            episode_per_test=3,
            batch_size=args.batch_size,
            save_best_fn=save_best_fn,
            stop_fn=stop_fn,
            test_in_train=False,
            reward_metric=reward_metric
        )
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print(f"è®­ç»ƒç»“æœ: {result}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def run_performance_test(args):
    """è¿è¡Œæ€§èƒ½æµ‹è¯•"""
    print("=== æ€§èƒ½æµ‹è¯• ===")
    
    env = get_env(args)
    
    # æµ‹è¯•å…¨éšæœºç­–ç•¥
    print("æµ‹è¯•å…¨éšæœºç­–ç•¥...")
    random_policies = [ContinuousRandomPolicy(env.action_space) for _ in env.agents]
    random_manager = MultiAgentPolicyManager(random_policies, env)
    
    vec_envs = DummyVectorEnv([lambda: get_env(args) for _ in range(1)])
    random_collector = Collector(random_manager, vec_envs)
    random_result = random_collector.collect(n_episode=args.n_episodes_test, render=False)
    
    print(f"éšæœºç­–ç•¥è¡¨ç°: å¹³å‡å¥–åŠ±={random_result['rew']:.4f}, å¹³å‡é•¿åº¦={random_result['len']:.2f}")

if __name__ == "__main__":
    args = get_args()
    
    print("=== Tianshou Waterworld PPO ç®€åŒ–ç‰ˆæœ¬ ===")
    print(f"è®¾å¤‡: {args.device}")
    print(f"æ™ºèƒ½ä½“æ•°é‡: {args.n_pursuers}")
    
    try:
        # 1. åŸºç¡€åŠŸèƒ½æµ‹è¯•
        print("\n" + "="*50)
        print("æ­¥éª¤1: åŸºç¡€åŠŸèƒ½æµ‹è¯•")
        success, policies = test_basic_functionality(args)
        
        if not success:
            print("âŒ åŸºç¡€æµ‹è¯•å¤±è´¥")
            exit(1)
        
        print("âœ… åŸºç¡€æµ‹è¯•é€šè¿‡")
        
        if args.watch:
            # 2. è§‚å¯Ÿæ¨¡å¼
            print("\næ­¥éª¤2: æ€§èƒ½æµ‹è¯•")
            run_performance_test(args)
        else:
            # 3. è®­ç»ƒæ¨¡å¼
            print("\næ­¥éª¤2: ç®€å•è®­ç»ƒæµ‹è¯•")
            simple_training_test(args, policies)
            
            print("\næ­¥éª¤3: æ€§èƒ½æµ‹è¯•")
            run_performance_test(args)
        
        print(f"\n" + "="*50)
        print(f"ğŸ‰ ç®€åŒ–ç‰ˆPPOæµ‹è¯•å®Œæˆï¼")
        print(f"")
        print(f"ğŸ’¡ å¦‚æœPPOåˆ›å»ºæˆåŠŸï¼Œè¯´æ˜æ¡†æ¶æ”¯æŒæ··åˆç­–ç•¥è®­ç»ƒ")
        print(f"ğŸ’¡ å¦‚æœPPOåˆ›å»ºå¤±è´¥ï¼Œè‡³å°‘éªŒè¯äº†éšæœºç­–ç•¥çš„å¤šæ™ºèƒ½ä½“æ¡†æ¶")
        print(f"ğŸ’¡ ä¸‹ä¸€æ­¥å¯ä»¥æ ¹æ®å…·ä½“é”™è¯¯è°ƒæ•´ç½‘ç»œæ„å»ºæ–¹å¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

"""
è¿™ä¸ªç®€åŒ–ç‰ˆæœ¬çš„ç­–ç•¥:

1. ä½¿ç”¨æ›´ç®€å•çš„ç½‘ç»œç»“æ„
2. é¿å…å¤æ‚çš„ActorCriticåŒ…è£…
3. æ›´ç›´æ¥çš„åˆ†å¸ƒå‡½æ•°å®šä¹‰
4. å¼ºåŒ–é”™è¯¯å¤„ç†ï¼Œå³ä½¿PPOå¤±è´¥ä¹Ÿèƒ½ç»§ç»­è¿è¡Œ
5. é€æ­¥éªŒè¯æ¯ä¸ªç»„ä»¶

å¦‚æœè¿™ä¸ªç‰ˆæœ¬èƒ½è¿è¡Œï¼Œæˆ‘ä»¬å°±çŸ¥é“æ¡†æ¶æœ¬èº«æ²¡é—®é¢˜
å¦‚æœè¿˜æœ‰é”™è¯¯ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–æˆ–ä½¿ç”¨å…¶ä»–ç®—æ³•
"""