"""
Waterworld: Flexible Training System
æ”¯æŒä»»æ„ agent ç»„åˆçš„è®­ç»ƒé…ç½®
"""

from pettingzoo.sisl import waterworld_v4
import supersuit as ss
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecEnv, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import torch
import matplotlib.pyplot as plt
from scipy.ndimage import uniform_filter1d
import gymnasium as gym
from abc import ABC, abstractmethod
from typing import List, Dict, Optional, Union
import os


# ============================================================================
# ç­–ç•¥æ¥å£ï¼šæ”¯æŒå¤šç§å›ºå®šç­–ç•¥
# ============================================================================

class AgentPolicy(ABC):
    """å›ºå®š Agent ç­–ç•¥çš„æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def get_action(self, obs):
        """
        æ ¹æ®è§‚å¯Ÿè·å–åŠ¨ä½œ
        
        Args:
            obs: è§‚å¯Ÿå€¼ (obs_dim,)
            
        Returns:
            action: åŠ¨ä½œ (action_dim,)
        """
        pass
    
    @abstractmethod
    def reset(self):
        """é‡ç½®ç­–ç•¥çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        pass
    
    def __repr__(self):
        return f"{self.__class__.__name__}()"


class RandomPolicy(AgentPolicy):
    """éšæœºç­–ç•¥"""
    
    def __init__(self, action_dim=2, low=-1.0, high=1.0):
        self.action_dim = action_dim
        self.low = low
        self.high = high
    
    def get_action(self, obs):
        return np.random.uniform(
            low=self.low,
            high=self.high,
            size=self.action_dim
        ).astype(np.float32)
    
    def reset(self):
        pass


class TrainedModelPolicy(AgentPolicy):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ä½œä¸ºç­–ç•¥"""
    
    def __init__(self, model_path, device='cpu'):
        """
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (.zip æ–‡ä»¶)
            device: 'cpu' æˆ– 'cuda'
        """
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")
        
        self.model = PPO.load(model_path, device=device)
        self.model_path = model_path
        print(f"    Loaded model: {model_path}")
    
    def get_action(self, obs):
        action, _ = self.model.predict(obs, deterministic=True)
        return action
    
    def reset(self):
        pass
    
    def __repr__(self):
        return f"TrainedModelPolicy('{os.path.basename(self.model_path)}')"


class RuleBasedPolicy(AgentPolicy):
    """åŸºäºè§„åˆ™çš„ç­–ç•¥ï¼ˆå¯æ‰©å±•ï¼‰"""
    
    def __init__(self, rule_type='stay'):
        """
        Args:
            rule_type: è§„åˆ™ç±»å‹
                - 'stay': ä¿æŒé™æ­¢
                - 'forward': å‘å‰ç§»åŠ¨
                - 'circle': åœ†å‘¨è¿åŠ¨
        """
        self.rule_type = rule_type
        self.step_count = 0
    
    def get_action(self, obs):
        self.step_count += 1
        
        if self.rule_type == 'stay':
            return np.array([0.0, 0.0], dtype=np.float32)
        
        elif self.rule_type == 'forward':
            return np.array([1.0, 0.0], dtype=np.float32)
        
        elif self.rule_type == 'circle':
            angle = self.step_count * 0.1
            return np.array([np.cos(angle), np.sin(angle)], dtype=np.float32)
        
        else:
            return np.array([0.0, 0.0], dtype=np.float32)
    
    def reset(self):
        self.step_count = 0
    
    def __repr__(self):
        return f"RuleBasedPolicy('{self.rule_type}')"


# ============================================================================
# Agent é…ç½®ç³»ç»Ÿ
# ============================================================================

class AgentConfig:
    """Agent é…ç½®ç±»"""
    
    def __init__(self, agent_idx, agent_type, agent_name, role, policy=None):
        """
        Args:
            agent_idx: Agent åœ¨ç¯å¢ƒä¸­çš„å…¨å±€ç´¢å¼•
            agent_type: 'predator' æˆ– 'prey'
            agent_name: Agent åç§°ï¼Œä¾‹å¦‚ 'predator_0'
            role: 'training' æˆ– 'fixed'
            policy: AgentPolicy å®ä¾‹ (role='fixed' æ—¶å¿…é¡»æä¾›)
        """
        self.agent_idx = agent_idx
        self.agent_type = agent_type
        self.agent_name = agent_name
        self.role = role
        self.policy = policy
        
        # éªŒè¯
        if role not in ['training', 'fixed']:
            raise ValueError(f"role must be 'training' or 'fixed', got: {role}")
        
        if role == 'fixed' and policy is None:
            raise ValueError(f"policy must be provided when role='fixed' for agent {agent_name}")
        
        if role == 'training' and policy is not None:
            raise ValueError(f"policy should be None when role='training' for agent {agent_name}")
    
    def __repr__(self):
        if self.role == 'training':
            return f"AgentConfig({self.agent_name}, role=training)"
        else:
            return f"AgentConfig({self.agent_name}, role=fixed, policy={self.policy})"


def create_agent_configs(
    n_predators: int,
    n_preys: int,
    train_predators: Optional[List[int]] = None,
    train_preys: Optional[List[int]] = None,
    predator_policies: Optional[Union[AgentPolicy, Dict[int, AgentPolicy]]] = None,
    prey_policies: Optional[Union[AgentPolicy, Dict[int, AgentPolicy]]] = None
) -> List[AgentConfig]:
    """
    åˆ›å»º Agent é…ç½®åˆ—è¡¨
    
    Args:
        n_predators: Predator æ€»æ•°
        n_preys: Prey æ€»æ•°
        train_predators: è¦è®­ç»ƒçš„ predator ç´¢å¼•åˆ—è¡¨ï¼ˆ0-basedï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨å›ºå®š
        train_preys: è¦è®­ç»ƒçš„ prey ç´¢å¼•åˆ—è¡¨ï¼ˆ0-basedï¼‰ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨å›ºå®š
        predator_policies: Predator çš„å›ºå®šç­–ç•¥
            - AgentPolicy: æ‰€æœ‰å›ºå®š predators ä½¿ç”¨ç›¸åŒç­–ç•¥
            - Dict[int, AgentPolicy]: æ¯ä¸ª predator ä½¿ç”¨ä¸åŒç­–ç•¥ {predator_idx: policy}
        prey_policies: Prey çš„å›ºå®šç­–ç•¥ï¼ˆåŒä¸Šï¼‰
    
    Returns:
        List[AgentConfig]: Agent é…ç½®åˆ—è¡¨
        
    Examples:
        # è®­ç»ƒ predator 0 å’Œ 2ï¼Œå…¶ä»–å…¨éƒ¨éšæœº
        configs = create_agent_configs(
            n_predators=3, n_preys=5,
            train_predators=[0, 2],
            predator_policies=RandomPolicy(),
            prey_policies=RandomPolicy()
        )
        
        # è®­ç»ƒ prey 1 å’Œ 3ï¼Œæ¯ä¸ª predator ä½¿ç”¨ä¸åŒç­–ç•¥
        configs = create_agent_configs(
            n_predators=3, n_preys=5,
            train_preys=[1, 3],
            predator_policies={
                0: RandomPolicy(),
                1: TrainedModelPolicy('model1.zip'),
                2: RuleBasedPolicy('circle')
            },
            prey_policies=RandomPolicy()
        )
    """
    configs = []
    
    # é»˜è®¤å€¼
    if train_predators is None:
        train_predators = []
    if train_preys is None:
        train_preys = []
    
    # éªŒè¯ç´¢å¼•èŒƒå›´
    if any(i >= n_predators or i < 0 for i in train_predators):
        raise ValueError(f"train_predators indices must be in [0, {n_predators-1}]")
    if any(i >= n_preys or i < 0 for i in train_preys):
        raise ValueError(f"train_preys indices must be in [0, {n_preys-1}]")
    
    # é…ç½® Predatorsï¼ˆç´¢å¼• 0 åˆ° n_predators-1ï¼‰
    for pred_idx in range(n_predators):
        agent_idx = pred_idx
        agent_name = f'predator_{pred_idx}'
        
        if pred_idx in train_predators:
            # è®­ç»ƒçš„ predator
            config = AgentConfig(
                agent_idx=agent_idx,
                agent_type='predator',
                agent_name=agent_name,
                role='training',
                policy=None
            )
        else:
            # å›ºå®šçš„ predator
            if predator_policies is None:
                raise ValueError(f"predator_policies must be provided for fixed predator {pred_idx}")
            
            # è·å–è¯¥ predator çš„ç­–ç•¥
            if isinstance(predator_policies, dict):
                if pred_idx not in predator_policies:
                    raise ValueError(f"No policy provided for predator {pred_idx} in predator_policies dict")
                policy = predator_policies[pred_idx]
            else:
                policy = predator_policies
            
            config = AgentConfig(
                agent_idx=agent_idx,
                agent_type='predator',
                agent_name=agent_name,
                role='fixed',
                policy=policy
            )
        
        configs.append(config)
    
    # é…ç½® Preysï¼ˆç´¢å¼• n_predators åˆ° n_predators+n_preys-1ï¼‰
    for prey_idx in range(n_preys):
        agent_idx = n_predators + prey_idx
        agent_name = f'prey_{prey_idx}'
        
        if prey_idx in train_preys:
            # è®­ç»ƒçš„ prey
            config = AgentConfig(
                agent_idx=agent_idx,
                agent_type='prey',
                agent_name=agent_name,
                role='training',
                policy=None
            )
        else:
            # å›ºå®šçš„ prey
            if prey_policies is None:
                raise ValueError(f"prey_policies must be provided for fixed prey {prey_idx}")
            
            # è·å–è¯¥ prey çš„ç­–ç•¥
            if isinstance(prey_policies, dict):
                if prey_idx not in prey_policies:
                    raise ValueError(f"No policy provided for prey {prey_idx} in prey_policies dict")
                policy = prey_policies[prey_idx]
            else:
                policy = prey_policies
            
            config = AgentConfig(
                agent_idx=agent_idx,
                agent_type='prey',
                agent_name=agent_name,
                role='fixed',
                policy=policy
            )
        
        configs.append(config)
    
    return configs


def print_agent_configs(configs: List[AgentConfig]):
    """æ‰“å° Agent é…ç½®ä¿¡æ¯"""
    print("\n" + "="*70)
    print("Agent Configuration")
    print("="*70)
    
    training_agents = [c for c in configs if c.role == 'training']
    fixed_agents = [c for c in configs if c.role == 'fixed']
    
    print(f"\nğŸ“Š Summary:")
    print(f"  Total Agents: {len(configs)}")
    print(f"  Training Agents: {len(training_agents)}")
    print(f"  Fixed Agents: {len(fixed_agents)}")
    
    if training_agents:
        print(f"\nğŸ¯ Training Agents ({len(training_agents)}):")
        for config in training_agents:
            print(f"  - {config.agent_name} (idx={config.agent_idx})")
    
    if fixed_agents:
        print(f"\nğŸ”’ Fixed Agents ({len(fixed_agents)}):")
        for config in fixed_agents:
            print(f"  - {config.agent_name} (idx={config.agent_idx}): {config.policy}")
    
    print("="*70)


# ============================================================================
# çµæ´»çš„ VecEnv åŒ…è£…å™¨
# ============================================================================

class FlexibleMixedAgentVecEnv(VecEnv):
    """
    æ”¯æŒä»»æ„ agent ç»„åˆè®­ç»ƒçš„è‡ªå®šä¹‰ VecEnv
    """

    def __init__(self, venv, agent_configs: List[AgentConfig]):
        """
        Args:
            venv: åŒ…è£…åçš„å‘é‡åŒ–ç¯å¢ƒ
            agent_configs: Agent é…ç½®åˆ—è¡¨
        """
        self.venv = venv
        self.agent_configs = agent_configs
        self.n_total_agents = len(agent_configs)
        
        # åˆ†ç¦»è®­ç»ƒå’Œå›ºå®šçš„ agents
        self.training_configs = [c for c in agent_configs if c.role == 'training']
        self.fixed_configs = [c for c in agent_configs if c.role == 'fixed']
        
        self.training_indices = [c.agent_idx for c in self.training_configs]
        self.fixed_indices = [c.agent_idx for c in self.fixed_configs]
        
        self.n_training = len(self.training_indices)
        self.n_fixed = len(self.fixed_indices)
        
        if self.n_training == 0:
            raise ValueError("Must have at least one training agent")
        
        # åˆ›å»ºç´¢å¼•åˆ°é…ç½®çš„æ˜ å°„
        self.idx_to_config = {c.agent_idx: c for c in agent_configs}
        
        # è·å–åŸå§‹ç©ºé—´
        original_obs_space = venv.observation_space
        original_action_space = venv.action_space
        
        # åˆ›å»ºæ–°çš„ VecEnvï¼Œnum_envs = è®­ç»ƒ agents çš„æ•°é‡
        super().__init__(
            num_envs=self.n_training,
            observation_space=original_obs_space,
            action_space=original_action_space
        )
        
        # ç¼“å­˜æœ€æ–°çš„è§‚å¯Ÿå€¼
        self.latest_obs = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        print(f"\n  FlexibleMixedAgentVecEnv initialized:")
        print(f"    - Total agents: {self.n_total_agents}")
        print(f"    - Training agents: {self.n_training}")
        print(f"    - Fixed agents: {self.n_fixed}")

    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        obs = self.venv.reset()
        self.latest_obs = obs
        
        # é‡ç½®æ‰€æœ‰å›ºå®šç­–ç•¥
        for config in self.fixed_configs:
            config.policy.reset()
        
        # è¿”å›è®­ç»ƒ agents çš„è§‚å¯Ÿ
        training_obs = obs[self.training_indices]
        return training_obs

    def step_async(self, actions):
        """
        ç»„åˆè®­ç»ƒ agents çš„åŠ¨ä½œå’Œå›ºå®š agents çš„åŠ¨ä½œ
        
        Args:
            actions: shape (n_training, action_dim) - è®­ç»ƒ agents çš„åŠ¨ä½œ
        """
        # ç”Ÿæˆå›ºå®š agents çš„åŠ¨ä½œ
        fixed_actions = np.zeros((self.n_fixed, 2), dtype=np.float32)
        for i, config in enumerate(self.fixed_configs):
            agent_idx = config.agent_idx
            obs = self.latest_obs[agent_idx] if self.latest_obs is not None else None
            fixed_actions[i] = config.policy.get_action(obs)
        
        # ç»„åˆæ‰€æœ‰åŠ¨ä½œ
        full_actions = np.zeros((self.n_total_agents, 2), dtype=np.float32)
        
        # å¡«å……è®­ç»ƒ agents çš„åŠ¨ä½œ
        for i, agent_idx in enumerate(self.training_indices):
            full_actions[agent_idx] = actions[i]
        
        # å¡«å……å›ºå®š agents çš„åŠ¨ä½œ
        for i, agent_idx in enumerate(self.fixed_indices):
            full_actions[agent_idx] = fixed_actions[i]
        
        # ä¼ é€’ç»™åº•å±‚ç¯å¢ƒ
        self.venv.step_async(full_actions)

    def step_wait(self):
        """è·å–ç¯å¢ƒç»“æœ"""
        obs, rewards, dones, infos = self.venv.step_wait()
        
        # ç¼“å­˜è§‚å¯Ÿå€¼
        self.latest_obs = obs
        
        # æå–è®­ç»ƒ agents çš„æ•°æ®
        training_obs = obs[self.training_indices]
        training_rewards = rewards[self.training_indices]
        training_dones = dones[self.training_indices]
        training_infos = [infos[i] for i in self.training_indices]
        
        return training_obs, training_rewards, training_dones, training_infos

    def close(self):
        """å…³é—­åº•å±‚ç¯å¢ƒ"""
        return self.venv.close()

    def get_attr(self, attr_name, indices=None):
        return self.venv.get_attr(attr_name, indices)

    def set_attr(self, attr_name, value, indices=None):
        return self.venv.set_attr(attr_name, value, indices)

    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):
        return self.venv.env_method(method_name, *method_args, indices=indices, **method_kwargs)

    def env_is_wrapped(self, wrapper_class, indices=None):
        return self.venv.env_is_wrapped(wrapper_class, indices)


# ============================================================================
# è®­ç»ƒç›‘æ§å›è°ƒ
# ============================================================================

class TrainingMonitorCallback(BaseCallback):
    """ç›‘æ§è®­ç»ƒè¿‡ç¨‹ + æ€§èƒ½æŒ‡æ ‡"""

    def __init__(self, training_agent_names, check_freq=1000, verbose=1):
        """
        Args:
            training_agent_names: è®­ç»ƒ agents çš„åç§°åˆ—è¡¨
        """
        super().__init__(verbose)
        self.training_agent_names = training_agent_names
        self.check_freq = check_freq
        
        # åŸæœ‰çš„å¥–åŠ±ç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        self.current_ep_reward = 0
        self.current_ep_length = 0
        
        # âœ… æ–°å¢ï¼šæ€§èƒ½æŒ‡æ ‡ç»Ÿè®¡
        self.episode_metrics = {
            'hunting_rate': [],
            'escape_rate': [],
            'foraging_rate': []
        }

    def _on_step(self):
        reward_sum = np.sum(self.locals['rewards'])
        self.current_ep_reward += reward_sum
        self.current_ep_length += 1

        if np.any(self.locals['dones']):
            # è®°å½•å¥–åŠ±
            self.episode_rewards.append(self.current_ep_reward)
            self.episode_lengths.append(self.current_ep_length)
            
            # âœ… æå–æ€§èƒ½æŒ‡æ ‡
            infos = self.locals.get('infos', [])
            self._extract_performance_metrics(infos)

            if len(self.episode_rewards) % 10 == 0:
                recent_rewards = self.episode_rewards[-10:]
                print(f"\n[Training] Episode {len(self.episode_rewards)}:")
                print(f"  Agents: {', '.join(self.training_agent_names)}")
                print(f"  Avg Reward: {np.mean(recent_rewards):.2f}")
                print(f"  Avg Length: {np.mean(self.episode_lengths[-10:]):.0f}")
                print(f"  Max Reward: {np.max(recent_rewards):.2f}")
                
                # âœ… æ‰“å°æ€§èƒ½æŒ‡æ ‡
                self._print_performance_metrics()

            self.current_ep_reward = 0
            self.current_ep_length = 0

        return True
    
    def _extract_performance_metrics(self, infos):
        """âœ… ä» infos æå–æ€§èƒ½æŒ‡æ ‡"""
        if not infos:
            return
        
        # èšåˆæœ¬episodeçš„æŒ‡æ ‡
        ep_metrics = {
            'hunting_rate': [],
            'escape_rate': [],
            'foraging_rate': []
        }
        
        for info in infos:
            if not isinstance(info, dict):
                continue
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ performance_metrics
            pm = info.get('performance_metrics', {})
            agent_name = info.get('agent_name', None)  # å¯èƒ½æ²¡æœ‰è¿™ä¸ªå­—æ®µ
            
            if pm:
                # æ”¶é›†æ‰€æœ‰è®­ç»ƒagentçš„æŒ‡æ ‡
                # æ³¨æ„ï¼šåœ¨SB3çš„VecEnvä¸­ï¼Œinfoså¯èƒ½åªåŒ…å«è®­ç»ƒagentçš„æ•°æ®
                if 'hunting_rate' in pm:
                    ep_metrics['hunting_rate'].append(pm['hunting_rate'])
                if 'escape_rate' in pm:
                    ep_metrics['escape_rate'].append(pm['escape_rate'])
                if 'foraging_rate' in pm:
                    ep_metrics['foraging_rate'].append(pm['foraging_rate'])
        
        # è®¡ç®—å¹³å‡å€¼å¹¶è®°å½•
        for key in ['hunting_rate', 'escape_rate', 'foraging_rate']:
            if ep_metrics[key]:
                avg = np.mean(ep_metrics[key])
                self.episode_metrics[key].append(avg)
    
    def _print_performance_metrics(self):
        """âœ… æ‰“å°æœ€è¿‘10ä¸ªepisodeçš„æ€§èƒ½æŒ‡æ ‡"""
        if not self.episode_metrics['hunting_rate']:
            return
        
        recent_n = min(10, len(self.episode_metrics['hunting_rate']))
        
        print(f"\n  ğŸ“Š Performance Metrics (last {recent_n} episodes):")
        
        for key, values in self.episode_metrics.items():
            if values:
                recent = values[-recent_n:]
                avg = np.mean(recent)
                std = np.std(recent)
                
                # æ ¹æ®æŒ‡æ ‡ç±»å‹é€‰æ‹©emoji
                if 'hunting' in key:
                    emoji = "ğŸ¯"
                elif 'escape' in key:
                    emoji = "ğŸƒ"
                elif 'foraging' in key:
                    emoji = "ğŸ"
                else:
                    emoji = "ğŸ“ˆ"
                
                print(f"    {emoji} {key}: {avg:.3f} Â± {std:.3f}")

# ============================================================================
# è®­ç»ƒæ›²çº¿ç»˜åˆ¶
# ============================================================================

def plot_training_curve(episode_rewards, training_info, save_path='training_curve.png'):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    
    Args:
        episode_rewards: Episode å¥–åŠ±åˆ—è¡¨
        training_info: è®­ç»ƒä¿¡æ¯å­—ç¬¦ä¸²
        save_path: ä¿å­˜è·¯å¾„
    """
    episodes = np.arange(1, len(episode_rewards) + 1)
    rewards = np.array(episode_rewards)

    plt.figure(figsize=(12, 6))

    plt.plot(episodes, rewards, alpha=0.3, color='blue', label='Raw Rewards')

    if len(rewards) >= 10:
        window_size = min(10, len(rewards))
        smoothed = uniform_filter1d(rewards, size=window_size, mode='nearest')
        plt.plot(episodes, smoothed, color='red', linewidth=2, 
                label=f'Moving Average (window={window_size})')

    if len(rewards) >= 50:
        window_size = min(50, len(rewards))
        trend = uniform_filter1d(rewards, size=window_size, mode='nearest')
        plt.plot(episodes, trend, color='green', linewidth=2, linestyle='--', 
                label=f'Trend (window={window_size})')

    plt.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)

    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Episode Reward', fontsize=12)
    plt.title(f'PPO Training: {training_info}', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)

    stats_text = f'Episodes: {len(rewards)}\n'
    stats_text += f'Mean: {np.mean(rewards):.2f}\n'
    stats_text += f'Std: {np.std(rewards):.2f}\n'
    stats_text += f'Max: {np.max(rewards):.2f}\n'
    stats_text += f'Min: {np.min(rewards):.2f}'

    n = len(rewards)
    if n >= 10:
        early = rewards[:max(1, n//10)]
        late = rewards[-max(1, n//10):]
        improvement = np.mean(late) - np.mean(early)
        stats_text += f'\nImprovement: {improvement:+.2f}'

    plt.text(0.02, 0.98, stats_text,
             transform=plt.gca().transAxes,
             fontsize=10,
             verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“ˆ Training curve saved to: {save_path}")
    plt.close()


# ============================================================================
# ç¯å¢ƒåˆ›å»ºå’Œå‡†å¤‡
# ============================================================================

def create_env(n_predators, n_preys, agent_configs):
    """åˆ›å»ºç¯å¢ƒ"""
    print("\n" + "="*70)
    print("Creating Waterworld Environment")
    print("="*70)

    total_agents = n_predators + n_preys
    
    # æ„å»º agent_algorithms åˆ—è¡¨
    agent_algos = []
    for config in agent_configs:
        if config.role == 'training':
            agent_algos.append("PPO")
        else:
            agent_algos.append("Fixed")

    env = waterworld_v4.parallel_env(
        render_mode=None,
        n_predators=n_predators,
        n_preys=n_preys,
        n_evaders=90,
        n_obstacles=2,
        obstacle_coord=[(0.2, 0.2), (0.8, 0.2)],
        n_poisons=10,
        agent_algorithms=agent_algos,
        max_cycles=3000,
        static_food=True,
        static_poison=True,
    )

    print(f"\nEnvironment Details:")
    print(f"  Predators: {n_predators}")
    print(f"  Preys: {n_preys}")
    print(f"  Total Agents: {total_agents}")
    print(f"  Food: 180 (static)")
    print(f"  Poison: 10 (static)")
    print(f"  Observation Dim: {env.observation_space('prey_0').shape}")
    print(f"  Action Dim: {env.action_space('prey_0').shape}")

    return env


def prepare_env_for_training(env, agent_configs):
    """å‡†å¤‡è®­ç»ƒç¯å¢ƒ"""
    print("\n" + "="*70)
    print("Converting Environment Format")
    print("="*70)

    # æ ‡å‡†è½¬æ¢
    env = ss.black_death_v3(env)
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    env = ss.concat_vec_envs_v1(env, 1, num_cpus=1, base_class='stable_baselines3')

    print("  âœ“ Standard conversion complete")
    print(f"  âœ“ num_envs: {env.num_envs}")

    # åº”ç”¨çµæ´»çš„æ··åˆç¯å¢ƒåŒ…è£…å™¨
    env = FlexibleMixedAgentVecEnv(env, agent_configs)

    # æ·»åŠ ç›‘æ§
    env = VecMonitor(env)
    print("  âœ“ Environment preparation complete")

    return env


# ============================================================================
# è®­ç»ƒå’Œè¯„ä¼°
# ============================================================================

def train_ppo(env, agent_configs, total_timesteps=1000000):
    """ä½¿ç”¨ PPO è®­ç»ƒ"""
    training_configs = [c for c in agent_configs if c.role == 'training']
    training_names = [c.agent_name for c in training_configs]
    
    print("\n" + "="*70)
    print("Starting PPO Training")
    print("="*70)
    print(f"Training Agents: {', '.join(training_names)}")
    print(f"Total Timesteps: {total_timesteps:,}")
    print(f"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")

    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        device='cpu'
    )

    callback = TrainingMonitorCallback(training_agent_names=training_names, check_freq=1000)

    print("\nğŸš€ Starting training...")

    model.learn(
        total_timesteps=total_timesteps,
        callback=callback,
        progress_bar=True
    )

    print("\nâœ“ Training complete!")

    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
    if callback.episode_rewards:
        rewards = np.array(callback.episode_rewards)
        print("\n" + "="*70)
        print("Training Statistics")
        print("="*70)
        print(f"Total Episodes: {len(rewards)}")
        print(f"Mean Reward: {np.mean(rewards):.2f}")
        print(f"Max Reward: {np.max(rewards):.2f}")
        print(f"Min Reward: {np.min(rewards):.2f}")

        n = len(rewards)
        if n >= 10:
            early = rewards[:max(1, n//10)]
            late = rewards[-max(1, n//10):]
            improvement = np.mean(late) - np.mean(early)

            print(f"\nLearning Analysis:")
            print(f"  Early Mean (first 10%): {np.mean(early):.2f}")
            print(f"  Late Mean (last 10%): {np.mean(late):.2f}")
            print(f"  Improvement: {improvement:+.2f}")

            if improvement > 5:
                print("  Conclusion: âœ“ Effective Learning")
            elif improvement > -5:
                print("  Conclusion: ~ Limited Learning")
            else:
                print("  Conclusion: âœ— No Effective Learning")

    return model, callback


def evaluate_model(model, env, n_episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("\n" + "="*70)
    print(f"Evaluating Model ({n_episodes} episodes)")
    print("="*70)

    episode_rewards = []

    for ep in range(n_episodes):
        obs = env.reset()
        ep_reward = 0
        ep_length = 0

        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            ep_reward += np.sum(reward)
            ep_length += 1

            if np.any(done):
                break

        episode_rewards.append(ep_reward)
        print(f"  Episode {ep+1}: Reward={ep_reward:.2f}, Length={ep_length}")

    print(f"\nEvaluation Results:")
    print(f"  Mean Reward: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"  Max Reward: {np.max(episode_rewards):.2f}")
    print(f"  Min Reward: {np.min(episode_rewards):.2f}")

    return episode_rewards


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("Waterworld: Flexible Training System")
    print("="*70)

    # ========================================
    # é…ç½®åŒºåŸŸï¼šçµæ´»é…ç½®è®­ç»ƒåœºæ™¯
    # ========================================
    
    # ç¯å¢ƒé…ç½®
    N_PREDATORS = 5
    N_PREYS = 10
    TOTAL_TIMESTEPS = 10000000
    
    # ========================================
    # åœºæ™¯ 1: è®­ç»ƒéƒ¨åˆ† Predatorsï¼Œå…¶ä»–å…¨éƒ¨éšæœº
    # ========================================
    """
    agent_configs = create_agent_configs(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        train_predators=[0, 2, 4],  # è®­ç»ƒ predator 0, 2, 4
        train_preys=None,  # ä¸è®­ç»ƒä»»ä½• prey
        predator_policies=RandomPolicy(),  # å›ºå®šçš„ predators ä½¿ç”¨éšæœºç­–ç•¥
        prey_policies=RandomPolicy()  # æ‰€æœ‰ preys ä½¿ç”¨éšæœºç­–ç•¥
    )
    """
    
    # ========================================
    # åœºæ™¯ 2: è®­ç»ƒéƒ¨åˆ† Preysï¼Œå…¶ä»–å…¨éƒ¨éšæœº
    # ========================================
    # agent_configs = create_agent_configs(
    #     n_predators=N_PREDATORS,
    #     n_preys=N_PREYS,
    #     train_predators=[1, 2],  # è®­ç»ƒ predator 1, 2
    #     train_preys=None,  # è®­ç»ƒ prey 1, 3, 5, 7
    #     predator_policies=RandomPolicy(),  # æ‰€æœ‰ predators ä½¿ç”¨éšæœºç­–ç•¥
    #     prey_policies=RandomPolicy()  # å›ºå®šçš„ preys ä½¿ç”¨éšæœºç­–ç•¥
    # )
    
    # ========================================
    # åœºæ™¯ 3: åŒæ—¶è®­ç»ƒéƒ¨åˆ† Predators å’Œ Preys
    # ========================================
    """
    agent_configs = create_agent_configs(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        train_predators=[0, 1],  # è®­ç»ƒ predator 0, 1
        train_preys=[2, 4, 6],  # è®­ç»ƒ prey 2, 4, 6
        predator_policies=RandomPolicy(),
        prey_policies=RandomPolicy()
    )
    """
    
    # ========================================
    # åœºæ™¯ 4: æ¯ä¸ªå›ºå®š agent ä½¿ç”¨ä¸åŒç­–ç•¥
    # ========================================
    """
    agent_configs = create_agent_configs(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        train_predators=[0],  # åªè®­ç»ƒ predator 0
        train_preys=[0, 1],  # è®­ç»ƒ prey 0, 1
        predator_policies={
            # ä¸ºæ¯ä¸ªå›ºå®šçš„ predator æŒ‡å®šä¸åŒç­–ç•¥
            1: RandomPolicy(),
            2: RuleBasedPolicy('circle'),
            3: RuleBasedPolicy('forward'),
            4: RandomPolicy()
        },
        prey_policies={
            # ä¸ºæ¯ä¸ªå›ºå®šçš„ prey æŒ‡å®šä¸åŒç­–ç•¥
            2: RandomPolicy(),
            3: RuleBasedPolicy('stay'),
            4: RandomPolicy(),
            5: RuleBasedPolicy('circle'),
            6: RandomPolicy(),
            7: RandomPolicy(),
            8: RuleBasedPolicy('forward'),
            9: RandomPolicy()
        }
    )
    """
    
    # ========================================
    # åœºæ™¯ 5: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ä½œä¸ºå›ºå®šç­–ç•¥ï¼ˆéœ€è¦å…ˆæœ‰æ¨¡å‹ï¼‰
    # ========================================

    agent_configs = create_agent_configs(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        train_predators=[0, 1],
        train_preys=None,
        predator_policies={
            2: TrainedModelPolicy('predator_ppo_model.zip'),
            3: RandomPolicy(),
            4: RandomPolicy()
        },
        prey_policies={
            # éƒ¨åˆ† prey ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œéƒ¨åˆ†éšæœº
            0: RandomPolicy(),
            1: RandomPolicy(),
            2: RandomPolicy(),
            3: RandomPolicy(),
            4: RandomPolicy(),
            5: RandomPolicy(),
            6: RandomPolicy(),
            7: RandomPolicy(),
            8: RandomPolicy(),
            9: RandomPolicy()
        }
    )

    
    # ========================================
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print_agent_configs(agent_configs)
    
    # ç”Ÿæˆè®­ç»ƒä¿¡æ¯å­—ç¬¦ä¸²ï¼ˆç”¨äºæ–‡ä»¶åå’Œå›¾è¡¨ï¼‰
    training_configs = [c for c in agent_configs if c.role == 'training']
    training_predators = [c for c in training_configs if c.agent_type == 'predator']
    training_preys = [c for c in training_configs if c.agent_type == 'prey']
    
    training_info_parts = []
    if training_predators:
        pred_names = [c.agent_name for c in training_predators]
        training_info_parts.append(f"Predators[{','.join([n.split('_')[1] for n in pred_names])}]")
    if training_preys:
        prey_names = [c.agent_name for c in training_preys]
        training_info_parts.append(f"Preys[{','.join([n.split('_')[1] for n in prey_names])}]")
    
    training_info = "_".join(training_info_parts)
    model_filename = f'model_{training_info}'
    curve_filename = f'training_curve_{training_info}.png'
    
    print(f"\nğŸ“ Files will be saved as:")
    print(f"  - Model: {model_filename}.zip")
    print(f"  - Curve: {curve_filename}")
    
    # 1. åˆ›å»ºç¯å¢ƒ
    raw_env = create_env(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        agent_configs=agent_configs
    )

    # 2. å‡†å¤‡è®­ç»ƒç¯å¢ƒ
    env = prepare_env_for_training(raw_env, agent_configs)

    # 3. è®­ç»ƒ
    model, callback = train_ppo(env, agent_configs, total_timesteps=TOTAL_TIMESTEPS)

    # 4. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if callback.episode_rewards:
        print("\n" + "="*70)
        print("Generating Training Curve")
        print("="*70)
        plot_training_curve(
            callback.episode_rewards,
            training_info=training_info.replace('_', ' + '),
            save_path=curve_filename
        )

    # 5. è¯„ä¼°
    episode_rewards = evaluate_model(model, env, n_episodes=10)

    # 6. ä¿å­˜æ¨¡å‹
    model.save(model_filename)
    print(f"\nğŸ’¾ Model saved: {model_filename}.zip")

    # 7. æœ€ç»ˆæ€»ç»“
    print("\n" + "="*70)
    print("Training Complete Summary")
    print("="*70)

    if callback.episode_rewards:
        rewards = np.array(callback.episode_rewards)
        eval_rewards = np.array(episode_rewards)

        print(f"\nğŸ¯ Training Configuration:")
        if training_predators:
            print(f"  Training Predators: {[c.agent_name for c in training_predators]}")
        if training_preys:
            print(f"  Training Preys: {[c.agent_name for c in training_preys]}")

        print(f"\nğŸ“Š Training Performance:")
        print(f"  Mean Reward: {np.mean(rewards):.2f}")
        print(f"  Max Reward: {np.max(rewards):.2f}")
        print(f"  Std Reward: {np.std(rewards):.2f}")

        print(f"\nğŸ§ª Evaluation Performance:")
        print(f"  Mean Reward: {np.mean(eval_rewards):.2f}")
        print(f"  Max Reward: {np.max(eval_rewards):.2f}")
        print(f"  Std Reward: {np.std(eval_rewards):.2f}")

        n = len(rewards)
        if n >= 10:
            early = rewards[:max(1, n//10)]
            late = rewards[-max(1, n//10):]
            improvement = np.mean(late) - np.mean(early)
            
            print(f"\nğŸ“ˆ Learning Progress:")
            print(f"  Early Mean (first 10%): {np.mean(early):.2f}")
            print(f"  Late Mean (last 10%): {np.mean(late):.2f}")
            print(f"  Improvement: {improvement:+.2f}")
            
            if improvement > 5:
                print(f"  Status: âœ“ Training agents learned effectively")
            elif improvement > -5:
                print(f"  Status: ~ Limited learning observed")
            else:
                print(f"  Status: âœ— No significant learning")

    env.close()
    
    print("\n" + "="*70)
    print("ğŸ‰ All Done!")
    print("="*70)
    print(f"\nğŸ“ Generated Files:")
    print(f"  âœ“ {model_filename}.zip")
    print(f"  âœ“ {curve_filename}")
    print("\nğŸ’¡ Tips:")
    print("  - Modify agent_configs in main() to try different training scenarios")
    print("  - Use saved models with TrainedModelPolicy() for hierarchical training")
    print("  - Create custom policies by extending AgentPolicy class")


if __name__ == "__main__":
    main()