"""
Waterworld: Test Script
æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹,ä¸è¿›è¡Œè®­ç»ƒ
"""

from train_selectedagent import (
    create_agent_configs,
    print_agent_configs,
    create_env,
    prepare_env_for_training,
    TrainedModelPolicy,
    RandomPolicy,
    RuleBasedPolicy
)
from stable_baselines3 import PPO
import numpy as np
import matplotlib.pyplot as plt
from typing import List
import os


def test_model(
    model_path: str,
    agent_configs: List,
    n_predators: int,
    n_preys: int,
    n_episodes: int = 20,
    render: bool = False,
    save_results: bool = True
):
    """
    æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
    
    Args:
        model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
        agent_configs: Agent é…ç½®åˆ—è¡¨
        n_predators: Predator æ•°é‡
        n_preys: Prey æ•°é‡
        n_episodes: æµ‹è¯•å›åˆæ•°
        render: æ˜¯å¦æ¸²æŸ“ï¼ˆæš‚ä¸æ”¯æŒï¼‰
        save_results: æ˜¯å¦ä¿å­˜æµ‹è¯•ç»“æœ
    """
    
    print("="*70)
    print("ğŸ§ª Waterworld Model Testing")
    print("="*70)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ Model not found: {model_path}")
    
    print(f"\nğŸ“¦ Loading model: {model_path}")
    
    # æ‰“å°é…ç½®
    print_agent_configs(agent_configs)
    
    # 1. åˆ›å»ºç¯å¢ƒ
    raw_env = create_env(
        n_predators=n_predators,
        n_preys=n_preys,
        agent_configs=agent_configs
    )
    
    # 2. å‡†å¤‡ç¯å¢ƒ
    env = prepare_env_for_training(raw_env, agent_configs)
    
    # 3. åŠ è½½æ¨¡å‹
    model = PPO.load(model_path, device='cpu')
    print(f"âœ“ Model loaded successfully")
    
    # 4. å¼€å§‹æµ‹è¯•
    print("\n" + "="*70)
    print(f"ğŸš€ Starting Testing ({n_episodes} episodes)")
    print("="*70)
    
    episode_rewards = []
    episode_lengths = []
    episode_metrics = {
        'hunting_rate': [],
        'escape_rate': [],
        'foraging_rate': []
    }
    
    for ep in range(n_episodes):
        obs = env.reset()
        ep_reward = 0
        ep_length = 0
        
        # ç”¨äºæ”¶é›†æœ¬episodeçš„æŒ‡æ ‡
        ep_infos = []
        
        while True:
            # ä½¿ç”¨æ¨¡å‹é¢„æµ‹åŠ¨ä½œï¼ˆç¡®å®šæ€§ç­–ç•¥ï¼‰
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            
            ep_reward += np.sum(reward)
            ep_length += 1
            ep_infos.extend(info)
            
            if np.any(done):
                break
        
        episode_rewards.append(ep_reward)
        episode_lengths.append(ep_length)
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        _extract_metrics(ep_infos, episode_metrics)
        
        # æ‰“å°è¿›åº¦
        print(f"  Episode {ep+1:2d}/{n_episodes}: "
              f"Reward={ep_reward:7.2f}, Length={ep_length:4d}")
    
    # 5. ç»Ÿè®¡ç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š Test Results Summary")
    print("="*70)
    
    rewards_array = np.array(episode_rewards)
    lengths_array = np.array(episode_lengths)
    
    print(f"\nğŸ¯ Episode Rewards:")
    print(f"  Mean:   {np.mean(rewards_array):7.2f} Â± {np.std(rewards_array):.2f}")
    print(f"  Median: {np.median(rewards_array):7.2f}")
    print(f"  Max:    {np.max(rewards_array):7.2f}")
    print(f"  Min:    {np.min(rewards_array):7.2f}")
    
    print(f"\nâ±ï¸  Episode Lengths:")
    print(f"  Mean:   {np.mean(lengths_array):7.1f} Â± {np.std(lengths_array):.1f}")
    print(f"  Median: {np.median(lengths_array):7.1f}")
    print(f"  Max:    {np.max(lengths_array):7.0f}")
    print(f"  Min:    {np.min(lengths_array):7.0f}")
    
    # æ‰“å°æ€§èƒ½æŒ‡æ ‡
    if episode_metrics['hunting_rate'] or episode_metrics['escape_rate'] or episode_metrics['foraging_rate']:
        print(f"\nğŸ“ˆ Performance Metrics:")
        for key, values in episode_metrics.items():
            if values:
                avg = np.mean(values)
                std = np.std(values)
                
                if 'hunting' in key:
                    emoji = "ğŸ¯"
                elif 'escape' in key:
                    emoji = "ğŸƒ"
                elif 'foraging' in key:
                    emoji = "ğŸ"
                else:
                    emoji = "ğŸ“Š"
                
                print(f"  {emoji} {key:15s}: {avg:.3f} Â± {std:.3f}")
    
    # 6. ä¿å­˜ç»“æœ
    if save_results:
        results = {
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'metrics': episode_metrics,
            'statistics': {
                'mean_reward': float(np.mean(rewards_array)),
                'std_reward': float(np.std(rewards_array)),
                'mean_length': float(np.mean(lengths_array)),
                'std_length': float(np.std(lengths_array))
            }
        }
        
        # ä¿å­˜ä¸ºnumpyæ–‡ä»¶
        model_name = os.path.splitext(os.path.basename(model_path))[0]
        results_file = f'test_results_{model_name}.npz'
        np.savez(results_file, **results)
        print(f"\nğŸ’¾ Results saved: {results_file}")
        
        # ç»˜åˆ¶æµ‹è¯•ç»“æœå›¾
        plot_test_results(episode_rewards, episode_lengths, model_name)
    
    env.close()
    
    print("\n" + "="*70)
    print("âœ… Testing Complete!")
    print("="*70)
    
    return episode_rewards, episode_lengths, episode_metrics


def _extract_metrics(infos, episode_metrics):
    """ä» infos æå–æ€§èƒ½æŒ‡æ ‡"""
    ep_metrics = {
        'hunting_rate': [],
        'escape_rate': [],
        'foraging_rate': []
    }
    
    for info in infos:
        if not isinstance(info, dict):
            continue
        
        pm = info.get('performance_metrics', {})
        if pm:
            if 'hunting_rate' in pm:
                ep_metrics['hunting_rate'].append(pm['hunting_rate'])
            if 'escape_rate' in pm:
                ep_metrics['escape_rate'].append(pm['escape_rate'])
            if 'foraging_rate' in pm:
                ep_metrics['foraging_rate'].append(pm['foraging_rate'])
    
    # è®¡ç®—å¹³å‡å€¼å¹¶è®°å½•
    for key in ['hunting_rate', 'escape_rate', 'foraging_rate']:
        if ep_metrics[key]:
            avg = np.mean(ep_metrics[key])
            episode_metrics[key].append(avg)


def plot_test_results(episode_rewards, episode_lengths, model_name):
    """ç»˜åˆ¶æµ‹è¯•ç»“æœ"""
    
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    episodes = np.arange(1, len(episode_rewards) + 1)
    
    # å·¦å›¾ï¼šå¥–åŠ±
    ax1 = axes[0]
    ax1.plot(episodes, episode_rewards, marker='o', linestyle='-', 
             color='steelblue', linewidth=2, markersize=6, label='Episode Reward')
    ax1.axhline(y=np.mean(episode_rewards), color='red', linestyle='--', 
                linewidth=2, label=f'Mean: {np.mean(episode_rewards):.2f}')
    ax1.fill_between(episodes, 
                      np.mean(episode_rewards) - np.std(episode_rewards),
                      np.mean(episode_rewards) + np.std(episode_rewards),
                      alpha=0.2, color='red')
    ax1.set_xlabel('Episode', fontsize=12)
    ax1.set_ylabel('Reward', fontsize=12)
    ax1.set_title('Test Episode Rewards', fontsize=14, fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # å³å›¾ï¼šé•¿åº¦
    ax2 = axes[1]
    ax2.plot(episodes, episode_lengths, marker='s', linestyle='-', 
             color='forestgreen', linewidth=2, markersize=6, label='Episode Length')
    ax2.axhline(y=np.mean(episode_lengths), color='orange', linestyle='--', 
                linewidth=2, label=f'Mean: {np.mean(episode_lengths):.1f}')
    ax2.fill_between(episodes,
                      np.mean(episode_lengths) - np.std(episode_lengths),
                      np.mean(episode_lengths) + np.std(episode_lengths),
                      alpha=0.2, color='orange')
    ax2.set_xlabel('Episode', fontsize=12)
    ax2.set_ylabel('Length (steps)', fontsize=12)
    ax2.set_title('Test Episode Lengths', fontsize=14, fontweight='bold')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    plt.suptitle(f'Model: {model_name}', fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    
    save_path = f'test_results_{model_name}.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ğŸ“Š Test plot saved: {save_path}")
    plt.close()


# ============================================================================
# ä¸»å‡½æ•°ï¼šé…ç½®æµ‹è¯•åœºæ™¯
# ============================================================================

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    # ========================================
    # é…ç½®æµ‹è¯•å‚æ•°
    # ========================================
    
    # ç¯å¢ƒé…ç½®
    N_PREDATORS = 5
    N_PREYS = 10
    
    # æµ‹è¯•é…ç½®
    MODEL_PATH = 'predator_ppo_model.zip'  # ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„
    N_TEST_EPISODES = 20  # æµ‹è¯•å›åˆæ•°
    
    # ========================================
    # Agent é…ç½®
    # ========================================
    
    agent_configs = create_agent_configs(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        train_predators=[0, 1],  # è¿™ä¸¤ä¸ªä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
        train_preys=None,
        predator_policies={
            2: TrainedModelPolicy('predator_ppo_model.zip'),  # å¦‚æœæœ‰å…¶ä»–è®­ç»ƒå¥½çš„æ¨¡å‹
            3: RandomPolicy(),
            4: RandomPolicy()
        },
        prey_policies={
            0: RandomPolicy(),
            1: RandomPolicy(),
            2: RandomPolicy(),
            3: RandomPolicy(),
            4: RandomPolicy(),
            5: RandomPolicy(),
            6: RandomPolicy(),
            7: RandomPolicy(),
            8: RandomPolicy(),
            9: RandomPolicy()
        }
    )
    
    # ========================================
    # æ‰§è¡Œæµ‹è¯•
    # ========================================
    
    try:
        episode_rewards, episode_lengths, metrics = test_model(
            model_path=MODEL_PATH,
            agent_configs=agent_configs,
            n_predators=N_PREDATORS,
            n_preys=N_PREYS,
            n_episodes=N_TEST_EPISODES,
            render=False,
            save_results=True
        )
        
        print("\nğŸ’¡ Test completed successfully!")
        print(f"   - Tested {N_TEST_EPISODES} episodes")
        print(f"   - Average reward: {np.mean(episode_rewards):.2f}")
        print(f"   - Average length: {np.mean(episode_lengths):.1f}")
        
    except FileNotFoundError as e:
        print(f"\nâŒ Error: {e}")
        print("   Please make sure the model file exists!")
    except Exception as e:
        print(f"\nâŒ Unexpected error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()