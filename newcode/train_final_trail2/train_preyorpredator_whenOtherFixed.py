"""
Waterworld: Bidirectional Training System
æ”¯æŒ Prey è®­ç»ƒ vs Random Predators å’Œ Predator è®­ç»ƒ vs Fixed Prey
"""

from pettingzoo.sisl import waterworld_v4
import supersuit as ss
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import VecEnv, VecMonitor
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import torch
import matplotlib.pyplot as plt
from scipy.ndimage import uniform_filter1d
import gymnasium as gym
from abc import ABC, abstractmethod
import os


# ============================================================================
# ç­–ç•¥æ¥å£ï¼šæ”¯æŒå¤šç§å›ºå®šç­–ç•¥
# ============================================================================

class AgentPolicy(ABC):
    """å›ºå®š Agent ç­–ç•¥çš„æŠ½è±¡åŸºç±»"""
    
    @abstractmethod
    def get_action(self, obs, agent_idx):
        """
        æ ¹æ®è§‚å¯Ÿè·å–åŠ¨ä½œ
        
        Args:
            obs: è§‚å¯Ÿå€¼ (obs_dim,)
            agent_idx: Agent ç´¢å¼•
            
        Returns:
            action: åŠ¨ä½œ (action_dim,)
        """
        pass
    
    @abstractmethod
    def reset(self):
        """é‡ç½®ç­–ç•¥çŠ¶æ€ï¼ˆå¦‚æœéœ€è¦ï¼‰"""
        pass


class RandomPolicy(AgentPolicy):
    """éšæœºç­–ç•¥"""
    
    def __init__(self, action_dim=2, low=-1.0, high=1.0):
        self.action_dim = action_dim
        self.low = low
        self.high = high
    
    def get_action(self, obs, agent_idx):
        return np.random.uniform(
            low=self.low,
            high=self.high,
            size=self.action_dim
        ).astype(np.float32)
    
    def reset(self):
        pass


class TrainedModelPolicy(AgentPolicy):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ä½œä¸ºç­–ç•¥"""
    
    def __init__(self, model_path, device='cpu'):
        """
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ (.zip æ–‡ä»¶)
            device: 'cpu' æˆ– 'cuda'
        """
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model not found: {model_path}")
        
        self.model = PPO.load(model_path, device=device)
        print(f"  Loaded trained model from: {model_path}")
    
    def get_action(self, obs, agent_idx):
        # ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
        action, _ = self.model.predict(obs, deterministic=True)
        return action
    
    def reset(self):
        pass


class RuleBasedPolicy(AgentPolicy):
    """åŸºäºè§„åˆ™çš„ç­–ç•¥ï¼ˆç¤ºä¾‹ï¼šå¯ä»¥æ‰©å±•ï¼‰"""
    
    def __init__(self):
        pass
    
    def get_action(self, obs, agent_idx):
        # ç¤ºä¾‹ï¼šç®€å•çš„è§„åˆ™ç­–ç•¥
        # è¿™é‡Œå¯ä»¥æ ¹æ®è§‚å¯Ÿå€¼å®ç°å¤æ‚çš„è§„åˆ™
        # æ¯”å¦‚ï¼šæœæœ€è¿‘çš„ç›®æ ‡ç§»åŠ¨ç­‰
        return np.array([0.5, 0.5], dtype=np.float32)
    
    def reset(self):
        pass


# ============================================================================
# è‡ªå®šä¹‰ VecEnvï¼šæ”¯æŒæ··åˆè®­ç»ƒæ¨¡å¼
# ============================================================================

class MixedAgentVecEnv(VecEnv):
    """
    æ”¯æŒä¸¤ç§è®­ç»ƒæ¨¡å¼çš„è‡ªå®šä¹‰ VecEnvï¼š
    1. mode='train_prey': è®­ç»ƒ preyï¼Œpredator ä½¿ç”¨å›ºå®šç­–ç•¥
    2. mode='train_predator': è®­ç»ƒ predatorï¼Œprey ä½¿ç”¨å›ºå®šç­–ç•¥
    """

    def __init__(self, venv, n_predators, n_preys, mode='train_prey', fixed_policy=None):
        """
        Args:
            venv: åŒ…è£…åçš„å‘é‡åŒ–ç¯å¢ƒ
            n_predators: Predator æ•°é‡
            n_preys: Prey æ•°é‡
            mode: 'train_prey' æˆ– 'train_predator'
            fixed_policy: AgentPolicy å®ä¾‹ï¼Œç”¨äºå›ºå®šçš„ agents
        """
        self.venv = venv
        self.n_predators = n_predators
        self.n_preys = n_preys
        self.n_total_agents = n_predators + n_preys
        self.mode = mode
        
        # è®¾ç½®å›ºå®šç­–ç•¥
        if fixed_policy is None:
            self.fixed_policy = RandomPolicy()
            print(f"  Using default RandomPolicy for fixed agents")
        else:
            self.fixed_policy = fixed_policy
        
        # Agent ç´¢å¼•
        self.predator_indices = list(range(n_predators))
        self.prey_indices = list(range(n_predators, n_predators + n_preys))
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®è®­ç»ƒå’Œå›ºå®šçš„ agents
        if mode == 'train_prey':
            self.training_indices = self.prey_indices
            self.fixed_indices = self.predator_indices
            self.n_training = n_preys
            print(f"  Mode: Training {n_preys} Preys, Fixed {n_predators} Predators")
        elif mode == 'train_predator':
            self.training_indices = self.predator_indices
            self.fixed_indices = self.prey_indices
            self.n_training = n_predators
            print(f"  Mode: Training {n_predators} Predators, Fixed {n_preys} Preys")
        else:
            raise ValueError(f"Invalid mode: {mode}. Must be 'train_prey' or 'train_predator'")
        
        # è·å–åŸå§‹ç©ºé—´
        original_obs_space = venv.observation_space
        original_action_space = venv.action_space
        
        # åˆ›å»ºæ–°çš„ VecEnvï¼Œnum_envs = è®­ç»ƒ agents çš„æ•°é‡
        super().__init__(
            num_envs=self.n_training,
            observation_space=original_obs_space,
            action_space=original_action_space
        )
        
        # ç¼“å­˜æœ€æ–°çš„è§‚å¯Ÿå€¼ï¼ˆç”¨äºå›ºå®šç­–ç•¥ï¼‰
        self.latest_obs = None

    def reset(self):
        """é‡ç½®ç¯å¢ƒï¼Œè¿”å›è®­ç»ƒ agents çš„è§‚å¯Ÿ"""
        obs = self.venv.reset()
        self.latest_obs = obs
        self.fixed_policy.reset()
        training_obs = obs[self.training_indices]
        return training_obs

    def step_async(self, actions):
        """
        ç»„åˆè®­ç»ƒ agents çš„åŠ¨ä½œå’Œå›ºå®š agents çš„åŠ¨ä½œ
        
        Args:
            actions: shape (n_training, action_dim) - è®­ç»ƒ agents çš„åŠ¨ä½œ
        """
        # ç”Ÿæˆå›ºå®š agents çš„åŠ¨ä½œ
        fixed_actions = np.zeros((len(self.fixed_indices), 2), dtype=np.float32)
        for i, agent_idx in enumerate(self.fixed_indices):
            # è·å–è¯¥ agent çš„è§‚å¯Ÿ
            obs = self.latest_obs[agent_idx] if self.latest_obs is not None else None
            fixed_actions[i] = self.fixed_policy.get_action(obs, agent_idx)
        
        # ç»„åˆæ‰€æœ‰åŠ¨ä½œï¼šæŒ‰ç…§ agent é¡ºåº
        full_actions = np.zeros((self.n_total_agents, 2), dtype=np.float32)
        
        if self.mode == 'train_prey':
            # Predators (fixed) åœ¨å‰ï¼ŒPreys (training) åœ¨å
            full_actions[self.predator_indices] = fixed_actions
            full_actions[self.prey_indices] = actions
        else:  # train_predator
            # Predators (training) åœ¨å‰ï¼ŒPreys (fixed) åœ¨å
            full_actions[self.predator_indices] = actions
            full_actions[self.prey_indices] = fixed_actions
        
        # ä¼ é€’ç»™åº•å±‚ç¯å¢ƒ
        self.venv.step_async(full_actions)

    def step_wait(self):
        """è·å–ç¯å¢ƒç»“æœï¼Œæå–è®­ç»ƒ agents çš„æ•°æ®"""
        obs, rewards, dones, infos = self.venv.step_wait()
        
        # ç¼“å­˜è§‚å¯Ÿå€¼
        self.latest_obs = obs
        
        # æå–è®­ç»ƒ agents çš„æ•°æ®
        training_obs = obs[self.training_indices]
        training_rewards = rewards[self.training_indices]
        training_dones = dones[self.training_indices]
        training_infos = [infos[i] for i in self.training_indices]
        
        return training_obs, training_rewards, training_dones, training_infos

    def close(self):
        """å…³é—­åº•å±‚ç¯å¢ƒ"""
        return self.venv.close()

    def get_attr(self, attr_name, indices=None):
        """è·å–åº•å±‚ç¯å¢ƒå±æ€§"""
        return self.venv.get_attr(attr_name, indices)

    def set_attr(self, attr_name, value, indices=None):
        """è®¾ç½®åº•å±‚ç¯å¢ƒå±æ€§"""
        return self.venv.set_attr(attr_name, value, indices)

    def env_method(self, method_name, *method_args, indices=None, **method_kwargs):
        """è°ƒç”¨åº•å±‚ç¯å¢ƒæ–¹æ³•"""
        return self.venv.env_method(method_name, *method_args, indices=indices, **method_kwargs)

    def env_is_wrapped(self, wrapper_class, indices=None):
        """æ£€æŸ¥ç¯å¢ƒæ˜¯å¦è¢«åŒ…è£…"""
        return self.venv.env_is_wrapped(wrapper_class, indices)


# ============================================================================
# è®­ç»ƒç›‘æ§å›è°ƒ
# ============================================================================

class TrainingMonitorCallback(BaseCallback):
    """ç›‘æ§è®­ç»ƒè¿‡ç¨‹"""

    def __init__(self, agent_type='Prey', check_freq=1000, verbose=1):
        """
        Args:
            agent_type: 'Prey' æˆ– 'Predator'
        """
        super().__init__(verbose)
        self.agent_type = agent_type
        self.check_freq = check_freq
        self.episode_rewards = []
        self.episode_lengths = []
        self.current_ep_reward = 0
        self.current_ep_length = 0

    def _on_step(self):
        # ç´¯åŠ å½“å‰æ­¥çš„æ‰€æœ‰è®­ç»ƒ agents çš„å¥–åŠ±
        reward_sum = np.sum(self.locals['rewards'])
        self.current_ep_reward += reward_sum
        self.current_ep_length += 1

        # æ£€æŸ¥æ˜¯å¦æœ‰ agent å®Œæˆ episode
        if np.any(self.locals['dones']):
            self.episode_rewards.append(self.current_ep_reward)
            self.episode_lengths.append(self.current_ep_length)

            if len(self.episode_rewards) % 10 == 0:
                recent_rewards = self.episode_rewards[-10:]
                print(f"\n[{self.agent_type} Training] Episode {len(self.episode_rewards)}:")
                print(f"  Avg Reward: {np.mean(recent_rewards):.2f}")
                print(f"  Avg Length: {np.mean(self.episode_lengths[-10:]):.0f}")
                print(f"  Max Reward: {np.max(recent_rewards):.2f}")

            self.current_ep_reward = 0
            self.current_ep_length = 0

        return True


# ============================================================================
# è®­ç»ƒæ›²çº¿ç»˜åˆ¶
# ============================================================================

def plot_training_curve(episode_rewards, agent_type='Prey', save_path=None):
    """
    ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    
    Args:
        episode_rewards: Episode å¥–åŠ±åˆ—è¡¨
        agent_type: 'Prey' æˆ– 'Predator'
        save_path: ä¿å­˜è·¯å¾„
    """
    if save_path is None:
        save_path = f'training_curve_{agent_type.lower()}.png'
    
    episodes = np.arange(1, len(episode_rewards) + 1)
    rewards = np.array(episode_rewards)

    plt.figure(figsize=(12, 6))

    # åŸå§‹å¥–åŠ±
    plt.plot(episodes, rewards, alpha=0.3, color='blue', label='Raw Rewards')

    # ç§»åŠ¨å¹³å‡
    if len(rewards) >= 10:
        window_size = min(10, len(rewards))
        smoothed = uniform_filter1d(rewards, size=window_size, mode='nearest')
        plt.plot(episodes, smoothed, color='red', linewidth=2, 
                label=f'Moving Average (window={window_size})')

    # è¶‹åŠ¿çº¿
    if len(rewards) >= 50:
        window_size = min(50, len(rewards))
        trend = uniform_filter1d(rewards, size=window_size, mode='nearest')
        plt.plot(episodes, trend, color='green', linewidth=2, linestyle='--', 
                label=f'Trend (window={window_size})')

    plt.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)

    plt.xlabel('Episode', fontsize=12)
    plt.ylabel(f'Episode Reward ({agent_type} Only)', fontsize=12)
    plt.title(f'PPO Training: {agent_type} Agents', fontsize=14, fontweight='bold')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)

    # ç»Ÿè®¡ä¿¡æ¯
    stats_text = f'Episodes: {len(rewards)}\n'
    stats_text += f'Mean: {np.mean(rewards):.2f}\n'
    stats_text += f'Std: {np.std(rewards):.2f}\n'
    stats_text += f'Max: {np.max(rewards):.2f}\n'
    stats_text += f'Min: {np.min(rewards):.2f}'

    n = len(rewards)
    if n >= 10:
        early = rewards[:max(1, n//10)]
        late = rewards[-max(1, n//10):]
        improvement = np.mean(late) - np.mean(early)
        stats_text += f'\nImprovement: {improvement:+.2f}'

    plt.text(0.02, 0.98, stats_text,
             transform=plt.gca().transAxes,
             fontsize=10,
             verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nTraining curve saved to: {save_path}")
    plt.close()


# ============================================================================
# ç¯å¢ƒåˆ›å»ºå’Œå‡†å¤‡
# ============================================================================

def create_mixed_env(n_predators=3, n_preys=5, mode='train_prey'):
    """åˆ›å»ºæ··åˆç¯å¢ƒ"""
    print("\n" + "="*60)
    print(f"Creating Mixed Environment")
    print("="*60)

    total_agents = n_predators + n_preys
    
    # è®¾ç½®ç®—æ³•æ ‡ç­¾ï¼ˆä»…ç”¨äºç¯å¢ƒåˆå§‹åŒ–ï¼‰
    if mode == 'train_prey':
        agent_algos = ["Fixed"] * n_predators + ["PPO"] * n_preys
    else:  # train_predator
        agent_algos = ["PPO"] * n_predators + ["Fixed"] * n_preys

    env = waterworld_v4.parallel_env(
        render_mode=None,
        n_predators=n_predators,
        n_preys=n_preys,
        n_evaders=180,
        n_obstacles=2,
        obstacle_coord=[(0.2, 0.2), (0.8, 0.2)],
        n_poisons=10,
        agent_algorithms=agent_algos,
        max_cycles=3000,
        static_food=True,
        static_poison=True,
    )

    print(f"Environment Configuration:")
    print(f"  Training Mode: {mode}")
    print(f"  Predators: {n_predators}")
    print(f"  Preys: {n_preys}")
    print(f"  Total Agents: {total_agents}")
    print(f"  All Agents: {env.possible_agents}")
    print(f"  Food: 180 (static)")
    print(f"  Poison: 10 (static)")
    print(f"  Observation Dim: {env.observation_space('prey_0').shape}")
    print(f"  Action Dim: {env.action_space('prey_0').shape}")

    return env, n_predators, n_preys


def prepare_env_for_training(env, n_predators, n_preys, mode='train_prey', 
                            fixed_policy=None):
    """
    å‡†å¤‡è®­ç»ƒç¯å¢ƒ
    
    Args:
        env: åŸå§‹ç¯å¢ƒ
        n_predators: Predator æ•°é‡
        n_preys: Prey æ•°é‡
        mode: 'train_prey' æˆ– 'train_predator'
        fixed_policy: å›ºå®š agents ä½¿ç”¨çš„ç­–ç•¥ï¼ˆAgentPolicy å®ä¾‹ï¼‰
    """
    print("\nConverting environment format...")

    # æ ‡å‡†è½¬æ¢
    env = ss.black_death_v3(env)
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    env = ss.concat_vec_envs_v1(env, 1, num_cpus=1, base_class='stable_baselines3')

    print("  Standard conversion complete")
    print(f"  num_envs after conversion: {env.num_envs}")

    # åº”ç”¨è‡ªå®šä¹‰æ··åˆç¯å¢ƒåŒ…è£…å™¨
    env = MixedAgentVecEnv(
        env, 
        n_predators=n_predators, 
        n_preys=n_preys,
        mode=mode,
        fixed_policy=fixed_policy
    )
    
    print(f"  Applied MixedAgentVecEnv wrapper")
    print(f"    - Final num_envs: {env.num_envs}")

    # æ·»åŠ ç›‘æ§
    env = VecMonitor(env)
    print("  Environment preparation complete")

    return env


# ============================================================================
# è®­ç»ƒå’Œè¯„ä¼°
# ============================================================================

def train_ppo(env, agent_type='Prey', total_timesteps=1000000):
    """ä½¿ç”¨ PPO è®­ç»ƒ"""
    print("\n" + "="*60)
    print(f"Starting PPO Training ({agent_type} Only)")
    print("="*60)
    print(f"Total Timesteps: {total_timesteps}")
    print(f"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")

    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        verbose=1,
        device='cpu'
    )

    callback = TrainingMonitorCallback(agent_type=agent_type, check_freq=1000)

    print(f"\nStarting training...")
    print(f"Note: {agent_type} agents are learning")
    print(f"      Other agents are executing fixed policy")

    model.learn(
        total_timesteps=total_timesteps,
        callback=callback,
        progress_bar=True
    )

    print("\nTraining complete!")

    # æ˜¾ç¤ºè®­ç»ƒç»Ÿè®¡
    if callback.episode_rewards:
        rewards = np.array(callback.episode_rewards)
        print("\n" + "="*60)
        print(f"Training Statistics ({agent_type} Only)")
        print("="*60)
        print(f"Total Episodes: {len(rewards)}")
        print(f"Mean Reward: {np.mean(rewards):.2f}")
        print(f"Max Reward: {np.max(rewards):.2f}")
        print(f"Min Reward: {np.min(rewards):.2f}")

        n = len(rewards)
        early = rewards[:max(1, n//10)]
        late = rewards[-max(1, n//10):]
        improvement = np.mean(late) - np.mean(early)

        print(f"\nLearning Analysis:")
        print(f"  Early Mean: {np.mean(early):.2f}")
        print(f"  Late Mean: {np.mean(late):.2f}")
        print(f"  Improvement: {improvement:+.2f}")

        if improvement > 5:
            print("  Conclusion: Effective Learning")
        elif improvement > -5:
            print("  Conclusion: Limited Learning")
        else:
            print("  Conclusion: No Effective Learning")

    return model, callback


def evaluate_model(model, env, agent_type='Prey', n_episodes=10):
    """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("\n" + "="*60)
    print(f"Evaluating {agent_type} Model ({n_episodes} episodes)")
    print("="*60)

    episode_rewards = []

    for ep in range(n_episodes):
        obs = env.reset()
        ep_reward = 0
        ep_length = 0

        while True:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            ep_reward += np.sum(reward)
            ep_length += 1

            if np.any(done):
                break

        episode_rewards.append(ep_reward)
        print(f"  Episode {ep+1}: Reward={ep_reward:.2f}, Length={ep_length}")

    print(f"\nEvaluation Results:")
    print(f"  Mean Reward: {np.mean(episode_rewards):.2f} Â± {np.std(episode_rewards):.2f}")
    print(f"  Max Reward: {np.max(episode_rewards):.2f}")

    return episode_rewards


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("Waterworld: Bidirectional Training System")
    print("="*60)

    # ========================================
    # é…ç½®åŒºåŸŸï¼šå¯ä»¥è½»æ¾åˆ‡æ¢è®­ç»ƒæ¨¡å¼
    # ========================================
    
    # é€‰æ‹©è®­ç»ƒæ¨¡å¼ï¼š'train_prey' æˆ– 'train_predator'
    TRAINING_MODE = 'train_predator'  # ä¿®æ”¹è¿™é‡Œæ¥åˆ‡æ¢æ¨¡å¼
    
    # ç¯å¢ƒé…ç½®
    N_PREDATORS = 3
    N_PREYS = 15
    TOTAL_TIMESTEPS = 150000
    
    # å›ºå®šç­–ç•¥é…ç½®
    # é€‰é¡¹1: ä½¿ç”¨éšæœºç­–ç•¥
    fixed_policy = RandomPolicy()
    
    # é€‰é¡¹2: ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå–æ¶ˆæ³¨é‡Šæ¥ä½¿ç”¨ï¼‰
    # fixed_policy = TrainedModelPolicy('prey_ppo_model.zip', device='cpu')
    
    # é€‰é¡¹3: ä½¿ç”¨è§„åˆ™ç­–ç•¥ï¼ˆå–æ¶ˆæ³¨é‡Šæ¥ä½¿ç”¨ï¼‰
    # fixed_policy = RuleBasedPolicy()
    
    # ========================================
    
    # æ ¹æ®æ¨¡å¼è®¾ç½®æ–‡ä»¶åå’Œ agent ç±»å‹
    if TRAINING_MODE == 'train_prey':
        agent_type = 'Prey'
        model_filename = 'prey_ppo_model'
        curve_filename = 'training_curve_prey.png'
    else:  # train_predator
        agent_type = 'Predator'
        model_filename = 'predator_ppo_model'
        curve_filename = 'training_curve_predator.png'
    
    print(f"\nğŸ¯ Selected Mode: Training {agent_type} Agents")
    print(f"ğŸ“¦ Fixed Policy: {fixed_policy.__class__.__name__}")
    
    # 1. åˆ›å»ºç¯å¢ƒ
    raw_env, n_predators, n_preys = create_mixed_env(
        n_predators=N_PREDATORS,
        n_preys=N_PREYS,
        mode=TRAINING_MODE
    )

    # 2. å‡†å¤‡è®­ç»ƒç¯å¢ƒ
    env = prepare_env_for_training(
        raw_env, 
        n_predators, 
        n_preys, 
        mode=TRAINING_MODE,
        fixed_policy=fixed_policy
    )

    # 3. è®­ç»ƒ
    model, callback = train_ppo(env, agent_type=agent_type, total_timesteps=TOTAL_TIMESTEPS)

    # 4. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if callback.episode_rewards:
        print("\n" + "="*60)
        print("Generating Training Curve")
        print("="*60)
        plot_training_curve(
            callback.episode_rewards,
            agent_type=agent_type,
            save_path=curve_filename
        )

    # 5. è¯„ä¼°
    episode_rewards = evaluate_model(model, env, agent_type=agent_type, n_episodes=10)

    # 6. ä¿å­˜æ¨¡å‹
    model.save(model_filename)
    print(f"\nğŸ’¾ Model saved: {model_filename}.zip")

    # 7. æœ€ç»ˆæ€»ç»“
    print("\n" + "="*60)
    print("Training Complete Summary")
    print("="*60)

    if callback.episode_rewards:
        rewards = np.array(callback.episode_rewards)
        eval_rewards = np.array(episode_rewards)

        print(f"Training Mode: {TRAINING_MODE}")
        print(f"  Training Agents: {agent_type}")
        print(f"  Fixed Policy: {fixed_policy.__class__.__name__}")

        print(f"\nDuring Training:")
        print(f"  Mean Reward: {np.mean(rewards):.2f}")
        print(f"  Max Reward: {np.max(rewards):.2f}")

        print(f"\nEvaluation Results:")
        print(f"  Mean Reward: {np.mean(eval_rewards):.2f}")
        print(f"  Max Reward: {np.max(eval_rewards):.2f}")

        if np.mean(eval_rewards) > np.mean(rewards[:len(rewards)//10]):
            print(f"\nâœ“ {agent_type} training is effective")
        else:
            print(f"\nâœ— {agent_type} training shows limited effectiveness")

    env.close()
    print("\nğŸ‰ Training complete!")
    print(f"ğŸ“ Files generated:")
    print(f"  - {model_filename}.zip")
    print(f"  - {curve_filename}")


if __name__ == "__main__":
    main()