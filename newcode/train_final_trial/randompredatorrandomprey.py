"""
Waterworld Dual-List Multi-Agent Training System
æ”¯æŒ Predator å’Œ Prey ç‹¬ç«‹é…ç½®çš„å¤šæ™ºèƒ½ä½“è®­ç»ƒæ¡†æ¶
âœ… åªè®°å½•è®­ç»ƒagentsçš„rewardæ•°æ®
"""

from pettingzoo.sisl import waterworld_v4
import supersuit as ss
from stable_baselines3 import PPO, SAC, TD3, A2C
from stable_baselines3.common.vec_env import VecMonitor, VecEnvWrapper
from stable_baselines3.common.callbacks import BaseCallback
import numpy as np
import torch
import matplotlib.pyplot as plt
from scipy.ndimage import uniform_filter1d
from typing import Dict, Any, Optional, List, Tuple
from abc import ABC, abstractmethod
import os
from datetime import datetime
from collections import deque


# ============================================================================
# Algorithm Configuration System
# ============================================================================

class AlgorithmConfig(ABC):
    """Base class for algorithm configurations"""
    
    def __init__(self, name: str):
        self.name = name
    
    @abstractmethod
    def get_model_class(self):
        """Return the algorithm model class"""
        pass
    
    @abstractmethod
    def get_hyperparameters(self) -> Dict[str, Any]:
        """Return hyperparameters for the algorithm"""
        pass
    
    def get_color(self) -> str:
        """Return color for plotting"""
        return 'blue'


class PPOConfig(AlgorithmConfig):
    def __init__(self):
        super().__init__("PPO")
    
    def get_model_class(self):
        return PPO
    
    def get_hyperparameters(self) -> Dict[str, Any]:
        return {
            'learning_rate': 5e-4,
            'n_steps': 2048,
            'batch_size': 64,
            'n_epochs': 10,
            'gamma': 0.98,
            'gae_lambda': 0.95,
            'clip_range': 0.2,
            'ent_coef': 0.0001,
            'vf_coef': 0.4,
            'max_grad_norm': 0.5,
            'verbose': 1,
        }
    
    def get_color(self) -> str:
        return 'blue'


class SACConfig(AlgorithmConfig):
    def __init__(self):
        super().__init__("SAC")
    
    def get_model_class(self):
        return SAC
    
    def get_hyperparameters(self) -> Dict[str, Any]:
        return {
            'learning_rate': 3e-4,
            'buffer_size': 1000000,
            'learning_starts': 1000,
            'batch_size': 256,
            'tau': 0.005,
            'gamma': 0.99,
            'train_freq': 1,
            'gradient_steps': 1,
            'ent_coef': 'auto',
            'verbose': 1,
        }
    
    def get_color(self) -> str:
        return 'red'


class TD3Config(AlgorithmConfig):
    def __init__(self):
        super().__init__("TD3")
    
    def get_model_class(self):
        return TD3
    
    def get_hyperparameters(self) -> Dict[str, Any]:
        return {
            'learning_rate': 1e-3,
            'buffer_size': 1000000,
            'learning_starts': 1000,
            'batch_size': 256,
            'tau': 0.005,
            'gamma': 0.99,
            'train_freq': 1,
            'gradient_steps': 1,
            'policy_delay': 2,
            'target_policy_noise': 0.2,
            'target_noise_clip': 0.5,
            'verbose': 1,
        }
    
    def get_color(self) -> str:
        return 'green'


class A2CConfig(AlgorithmConfig):
    def __init__(self):
        super().__init__("A2C")
    
    def get_model_class(self):
        return A2C
    
    def get_hyperparameters(self) -> Dict[str, Any]:
        return {
            'learning_rate': 7e-4,
            'n_steps': 5,
            'gamma': 0.99,
            'gae_lambda': 1.0,
            'ent_coef': 0.01,
            'vf_coef': 0.5,
            'max_grad_norm': 0.5,
            'verbose': 1,
        }
    
    def get_color(self) -> str:
        return 'orange'


ALGORITHM_REGISTRY = {
    'ppo': PPOConfig,
    'sac': SACConfig,
    'td3': TD3Config,
    'a2c': A2CConfig,
}


def get_algorithm_config(algo_name: str) -> AlgorithmConfig:
    """Get algorithm configuration by name"""
    algo_name = algo_name.lower()
    if algo_name not in ALGORITHM_REGISTRY:
        available = ', '.join(ALGORITHM_REGISTRY.keys())
        raise ValueError(f"Unknown algorithm '{algo_name}'. Available: {available}")
    return ALGORITHM_REGISTRY[algo_name]()


# ============================================================================
# Agent Name Parser
# ============================================================================

class AgentNameParser:
    """è§£æagentåç§°: algorithm_role_version_mode"""
    
    @staticmethod
    def parse(name: str) -> Dict[str, Any]:
        """
        è§£æagentåç§°
        æ”¯æŒæ ¼å¼:
        - 'ppo_prey_v1_train'
        - 'ppo_predator_v0_exe'
        - 'random_prey_exe'
        """
        parts = name.split('_')
        
        if len(parts) == 3:  # random_prey_exe
            algorithm, role, mode = parts
            return {
                'algorithm': algorithm.lower(),
                'role': role.lower(),
                'version': None,
                'mode': mode.lower()
            }
        elif len(parts) == 4:  # ppo_prey_v1_train
            algorithm, role, version, mode = parts
            return {
                'algorithm': algorithm.lower(),
                'role': role.lower(),
                'version': version.lower(),
                'mode': mode.lower()
            }
        else:
            raise ValueError(
                f"Invalid agent name: '{name}'\n"
                f"Expected: 'algorithm_role_version_mode' or 'random_role_mode'"
            )
    
    @staticmethod
    def validate(name: str) -> Tuple[bool, str]:
        """éªŒè¯agentåç§°æ ¼å¼"""
        try:
            parsed = AgentNameParser.parse(name)
            
            valid_algorithms = ['ppo', 'sac', 'td3', 'a2c', 'random']
            if parsed['algorithm'] not in valid_algorithms:
                return False, f"Invalid algorithm '{parsed['algorithm']}'"
            
            valid_roles = ['prey', 'predator']
            if parsed['role'] not in valid_roles:
                return False, f"Invalid role '{parsed['role']}'"
            
            valid_modes = ['train', 'exe']
            if parsed['mode'] not in valid_modes:
                return False, f"Invalid mode '{parsed['mode']}'"
            
            return True, "Valid"
        except Exception as e:
            return False, str(e)
    
    @staticmethod
    def get_model_path(name: str, base_dir: str = 'models') -> Optional[str]:
        """ç”Ÿæˆæ¨¡å‹æ–‡ä»¶è·¯å¾„"""
        parsed = AgentNameParser.parse(name)
        
        if parsed['mode'] == 'train' or parsed['algorithm'] == 'random':
            return None
        
        filename = f"{parsed['algorithm']}_{parsed['role']}_{parsed['version']}.zip"
        return os.path.join(base_dir, filename)


# ============================================================================
# Dual-List Configuration Manager
# ============================================================================

class DualListConfigManager:
    """åŒåˆ—è¡¨é…ç½®ç®¡ç†å™¨ - åˆ†åˆ«ç®¡ç† Predator å’Œ Prey"""
    
    def __init__(
        self, 
        predator_configs: List[Tuple[int, str]],
        prey_configs: List[Tuple[int, str]],
        model_base_dir: str = 'models'
    ):
        """
        Args:
            predator_configs: [(count, name), ...] for predators
            prey_configs: [(count, name), ...] for preys
        """
        self.model_base_dir = model_base_dir
        
        # è§£æå’ŒéªŒè¯é…ç½®
        self.predator_configs = self._parse_configs(predator_configs, 'predator')
        self.prey_configs = self._parse_configs(prey_configs, 'prey')
        
        self.n_predators = len(self.predator_configs)
        self.n_preys = len(self.prey_configs)
        self.n_total = self.n_predators + self.n_preys
        
        # è¯†åˆ«è®­ç»ƒé…ç½®
        self._identify_training_config()
        
        # éªŒè¯é…ç½®åˆæ³•æ€§
        self._validate_training_config()
    
    def _parse_configs(self, configs: List[Tuple[int, str]], expected_role: str) -> List[Dict]:
        """è§£æé…ç½®åˆ—è¡¨"""
        parsed_list = []
        
        for count, name in configs:
            # éªŒè¯åç§°
            is_valid, error_msg = AgentNameParser.validate(name)
            if not is_valid:
                raise ValueError(f"Invalid config '{name}': {error_msg}")
            
            # è§£æ
            parsed = AgentNameParser.parse(name)
            
            # éªŒè¯è§’è‰²åŒ¹é…
            if parsed['role'] != expected_role:
                raise ValueError(
                    f"Role mismatch: config '{name}' has role '{parsed['role']}', "
                    f"expected '{expected_role}'"
                )
            
            # å±•å¼€åˆ°æ¯ä¸ªagent
            for _ in range(count):
                parsed_list.append({
                    'name': name,
                    'algorithm': parsed['algorithm'],
                    'role': parsed['role'],
                    'version': parsed['version'],
                    'mode': parsed['mode'],
                })
        
        return parsed_list
    
    def _identify_training_config(self):
        """è¯†åˆ«è®­ç»ƒé…ç½®"""
        self.training_role = None
        self.training_algorithm = None
        self.training_version = None
        self.training_indices = []
        
        # æ£€æŸ¥ predator è®­ç»ƒ
        predator_train = [i for i, cfg in enumerate(self.predator_configs) if cfg['mode'] == 'train']
        
        # æ£€æŸ¥ prey è®­ç»ƒ
        prey_train = [i for i, cfg in enumerate(self.prey_configs) if cfg['mode'] == 'train']
        
        if predator_train:
            self.training_role = 'predator'
            # âœ… ä¿®å¤ï¼špredatoråœ¨ç¯å¢ƒä¸­æ’åœ¨preyä¹‹åï¼Œéœ€è¦åŠ åç§»
            self.training_indices = [i + self.n_preys for i in predator_train]
            self.training_algorithm = self.predator_configs[predator_train[0]]['algorithm']
            self.training_version = self.predator_configs[predator_train[0]]['version']
        
        if prey_train:
            self.training_role = 'prey'
            # âœ… ä¿®å¤ï¼špreyåœ¨ç¯å¢ƒä¸­æ’åœ¨æœ€å‰é¢ï¼Œä¸éœ€è¦åç§»
            self.training_indices = prey_train
            self.training_algorithm = self.prey_configs[prey_train[0]]['algorithm']
            self.training_version = self.prey_configs[prey_train[0]]['version']
    def _validate_training_config(self):
        """éªŒè¯è®­ç»ƒé…ç½®"""
        # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒagent
        predator_train = any(cfg['mode'] == 'train' for cfg in self.predator_configs)
        prey_train = any(cfg['mode'] == 'train' for cfg in self.prey_configs)
        
        # âœ… å…è®¸å…¨éƒ¨éƒ½æ˜¯æ‰§è¡Œæ¨¡å¼ï¼ˆæ— è®­ç»ƒï¼‰
        if not predator_train and not prey_train:
            print("\nâš ï¸  Warning: No training agents configured. Running in EXECUTION-ONLY mode.")
            return
        
        # ä¸æ”¯æŒåŒæ—¶è®­ç»ƒä¸¤ç§è§’è‰²
        if predator_train and prey_train:
            raise ValueError(
                "Cannot train both predator and prey simultaneously! "
                "Only one role can be in training mode."
            )
        
        # éªŒè¯è®­ç»ƒè§’è‰²åªæœ‰ä¸€ç§ç®—æ³•
        if predator_train:
            train_algos = {cfg['algorithm'] for cfg in self.predator_configs if cfg['mode'] == 'train'}
            if len(train_algos) > 1:
                raise ValueError(
                    f"Multiple training algorithms in predator: {train_algos}. "
                    f"Only one algorithm can be trained at a time."
                )
        
        if prey_train:
            train_algos = {cfg['algorithm'] for cfg in self.prey_configs if cfg['mode'] == 'train'}
            if len(train_algos) > 1:
                raise ValueError(
                    f"Multiple training algorithms in prey: {train_algos}. "
                    f"Only one algorithm can be trained at a time."
                )
    
    def get_agent_config(self, agent_id: int) -> Dict[str, Any]:
        """è·å–æŒ‡å®šagentçš„é…ç½®ï¼ˆå…¨å±€ç´¢å¼•ï¼‰"""
        if agent_id < self.n_predators:
            return self.predator_configs[agent_id].copy()
        else:
            return self.prey_configs[agent_id - self.n_predators].copy()
    
    def is_training_agent(self, agent_id: int) -> bool:
        """åˆ¤æ–­agentæ˜¯å¦åœ¨è®­ç»ƒ"""
        return agent_id in self.training_indices
    
    def get_training_indices(self) -> List[int]:
        """è·å–è®­ç»ƒagentçš„å…¨å±€ç´¢å¼•"""
        return self.training_indices.copy()
    
    def print_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        print("\n" + "="*70)
        print("Dual-List Agent Configuration")
        print("="*70)
        
        # Prey é…ç½®ï¼ˆæ˜¾ç¤ºåœ¨å‰ï¼Œå› ä¸ºç¯å¢ƒä¸­preyåœ¨å‰ï¼‰
        print("\nğŸ° PREY Configuration:")
        print("-" * 70)
        self._print_role_summary(self.prey_configs, 'prey', 0)  # âœ… preyåœ¨ç¯å¢ƒç´¢å¼•0å¼€å§‹
        
        # Predator é…ç½®
        print("\nğŸ¦ PREDATOR Configuration:")
        print("-" * 70)
        self._print_role_summary(self.predator_configs, 'predator', self.n_preys)  # âœ… predatorä»n_preyså¼€å§‹
    def _print_role_summary(self, configs: List[Dict], role: str, index_offset: int):
        """æ‰“å°å•ä¸ªè§’è‰²çš„é…ç½®æ‘˜è¦"""
        if not configs:
            print(f"  No {role}s configured")
            return
        
        # æŒ‰åç§°åˆ†ç»„
        name_groups = {}
        for i, cfg in enumerate(configs):
            name = cfg['name']
            if name not in name_groups:
                name_groups[name] = []
            name_groups[name].append(i + index_offset)
        
        # æ‰“å°æ¯ç»„
        for name, indices in name_groups.items():
            parsed = AgentNameParser.parse(name)
            
            print(f"\n  ğŸ“‹ {name}")
            print(f"     Count: {len(indices)}")
            print(f"     Global Indices: [{indices[0]}..{indices[-1]}]")
            print(f"     Algorithm: {parsed['algorithm'].upper()}")
            if parsed['version']:
                print(f"     Version: {parsed['version']}")
            
            mode_str = 'ğŸ”¥ TRAINING' if parsed['mode'] == 'train' else 'â–¶ï¸  EXECUTION'
            print(f"     Mode: {mode_str}")
            
            # å¦‚æœæ˜¯æ‰§è¡Œæ¨¡å¼ä¸”éœ€è¦æ¨¡å‹
            if parsed['mode'] == 'exe' and parsed['algorithm'] != 'random':
                model_path = AgentNameParser.get_model_path(name, self.model_base_dir)
                exists = os.path.exists(model_path) if model_path else False
                status = "âœ“" if exists else "âœ— (not found)"
                print(f"     Model: {model_path} {status}")


# ============================================================================
# Custom VecMonitor for Training Agents Only
# ============================================================================

class TrainingAgentVecMonitor(VecEnvWrapper):
    """
    è‡ªå®šä¹‰VecMonitorï¼šåªè®°å½•è®­ç»ƒagentsçš„ç»Ÿè®¡ä¿¡æ¯
    
    å…³é”®ï¼šè¿‡æ»¤æ‰æ‰§è¡Œagentsï¼ˆå¦‚random agentsï¼‰çš„reward
    """
    
    def __init__(self, venv, training_indices: List[int], filename=None):
        """
        Args:
            venv: å‘é‡åŒ–ç¯å¢ƒ
            training_indices: è®­ç»ƒagentsçš„ç¯å¢ƒç´¢å¼•åˆ—è¡¨
            filename: å¯é€‰çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„
        """
        VecEnvWrapper.__init__(self, venv)
        
        self.training_indices = set(training_indices)
        self.n_training_agents = len(training_indices)
        
        # Episodeç»Ÿè®¡ï¼ˆå…³é”®ï¼šéœ€è¦ç”¨äºSB3çš„ç»Ÿè®¡ç³»ç»Ÿï¼‰
        self.episode_returns = None
        self.episode_lengths = None
        self.episode_count = 0
        
        # ğŸ‘‡ å…³é”®ï¼šSB3éœ€è¦è¿™äº›å±æ€§
        self.episode_reward_buffer = deque(maxlen=100)
        self.episode_length_buffer = deque(maxlen=100)
        
        self.filename = filename
        
        print(f"\nğŸ¯ TrainingAgentVecMonitor Initialized")
        print(f"   Total environments: {self.num_envs}")
        print(f"   Training agents: {self.n_training_agents}")
        print(f"   Training indices: {sorted(list(self.training_indices))[:10]}...")
        print(f"   Execution agents (ignored): {self.num_envs - self.n_training_agents}")
    
    def reset(self):
        obs = self.venv.reset()
        self.episode_returns = np.zeros(self.num_envs)
        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)
        return obs
    
    def step_wait(self):
        obs, rewards, dones, infos = self.venv.step_wait()
        
        # ç¡®ä¿infosæ˜¯åˆ—è¡¨
        if not isinstance(infos, (list, tuple)):
            infos = [infos] * self.num_envs
        else:
            infos = list(infos)
        
        # ç¡®ä¿æ¯ä¸ªinfoæ˜¯å­—å…¸
        for i in range(len(infos)):
            if not isinstance(infos[i], dict):
                infos[i] = {}
        
        # ç´¯ç§¯rewardå’Œlength
        self.episode_returns += rewards
        self.episode_lengths += 1
        
        # å¤„ç†å®Œæˆçš„episode
        for i in range(len(dones)):
            if dones[i]:
                # åªä¸ºè®­ç»ƒagentsè®°å½•episodeä¿¡æ¯
                if i in self.training_indices:
                    ep_return = float(self.episode_returns[i])
                    ep_length = int(self.episode_lengths[i])
                    
                    # æ·»åŠ åˆ°bufferï¼ˆSB3ä¼šä½¿ç”¨è¿™äº›ï¼‰
                    self.episode_reward_buffer.append(ep_return)
                    self.episode_length_buffer.append(ep_length)
                    self.episode_count += 1
                    
                    # ğŸ‘‡ å…³é”®ï¼šæ·»åŠ åˆ°infosï¼ŒSB3çš„loggerä¼šè¯»å–è¿™ä¸ª
                    ep_info = {
                        'r': ep_return,
                        'l': ep_length,
                        't': self.episode_count * ep_length
                    }
                    
                    # ç¡®ä¿'episode'é”®å­˜åœ¨
                    infos[i]['episode'] = ep_info
                    
                    # è°ƒè¯•è¾“å‡º
                    if self.episode_count <= 10:
                        print(f"   Episode {self.episode_count} (training agent {i}):")
                        print(f"     Reward: {ep_return:.2f}, Length: {ep_length}")
                
                # é‡ç½®æ‰€æœ‰agentsçš„è®¡æ•°å™¨
                self.episode_returns[i] = 0
                self.episode_lengths[i] = 0
        
        return obs, rewards, dones, infos
    
    # ğŸ‘‡ å…³é”®ï¼šæ·»åŠ è¿™äº›æ–¹æ³•ä¾›SB3ä½¿ç”¨
    def get_episode_rewards(self):
        """è¿”å›episode reward bufferï¼ˆSB3ä¼šè°ƒç”¨ï¼‰"""
        return list(self.episode_reward_buffer)
    
    def get_episode_lengths(self):
        """è¿”å›episode length bufferï¼ˆSB3ä¼šè°ƒç”¨ï¼‰"""
        return list(self.episode_length_buffer)
    
    def get_episode_times(self):
        """è¿”å›episode timesï¼ˆSB3å¯èƒ½éœ€è¦ï¼‰"""
        return []
# ============================================================================
# TensorBoard Helper Functions
# ============================================================================

def create_tensorboard_log_dir(algo_name: str, role: str, base_dir: str = "./tensorboard_logs") -> str:
    """åˆ›å»ºTensorBoardæ—¥å¿—ç›®å½•"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = os.path.join(base_dir, f"{algo_name.lower()}_{role}", timestamp)
    os.makedirs(log_dir, exist_ok=True)
    
    print(f"\n{'='*60}")
    print("TensorBoard Configuration")
    print(f"{'='*60}")
    print(f"Log Directory: {log_dir}")
    print(f"\nTo view TensorBoard, run:")
    print(f"  tensorboard --logdir={base_dir}")
    print(f"\nThen open: http://localhost:6006")
    print(f"{'='*60}\n")
    
    return log_dir


# ============================================================================
# Training Components
# ============================================================================

class TrainingMonitorCallback(BaseCallback):
    """
    Monitor training process with custom TensorBoard logging
    
    âœ… ç°åœ¨åªè®°å½•è®­ç»ƒagentsçš„æ•°æ®ï¼ˆé€šè¿‡TrainingAgentVecMonitorè¿‡æ»¤ï¼‰
    """
    
    def __init__(self, config_manager: 'DualListConfigManager', check_freq=1000, verbose=1):
        super().__init__(verbose)
        self.check_freq = check_freq
        self.config_manager = config_manager
        
        # Episodeç»Ÿè®¡
        self.episode_rewards = []
        self.episode_lengths = []
        
        # è®­ç»ƒé…ç½®
        self.n_training_agents = len(config_manager.get_training_indices())
        role = config_manager.training_role
        algo = config_manager.training_algorithm
        version = config_manager.training_version or 'v1'
        self.tag_prefix = f"{role}/{algo}_{version}"
        
        print(f"\nğŸ“Š TrainingMonitorCallback Initialized")
        print(f"   Tag Prefix: {self.tag_prefix}")
        print(f"   Training Agents: {self.n_training_agents}")
        print(f"   âœ… Only training agents' rewards will be recorded")
        
    def _on_step(self):
        """æ¯ä¸ªstepè°ƒç”¨"""
        # ä»infosä¸­æå–episodeä¿¡æ¯
        infos = self.locals.get('infos', [])
        
        for info in infos:
            if 'episode' in info:
                # Episodeç»“æŸï¼ˆå·²è¢«TrainingAgentVecMonitorè¿‡æ»¤ï¼ŒåªåŒ…å«è®­ç»ƒagentsï¼‰
                ep_reward = info['episode']['r']
                ep_length = info['episode']['l']
                
                self.episode_rewards.append(ep_reward)
                self.episode_lengths.append(ep_length)
                
                # æ¯10ä¸ªepisodeæ‰“å°è¿›åº¦
                if len(self.episode_rewards) % 10 == 0:
                    recent_rewards = self.episode_rewards[-10:]
                    print(f"\nğŸ“ˆ Training Episode {len(self.episode_rewards)}:")
                    print(f"   Avg Reward (training agents only): {np.mean(recent_rewards):.2f}")
                    print(f"   Avg Length: {np.mean(self.episode_lengths[-10:]):.0f}")
                    print(f"   Max Reward: {np.max(recent_rewards):.2f}")
                    print(f"   Min Reward: {np.min(recent_rewards):.2f}")
        
        return True
    
    def _on_rollout_end(self):
        """åœ¨æ¯ä¸ªrolloutç»“æŸæ—¶è®°å½•åˆ°TensorBoard"""
        if hasattr(self.logger, 'name_to_value'):
            # è·å–å¹³å‡rewardï¼ˆç°åœ¨åªåŒ…å«è®­ç»ƒagentsï¼‰
            ep_rew_mean = self.logger.name_to_value.get('rollout/ep_rew_mean', None)
            
            if ep_rew_mean is not None:
                # è®°å½•è®­ç»ƒagentsçš„å¹³å‡reward
                self.logger.record(
                    f"{self.tag_prefix}/training_agents_reward",
                    ep_rew_mean
                )
                
                # è®°å½•è®­ç»ƒagentæ•°é‡
                self.logger.record(
                    f"{self.tag_prefix}/n_training_agents",
                    self.n_training_agents
                )
                
                # è®°å½•æœ€è¿‘çš„ç»Ÿè®¡
                if len(self.episode_rewards) >= 10:
                    self.logger.record(
                        f"{self.tag_prefix}/reward_mean_10ep",
                        np.mean(self.episode_rewards[-10:])
                    )
                
                if len(self.episode_rewards) > 0:
                    self.logger.record(
                        f"{self.tag_prefix}/reward_std",
                        np.std(self.episode_rewards)
                    )
                    self.logger.record(
                        f"{self.tag_prefix}/reward_max",
                        np.max(self.episode_rewards)
                    )
                    self.logger.record(
                        f"{self.tag_prefix}/reward_min",
                        np.min(self.episode_rewards)
                    )
        
        return True


def plot_training_curve(episode_rewards, algo_name, role, save_path=None):
    """Plot episode rewards"""
    if save_path is None:
        save_path = f'training_curve_{algo_name.lower()}_{role}.png'
    
    episodes = np.arange(1, len(episode_rewards) + 1)
    rewards = np.array(episode_rewards)
    
    try:
        config = get_algorithm_config(algo_name)
        color = config.get_color()
    except:
        color = 'blue'
    
    plt.figure(figsize=(12, 6))
    plt.plot(episodes, rewards, alpha=0.3, color=color, label='Raw Rewards')
    
    if len(rewards) >= 10:
        window_size = min(10, len(rewards))
        smoothed = uniform_filter1d(rewards, size=window_size, mode='nearest')
        plt.plot(episodes, smoothed, color='red', linewidth=2, 
                label=f'Moving Average (window={window_size})')
    
    if len(rewards) >= 50:
        window_size = min(50, len(rewards))
        trend = uniform_filter1d(rewards, size=window_size, mode='nearest')
        plt.plot(episodes, trend, color='green', linewidth=2, 
                linestyle='--', label=f'Trend (window={window_size})')
    
    plt.axhline(y=0, color='black', linestyle='--', alpha=0.3, linewidth=1)
    plt.xlabel('Episode', fontsize=12)
    plt.ylabel('Episode Reward', fontsize=12)
    plt.title(f'{algo_name} ({role.capitalize()}) Training Progress', 
             fontsize=14, fontweight='bold')
    plt.legend(loc='lower right')
    plt.grid(True, alpha=0.3)
    
    # Statistics
    stats_text = f'Algorithm: {algo_name}\n'
    stats_text += f'Role: {role.capitalize()}\n'
    stats_text += f'Episodes: {len(rewards)}\n'
    stats_text += f'Mean: {np.mean(rewards):.2f}\n'
    stats_text += f'Std: {np.std(rewards):.2f}\n'
    stats_text += f'Max: {np.max(rewards):.2f}\n'
    stats_text += f'Min: {np.min(rewards):.2f}'
    
    n = len(rewards)
    if n >= 10:
        early = rewards[:max(1, n//10)]
        late = rewards[-max(1, n//10):]
        improvement = np.mean(late) - np.mean(early)
        stats_text += f'\nImprovement: {improvement:+.2f}'
    
    plt.text(0.02, 0.98, stats_text, 
             transform=plt.gca().transAxes,
             fontsize=10,
             verticalalignment='top',
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nğŸ“Š Training curve saved to: {save_path}")
    plt.close()


# ============================================================================
# Environment Setup
# ============================================================================

def create_waterworld_env(config_manager: DualListConfigManager):
    """åˆ›å»º Waterworld ç¯å¢ƒ"""
    print("\n" + "="*60)
    print("Creating Waterworld Environment")
    print("="*60)
    
    # ä¸º PettingZoo ç”Ÿæˆ agent_algorithms åˆ—è¡¨
    # æ ¼å¼: [preyç®—æ³•...] + [predatorç®—æ³•...]
    agent_algos = []
    
    # Prey algorithms
    for cfg in config_manager.prey_configs:
        algo = cfg['algorithm'].upper() if cfg['algorithm'] != 'random' else 'Random'
        agent_algos.append(algo)
    
    # Predator algorithms
    for cfg in config_manager.predator_configs:
        algo = cfg['algorithm'].upper() if cfg['algorithm'] != 'random' else 'Random'
        agent_algos.append(algo)
    
    env = waterworld_v4.parallel_env(
        render_mode=None,
        n_predators=config_manager.n_predators,
        n_preys=config_manager.n_preys,
        n_evaders=60,
        n_obstacles=2,
        thrust_penalty=0,
        obstacle_coord=[(0.2, 0.2), (0.8, 0.2)],
        n_poisons=10,
        agent_algorithms=agent_algos,
        max_cycles=1000,
        static_food=True,
        static_poison=True,
    )
    
    print(f"Environment Created:")
    print(f"  Predators: {config_manager.n_predators}")
    print(f"  Preys: {config_manager.n_preys}")
    print(f"  Total Agents: {config_manager.n_total}")
    print(f"  Agent Algorithms: {agent_algos[:5]}..." if len(agent_algos) > 5 else f"  Agent Algorithms: {agent_algos}")
    
    return env


def prepare_env_for_training(env, config_manager: DualListConfigManager):
    """å‡†å¤‡è®­ç»ƒç¯å¢ƒï¼ˆä¿®æ”¹ç‰ˆï¼šåªç›‘æ§è®­ç»ƒagentsï¼‰"""
    print("\nConverting environment format...")
    
    env = ss.black_death_v3(env)
    env = ss.pettingzoo_env_to_vec_env_v1(env)
    env = ss.concat_vec_envs_v1(env, 1, num_cpus=1, base_class='stable_baselines3')
    
    # ğŸ‘‡ ä½¿ç”¨è‡ªå®šä¹‰çš„TrainingAgentVecMonitor
    # åªè®°å½•è®­ç»ƒagentsçš„ç»Ÿè®¡ä¿¡æ¯
    training_indices = config_manager.get_training_indices()
    env = TrainingAgentVecMonitor(env, training_indices=training_indices)
    
    print("  Environment conversion complete")
    print(f"  âœ… Monitoring ONLY {len(training_indices)} training agents")
    
    return env


# ============================================================================
# Main Training Function
# ============================================================================

def main(
    # ğŸ‘‡ æ ¸å¿ƒé…ç½®ï¼šä¸¤ä¸ªåˆ—è¡¨
    predator_configs=[
        (2, 'random_predator_exe'),
    ],
    prey_configs=[
        (30, 'ppo_prey_v1_train'),
        (20, 'random_prey_exe'),
    ],
    
    # è®­ç»ƒå‚æ•°
    total_timesteps=10000000,
    use_tensorboard=True,
    model_base_dir='models',
    
    # ğŸ‘‡ æ‰§è¡Œæ¨¡å¼å‚æ•°
    execution_mode=False,  # å¦‚æœä¸ºTrueï¼Œåªè¿è¡Œç¯å¢ƒä¸è®­ç»ƒ
    n_episodes=100,        # æ‰§è¡Œæ¨¡å¼ä¸‹è¿è¡Œçš„å›åˆæ•°
):
    """
    åŒåˆ—è¡¨å¤šæ™ºèƒ½ä½“è®­ç»ƒä¸»å‡½æ•°
    
    Args:
        predator_configs: [(count, name), ...] Predatoré…ç½®
        prey_configs: [(count, name), ...] Preyé…ç½®
        total_timesteps: è®­ç»ƒæ€»æ­¥æ•°ï¼ˆç¯å¢ƒäº¤äº’æ¬¡æ•°ï¼‰
        use_tensorboard: æ˜¯å¦å¯ç”¨TensorBoard
        model_base_dir: æ¨¡å‹ä¿å­˜ç›®å½•
        execution_mode: æ˜¯å¦ä¸ºçº¯æ‰§è¡Œæ¨¡å¼ï¼ˆä¸è®­ç»ƒï¼Œåªè¿è¡Œï¼‰
        n_episodes: æ‰§è¡Œæ¨¡å¼ä¸‹è¿è¡Œçš„å›åˆæ•°
    """
    print("="*70)
    print("Waterworld Dual-List Multi-Agent System")
    print("="*70)
    
    # 1. åˆ›å»ºé…ç½®ç®¡ç†å™¨
    config_manager = DualListConfigManager(
        predator_configs=predator_configs,
        prey_configs=prey_configs,
        model_base_dir=model_base_dir
    )
    config_manager.print_summary()
    
    # 2. æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒagent
    has_training = config_manager.training_role is not None
    
    if not has_training and not execution_mode:
        print("\n" + "="*70)
        print("âš ï¸  No Training Agents Detected")
        print("="*70)
        print("You have two options:")
        print("  1. Add training agents to your configs")
        print("  2. Set execution_mode=True to run in execution-only mode")
        print("\nExample for execution-only mode:")
        print("  main(")
        print("      predator_configs=[(2, 'random_predator_exe')],")
        print("      prey_configs=[(50, 'random_prey_exe')],")
        print("      execution_mode=True,")
        print("      n_episodes=100")
        print("  )")
        return
    
    # 3. åˆ›å»ºç¯å¢ƒ
    raw_env = create_waterworld_env(config_manager)
    
    # ============================================================
    # åˆ†æ”¯1ï¼šæ‰§è¡Œæ¨¡å¼ï¼ˆæ— è®­ç»ƒï¼‰
    # ============================================================
    if not has_training or execution_mode:
        print("\n" + "="*70)
        print("ğŸ® EXECUTION-ONLY MODE")
        print("="*70)
        print(f"Running {n_episodes} episodes...")
        
        # TensorBoardè®¾ç½®
        tensorboard_log = None
        writer = None
        if use_tensorboard:
            try:
                from torch.utils.tensorboard import SummaryWriter
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                log_dir = os.path.join("./tensorboard_logs", "execution_mode", timestamp)
                os.makedirs(log_dir, exist_ok=True)
                
                writer = SummaryWriter(log_dir=log_dir)
                
                print(f"\n{'='*60}")
                print("TensorBoard Configuration (Execution Mode)")
                print(f"{'='*60}")
                print(f"Log Directory: {log_dir}")
                print(f"\nTo view TensorBoard, run:")
                print(f"  tensorboard --logdir=./tensorboard_logs")
                print(f"\nThen open: http://localhost:6006")
                print(f"{'='*60}\n")
            except ImportError:
                print("\nâš ï¸  TensorBoard not available. Install with: pip install tensorboard")
                use_tensorboard = False
        
        # ğŸ‘‡ åˆ†åˆ«è®°å½•predatorå’Œpreyçš„æ•°æ®
        episode_rewards_all = []
        episode_rewards_predator = []
        episode_rewards_prey = []
        episode_lengths = []
        
        for ep in range(n_episodes):
            observations, infos = raw_env.reset()
            ep_reward_all = 0
            ep_reward_predator = 0
            ep_reward_prey = 0
            ep_length = 0
            episode_done = False
            
            # è·å–å½“å‰ç¯å¢ƒä¸­çš„agentåˆ—è¡¨
            current_agents = list(raw_env.agents)
            
            # è¯†åˆ«predatorå’Œprey agentsï¼ˆåŸºäºç¯å¢ƒå‘½åè§„åˆ™ï¼‰
            predator_agents = [a for a in current_agents if 'predator' in a]
            prey_agents = [a for a in current_agents if 'prey' in a]
            
            while not episode_done:
                # ä¸ºæ¯ä¸ªagentç”ŸæˆéšæœºåŠ¨ä½œ
                actions = {}
                for agent in raw_env.agents:
                    actions[agent] = raw_env.action_space(agent).sample()
                
                # ç¯å¢ƒstep
                observations, rewards, terminations, truncations, infos = raw_env.step(actions)
                
                # åˆ†åˆ«ç´¯è®¡ä¸åŒè§’è‰²çš„å¥–åŠ±
                if rewards:
                    # æ‰€æœ‰agentsçš„å¹³å‡
                    ep_reward_all += np.mean(list(rewards.values()))
                    
                    # Predatorçš„å¹³å‡
                    predator_rewards = [r for agent, r in rewards.items() if agent in predator_agents]
                    if predator_rewards:
                        ep_reward_predator += np.mean(predator_rewards)
                    
                    # Preyçš„å¹³å‡
                    prey_rewards = [r for agent, r in rewards.items() if agent in prey_agents]
                    if prey_rewards:
                        ep_reward_prey += np.mean(prey_rewards)
                
                ep_length += 1
                
                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                episode_done = len(raw_env.agents) == 0 or all(terminations.values()) or all(truncations.values())
            
            # è®°å½•æ•°æ®
            episode_rewards_all.append(ep_reward_all)
            episode_rewards_predator.append(ep_reward_predator)
            episode_rewards_prey.append(ep_reward_prey)
            episode_lengths.append(ep_length)
            
            # è®°å½•åˆ°TensorBoard
            if use_tensorboard and writer is not None:
                # æ€»ä½“reward
                writer.add_scalar('execution/all/episode_reward', ep_reward_all, ep)
                
                # Predator reward
                writer.add_scalar('execution/predator/episode_reward', ep_reward_predator, ep)
                
                # Prey reward
                writer.add_scalar('execution/prey/episode_reward', ep_reward_prey, ep)
                
                # Episode length
                writer.add_scalar('execution/episode_length', ep_length, ep)
                
                # ç§»åŠ¨å¹³å‡
                if len(episode_rewards_all) >= 10:
                    writer.add_scalar('execution/all/reward_mean_10ep', 
                                    np.mean(episode_rewards_all[-10:]), ep)
                    writer.add_scalar('execution/predator/reward_mean_10ep', 
                                    np.mean(episode_rewards_predator[-10:]), ep)
                    writer.add_scalar('execution/prey/reward_mean_10ep', 
                                    np.mean(episode_rewards_prey[-10:]), ep)
                
                # ç´¯è®¡å¹³å‡
                writer.add_scalar('execution/all/reward_mean_all', 
                                np.mean(episode_rewards_all), ep)
                writer.add_scalar('execution/predator/reward_mean_all', 
                                np.mean(episode_rewards_predator), ep)
                writer.add_scalar('execution/prey/reward_mean_all', 
                                np.mean(episode_rewards_prey), ep)
            
            if (ep + 1) % 10 == 0:
                print(f"Episode {ep+1}/{n_episodes}:")
                print(f"  All Agents:  {np.mean(episode_rewards_all[-10:]):.2f}")
                print(f"  Predator:    {np.mean(episode_rewards_predator[-10:]):.2f}")
                print(f"  Prey:        {np.mean(episode_rewards_prey[-10:]):.2f}")
                print(f"  Length:      {np.mean(episode_lengths[-10:]):.0f}")
        
        # æœ€ç»ˆç»Ÿè®¡
        if use_tensorboard and writer is not None:
            # æ±‡æ€»ç»Ÿè®¡
            writer.add_scalar('execution/final/all_mean_reward', np.mean(episode_rewards_all), n_episodes)
            writer.add_scalar('execution/final/predator_mean_reward', np.mean(episode_rewards_predator), n_episodes)
            writer.add_scalar('execution/final/prey_mean_reward', np.mean(episode_rewards_prey), n_episodes)
            writer.add_scalar('execution/final/mean_length', np.mean(episode_lengths), n_episodes)
            
            # æ–‡æœ¬æ‘˜è¦
            summary_text = f"""
            ## Execution Mode Summary
            
            **Configuration:**
            - Episodes: {n_episodes}
            - Predators: {config_manager.n_predators}
            - Preys: {config_manager.n_preys}
            
            **Results:**
            - All Agents: {np.mean(episode_rewards_all):.2f} Â± {np.std(episode_rewards_all):.2f}
            - Predator: {np.mean(episode_rewards_predator):.2f} Â± {np.std(episode_rewards_predator):.2f}
            - Prey: {np.mean(episode_rewards_prey):.2f} Â± {np.std(episode_rewards_prey):.2f}
            - Mean Length: {np.mean(episode_lengths):.0f}
            """
            writer.add_text('execution/summary', summary_text, 0)
            
            writer.close()
            print(f"\nğŸ“Š TensorBoard logs saved to: {log_dir}")
        
        # æ‰“å°ç»Ÿè®¡
        print("\n" + "="*70)
        print("Execution Statistics")
        print("="*70)
        print(f"Episodes: {n_episodes}")
        print(f"\nAll Agents:")
        print(f"  Mean Reward: {np.mean(episode_rewards_all):.2f} Â± {np.std(episode_rewards_all):.2f}")
        print(f"\nPredator:")
        print(f"  Mean Reward: {np.mean(episode_rewards_predator):.2f} Â± {np.std(episode_rewards_predator):.2f}")
        print(f"\nPrey:")
        print(f"  Mean Reward: {np.mean(episode_rewards_prey):.2f} Â± {np.std(episode_rewards_prey):.2f}")
        print(f"\nMean Length: {np.mean(episode_lengths):.0f}")
        
        # ç»˜åˆ¶æ›²çº¿
        if episode_rewards_all:
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_training_curve(episode_rewards_all, 'Random', 'all_agents', 
                              save_path=f'execution_all_{timestamp_str}.png')
            plot_training_curve(episode_rewards_predator, 'Random', 'predator', 
                              save_path=f'execution_predator_{timestamp_str}.png')
            plot_training_curve(episode_rewards_prey, 'Random', 'prey', 
                              save_path=f'execution_prey_{timestamp_str}.png')
        
        raw_env.close()
        print("\nâœ… Execution Complete!")
        return
    
    # ============================================================
    # åˆ†æ”¯2ï¼šè®­ç»ƒæ¨¡å¼ - éœ€è¦å‘é‡åŒ–ç¯å¢ƒ
    # ============================================================
    env = prepare_env_for_training(raw_env, config_manager)  # ğŸ‘ˆ æ·»åŠ config_managerå‚æ•°
    
    # 4. è·å–è®­ç»ƒç®—æ³•é…ç½®
    algo_config = get_algorithm_config(config_manager.training_algorithm)
    
    # 5. è®¾ç½®TensorBoard
    tensorboard_log = None
    if use_tensorboard:
        tensorboard_log = create_tensorboard_log_dir(
            algo_config.name, 
            config_manager.training_role
        )
    
    # 6. åˆ›å»ºæ¨¡å‹
    print("\n" + "="*60)
    print(f"Creating {algo_config.name} Model")
    print("="*60)
    
    ModelClass = algo_config.get_model_class()
    hyperparams = algo_config.get_hyperparameters()
    
    model = ModelClass(
        "MlpPolicy",
        env,
        **hyperparams,
        tensorboard_log=tensorboard_log,
        device='cuda' if torch.cuda.is_available() else 'cpu'
    )
    
    print(f"Model: {algo_config.name}")
    print(f"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
    print(f"Training Role: {config_manager.training_role.upper()}")
    print(f"Training Agents: {len(config_manager.training_indices)}")
    
    # 7. è®­ç»ƒ
    print("\n" + "="*60)
    print("Starting Training")
    print("="*60)
    
    n_training_agents = len(config_manager.training_indices)
    
    print(f"\nâš ï¸  Training Step Explanation:")
    print(f"  Number of training agents: {n_training_agents}")
    print(f"  Total timesteps setting: {total_timesteps:,}")
    print(f"  â†’ Each agent will collect ~{total_timesteps:,} samples")
    print(f"  â†’ Total samples collected: ~{total_timesteps * n_training_agents:,}")
    print(f"  â†’ Environment steps: ~{total_timesteps:,}")
    print(f"\n  Note: With parameter sharing, all {n_training_agents} training agents")
    print(f"        share the same policy and learn from each other's experiences.")
    
    callback = TrainingMonitorCallback(config_manager=config_manager, check_freq=1000)
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=callback,
        progress_bar=True,
        tb_log_name=f"{algo_config.name}_{config_manager.training_role}"
    )
    
    print("\nâœ… Training Complete!")
    
    # 8. ä¿å­˜æ¨¡å‹
    os.makedirs(model_base_dir, exist_ok=True)
    
    version = config_manager.training_version or 'v1'
    model_filename = f"{config_manager.training_algorithm}_{config_manager.training_role}_{version}"
    model_path = os.path.join(model_base_dir, model_filename)
    
    model.save(model_path)
    print(f"\nğŸ’¾ Model saved: {model_path}.zip")
    
    # 9. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    if callback.episode_rewards:
        plot_training_curve(
            callback.episode_rewards, 
            algo_config.name,
            config_manager.training_role
        )
    
    # 10. ç»Ÿè®¡ä¿¡æ¯
    if callback.episode_rewards:
        rewards = np.array(callback.episode_rewards)
        print("\n" + "="*70)
        print("Training Statistics (Training Agents Only)")  # ğŸ‘ˆ ä¿®æ”¹æ ‡é¢˜
        print("="*70)
        print(f"âœ… Data Source: ONLY {n_training_agents} training agents")
        print(f"   (Execution agents' rewards are NOT included)")
        print(f"\nTotal Episodes: {len(rewards)}")
        print(f"Mean Reward: {np.mean(rewards):.2f}")
        print(f"Std Reward: {np.std(rewards):.2f}")
        print(f"Max Reward: {np.max(rewards):.2f}")
        print(f"Min Reward: {np.min(rewards):.2f}")
        
        n = len(rewards)
        if n >= 10:
            early = rewards[:max(1, n//10)]
            late = rewards[-max(1, n//10):]
            improvement = np.mean(late) - np.mean(early)
            
            print(f"\nLearning Progress:")
            print(f"  Early Mean: {np.mean(early):.2f}")
            print(f"  Late Mean: {np.mean(late):.2f}")
            print(f"  Improvement: {improvement:+.2f}")
            
            if improvement > 5:
                print("  âœ“ Effective Learning")
            elif improvement > -5:
                print("  ~ Limited Learning")
            else:
                print("  âœ— No Effective Learning")
    
    env.close()
    print("\n" + "="*70)
    print("ğŸ‰ Training Pipeline Complete!")
    print("="*70)


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    """
    ========================================
    ä½¿ç”¨æ–¹å¼ï¼šä¿®æ”¹ä¸‹é¢ä¸¤ä¸ªåˆ—è¡¨å³å¯
    ========================================
    """
    
    # ============================================================
    # åœºæ™¯1ï¼šè®­ç»ƒ Predatorï¼ŒPrey éšæœº
    # ============================================================
    predator_configs = [
        (3, 'random_predator_exe'),  # 3ä¸ªéšæœºæ•é£Ÿè€…
    ]
    
    prey_configs = [
        (3, 'random_prey_exe'),   # 3ä¸ªéšæœºçŒç‰©
        (3, 'random_prey_exe'),     # 3ä¸ªéšæœºçŒç‰©
    ]

    
    # ============================================================
    # åœºæ™¯2ï¼šè®­ç»ƒ Preyï¼ŒPredator éšæœº
    # ============================================================
    # predator_configs = [
    #     (2, 'random_predator_exe'),  # 2ä¸ªéšæœºæ•é£Ÿè€…
    # ]
    # 
    # prey_configs = [
    #     (30, 'ppo_prey_v1_train'),   # 30ä¸ªè®­ç»ƒPPO
    #     (20, 'random_prey_exe'),     # 20ä¸ªéšæœº
    # ]
    
    # ============================================================
    # åœºæ™¯3ï¼šå…¨éƒ¨randomæ‰§è¡Œï¼ˆæ— è®­ç»ƒï¼Œåªè¿è¡Œç¯å¢ƒï¼‰
    # ============================================================
    # predator_configs = [
    #     (2, 'random_predator_exe'),
    # ]
    # 
    # prey_configs = [
    #     (50, 'random_prey_exe'),
    # ]
    # 
    # main(
    #     predator_configs=predator_configs,
    #     prey_configs=prey_configs,
    #     execution_mode=True,
    #     n_episodes=100,
    #     use_tensorboard=True,
    # )
    
    # ============================================================
    # è¿è¡Œè®­ç»ƒ
    # ============================================================
    main(
        predator_configs=predator_configs,
        prey_configs=prey_configs,
        total_timesteps=1000000,
        use_tensorboard=True,
        execution_mode=True,
        model_base_dir='models',
    )